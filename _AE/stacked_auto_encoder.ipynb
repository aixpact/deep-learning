{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if module is imported\n",
    "def assert_imported(package):\n",
    "    mods = [m.__name__ for m in sys.modules.values() if m]\n",
    "    assert(package in mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset from any path in cwd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_file(filename, **kwargs):\n",
    "    assert_imported('pathlib')\n",
    "    fpath = list(Path('.').glob('**/' + str(filename)))[0]\n",
    "    return pd.read_csv(fpath, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "movies = read_file('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = read_file('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = read_file('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training set and the test set\n",
    "training_set = read_file('ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = read_file('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the Stacked Auto Encoder\n",
    "# inherit from Class nn\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_hl1, n_hl2, n_hl3):\n",
    "        # initialize nn.Module(super of SAE)\n",
    "        super(SAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(nb_movies, n_hl1)\n",
    "        self.fc2 = nn.Linear(n_hl1, n_hl2)\n",
    "        self.fc3 = nn.Linear(n_hl2, n_hl3)\n",
    "        self.fc4 = nn.Linear(n_hl3, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN\n",
    "sae = SAE(20, 10, 20)\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "def optimizer(name):\n",
    "    opt = {'RMS': 'optim.RMSprop(sae.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.5)', \n",
    "         'Adam': 'optim.Adam(sae.parameters(), lr=0.01, eps=1e-08, weight_decay=0.25)'}\n",
    "    return eval(opt[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAE\n",
    "N_EPOCH = 20\n",
    "\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) # [torch.FloatTensor of size 1x1682]\n",
    "        target = input.clone()\n",
    "        \n",
    "        # train rated movies only\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False # turn off gradient computation\n",
    "            output[target == 0] = 0 # save computation cost\n",
    "            \n",
    "            # compute (MSE) loss\n",
    "            # adjust trained/rated movies loss to all movies loss\n",
    "            loss = loss_fn(output, target)\n",
    "            mean_adjust = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            train_loss += np.sqrt(loss.data[0] * mean_adjust)\n",
    "            s += 1.\n",
    "            \n",
    "            # define loss direction and optimizer generator\n",
    "            loss.backward()\n",
    "            optimize = optimizer(\"Adam\")\n",
    "            optimize.step()\n",
    "            \n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
