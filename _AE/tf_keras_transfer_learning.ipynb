{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision - Transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications import vgg16, vgg19, resnet50\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mymods.lauthom import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = '../data/ae_images/louvre_small.jpg'\n",
    "style_reference_image_path = '../data/ae_images/monet.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the generated picture.\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre- and de- process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"util function to open, resize and format pictures into appropriate tensors\"\"\"\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\"util function to convert a tensor into a valid image\"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, img_nrows, img_ncols))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(plt.imread(base_image_path))\n",
    "_ = plt.show()\n",
    "\n",
    "_ = plt.imshow(plt.imread(style_reference_image_path))\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content and style image tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated image placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
    "else:\n",
    "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the 3 images into a single Keras tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the VGG19 network with our 3 images as input\n",
    "\n",
    "The model will be loaded with pre-trained ImageNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet', \n",
    "                    include_top=False)\n",
    "\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictify(outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the neural style loss, we first need to define 4 util functions:\n",
    "\n",
    "- gram_matrix: feature-wise outer product\n",
    "- style_loss: maintain the \"style\" of the reference image in the generated image\n",
    "- content_loss: maintain the \"content\" of the base image in the generated image\n",
    "- total_variation_loss: keep the generated image locally coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\"Return gram matrix of an image tensor(feature-wise outer product).\n",
    "    \n",
    "    Captures the style from images.\"\"\"\n",
    "    # Single image\n",
    "    assert K.ndim(x) == 3\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "        \n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style loss\n",
    "\n",
    "Is designed to maintain the style of the reference image in the generated image. It is based on the gram matrices (which capture style) of feature maps from the style reference image and from the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    \"\"\"maintain the \"style\" of the reference image in the generated image\"\"\"\n",
    "    # check is single image (not batch)\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    \n",
    "    GS = gram_matrix(style)\n",
    "    GC = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(GS - GC)) / (4. * (channels**2) * (size**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    \"\"\"maintain the \"content\" of the base image in the generated image\"\"\"\n",
    "    return K.sum(K.square(base - combination))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total variation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x, total_variation_weight=.025):\n",
    "    \"\"\"keep the generated image locally coherent\"\"\"\n",
    "    # batch of images\n",
    "    assert K.ndim(x) == 4\n",
    "    \n",
    "    r, c = img_nrows-1, img_ncols-1\n",
    "    \n",
    "    # Square difference in shifted(by 1) rows(a) and columns(b)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(x[:, :, :r, :c] - x[:, :, 1:, :c])\n",
    "        b = K.square(x[:, :, :r, :c] - x[:, :, :r, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :r, :c, :] - x[:, 1:, :c, :])\n",
    "        b = K.square(x[:, :r, :c, :] - x[:, :r, 1:, :])\n",
    "        \n",
    "    return K.sum(K.pow(a + b, 1.25)) * total_variation_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine loss functions into a single loss scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the different loss components\n",
    "style_weight = 1.\n",
    "content_weight = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise loss variable\n",
    "loss = K.variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content loss\n",
    "# block5_conv2 output on 2 images\n",
    "layer_outputs = outputs_dict['block5_conv2']\n",
    "base_image_output = layer_outputs[0, ...]\n",
    "combination_output = layer_outputs[2, ...]\n",
    "\n",
    "# WARNING:tensorflow:Variable += will be deprecated => Use variable.assign_add()\n",
    "loss += (content_weight * content_loss(base_image_output, combination_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted style loss added to content loss\n",
    "# Output layers\n",
    "output_layers = ['block1_conv1', 'block2_conv1','block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "\n",
    "# Can amend to different weights per output layer\n",
    "for layer_name in output_layers:\n",
    "    # Layerwise losses\n",
    "    layer_outputs = outputs_dict[layer_name]\n",
    "    style_output = layer_outputs[1, ...]\n",
    "    combination_output = layer_outputs[2, ...]\n",
    "    \n",
    "    sl = style_loss(style_output, combination_output)\n",
    "    loss += ((sl * style_weight/len(output_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted variation loss added to loss\n",
    "loss += (total_variation_loss(combination_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss  # <tf.Tensor 'add_7:0' shape=() dtype=float32>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-BFGS optimisation\n",
    "\n",
    "L-BFGS optimisation is a Quasi-Newton method\n",
    "\n",
    "Basically think of L-BFGS as a way of finding a (local) minimum of an objective function, making use of objective function values and the gradient of the objective function. That level of description covers many optimization methods in addition to L-BFGS though. \n",
    "\n",
    "You can read more about it in section 7.2 of Nocedal and Wright \"Numerical Optimization, 2nd edition\" http://www.springer.com/us/book/9780387303031 . A very cursory discussion of L-BFGS is provided at https://en.wikipedia.org/wiki/Limited-memory_BFGS .\n",
    "\n",
    "First order method means gradients (first derivatives) (and maybe objective function values) are used, but not Hessian (second derivatives). Think of, for instance, gradient descent and steepest descent, among many others.\n",
    "\n",
    "Second order method means gradients and Hessian are used (and maybe objective function values). Second order methods can be either based on\n",
    "\n",
    " - \"Exact\" Hessian matrix (or finite differences of gradients), in which case they are known as Newton methods \n",
    " or\n",
    " - Quasi-Newton methods, which approximate the Hessian based on differences of gradients over several iterations, by imposing a \"secant\" (Quasi-Newton) condition. There are many different Quasi-Newton methods, which estimate the Hessian in different ways. One of the most popular is BFGS.\n",
    " \n",
    "The BFGS Hessian approximation can either be based on the full history of gradients, in which case it is referred to as BFGS, or it can be based only on the most recent m gradients, in which case it is known as limited memory BFGS, abbreviated as L-BFGS. The advantage of L-BFGS is that it requires only retaining the most recent m gradients, where m is usually around 10 to 20, which is a much smaller storage requirement than n*(n+1)/2 elements required to store the full (triangle) of a Hessian estimate, as in required with BFGS, where n is the problem dimension. Unlike (full) BFGS, the estimate of the Hessian is never explicitly formed or stored in L-BFGS; rather, the calculations which would be required with the estimate of the Hessian are accomplished without explicitly forming it. L-BFGS is used instead of BFGS for very large problems (when n is very large), but might not perform as well as BFGS. Therefore, BFGS is preferred over L-BFGS when the memory requirements of BFGS can be met. On the other hand, L-BFGS may not be much worse in performance than BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients of the generated image wrt the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build intermediate layer outputs: [loss, gradients]\n",
    "# Loss is scalar\n",
    "outputs = [loss]\n",
    "\n",
    "# Get scalar, list, tuple of gradient(s) wrt loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "# Append gradients to loss\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "    n_grads = len(grads)\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "    n_grads = 1\n",
    "\n",
    "# K.function(input, output) retrieves output from intermediate layer\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n",
    "f_outputs = K.function([combination_image], outputs)\n",
    "\n",
    "\n",
    "def reshape_img(img):\n",
    "    \"\"\"Reshape image based on Keras model format\"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        return img.reshape((1, 3, img_nrows, img_ncols))\n",
    "    return img.reshape((1, img_nrows, img_ncols, 3))\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(img):\n",
    "    \"\"\"Get the gradients of the generated image wrt the loss\"\"\"\n",
    "    \n",
    "    # Get intermediate layer outputs\n",
    "    outs = f_outputs([reshape_img(img)])\n",
    "    loss_value = outs[0]\n",
    "    \n",
    "    # Flatten gradient(s) vector - single or multiple gradients\n",
    "    if len(outs[1:]) == 1: # n_grads == 1\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    \"\"\"Compute loss and gradients in one pass,\n",
    "    while retrieving them via two separate functions.\n",
    "    \n",
    "    'scipy.optimize' requires separate functions for loss and gradients\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, img):\n",
    "        \"\"\"Compute loss and gradients and return loss\"\"\"\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(img) # \n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        \"\"\"Return loss and reset(zero) loss and gradients.\"\"\"\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TensorBoard callback to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorboard\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/callbacks/\n",
    "tb_callback = TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True\n",
    ")\n",
    "tb_callback.set_model(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(dl_kernel) ✌️ >$ tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# Run L-BFGS optimization over the generated image to minimize the neural style loss\n",
    "proc_img = preprocess_image(base_image_path)\n",
    "\n",
    "# Effect = iterations x evaluations\n",
    "iterations = 30  # frequency of intermediate results\n",
    "evaluations = 5  # every 5 evaluations output result (5x5 = 9minutes, 1x30 = 9minutes)\n",
    "stopwatch = Timer()\n",
    "\n",
    "# Run optimizer, \n",
    "# proc_img loads recursively\n",
    "for i in range(iterations):\n",
    "    proc_img, min_val, info = fmin_l_bfgs_b(evaluator.loss,  # callable minimizing function\n",
    "                                     proc_img.flatten(),     # initial guess\n",
    "                                     fprime=evaluator.grads, # gradients of above minimizing function\n",
    "                                     maxfun=evaluations)     # max. # evaluations\n",
    "    \n",
    "    print(f'Iteration: {i:2} Current loss value: {min_val:>12.0f} {stopwatch()}')\n",
    "    \n",
    "    # save current generated image\n",
    "    img = deprocess_image(proc_img.copy())\n",
    "    fname =  f'./output/image_at_iteration_{i}.png'\n",
    "    save_img(fname, img)\n",
    "    print(f'Image saved as {fname}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show tensorboard graph _TODO\n",
    "\n",
    "https://stackoverflow.com/questions/37128652/creating-log-directory-in-tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TB(cleanup=False):\n",
    "    # https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter\n",
    "    import webbrowser\n",
    "    webbrowser.open('http://127.0.1.1:6006')\n",
    "\n",
    "    !tensorboard --logdir=./logs\n",
    "\n",
    "    if cleanup:\n",
    "        !rm -R logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
