{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Create synthetic data file (No Noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5).reshape(-1, 1)\n",
    "x.reshape(-1, 1)\n",
    "r = np.random.randint(-5, 5, 5)\n",
    "r.reshape(-1, 1)\n",
    "\n",
    "# Broadcasting to random shifted range\n",
    "x + r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "T = 20   # amplitude range\n",
    "L = 1000 # features, # inputs, shifted ranges, time steps\n",
    "N = 100  # examples & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.empty((N, L), 'int64')\n",
    "L_rand = np.random.randint(-4*T, 4*T, N)\n",
    "noise = np.random.randn(N*L) * 0.3\n",
    "\n",
    "# Broadcast random over range\n",
    "x[:] = np.arange(L) + L_rand.reshape(N, 1)\n",
    "data = np.sin(x/T).astype('float64')\n",
    "data += noise.reshape(N, -1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and load pytorch data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('traindata.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set [3:]: X_train = inputs = [:-1], y_train = outputs = [:1]\n",
    "inputs = Variable(torch.from_numpy(data[3:, :-1]), requires_grad=False)\n",
    "target = Variable(torch.from_numpy(data[3:, 1:]), requires_grad=False)\n",
    "\n",
    "# Test set [:3]: X_test = inputs = [:3], y_test = outputs = [1:]\n",
    "test_input = Variable(torch.from_numpy(data[:3, :-1]), requires_grad=False)\n",
    "test_target = Variable(torch.from_numpy(data[:3, 1:]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, inputs, future=0):\n",
    "        outputs = []\n",
    "        kwargs = {'torch.zeros(input.size(0), 51).double(), requires_grad=False'}\n",
    "        \n",
    "        h_t1 = Variable(torch.zeros(inputs.size(0), 51).double(), requires_grad=False)\n",
    "        c_t1 = Variable(torch.zeros(inputs.size(0), 51).double(), requires_grad=False)\n",
    "        h_t2 = Variable(torch.zeros(inputs.size(0), 51).double(), requires_grad=False)\n",
    "        c_t2 = Variable(torch.zeros(inputs.size(0), 51).double(), requires_grad=False)\n",
    "\n",
    "        for i, input_t in enumerate(inputs.chunk(inputs.size(1), dim=1)):\n",
    "            h_t1, c_t1 = self.lstm1(input_t, (h_t1, c_t1))\n",
    "            h_t2, c_t2 = self.lstm2(h_t1, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "            \n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t1, c_t1 = self.lstm1(output, (h_t1, c_t1))\n",
    "            h_t2, c_t2 = self.lstm2(h_t1, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "            \n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "seq = Sequence()\n",
    "seq.double() # TODO\n",
    "\n",
    "# loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# use LBFGS as optimizer since we can load the whole data to train\n",
    "optimizer = optim.LBFGS(seq.parameters(), lr=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Using closure paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'Iteration: {i}')\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(inputs)\n",
    "        loss = criterion(out, target)\n",
    "        print(f' - loss: {loss.data.numpy()}')\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    _ = optimizer.step(closure)\n",
    "    \n",
    "print('Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 1000\n",
    "pred = seq(test_input, future=future)\n",
    "loss = criterion(pred[:, :-future], test_target)\n",
    "\n",
    "print(f'test loss: {loss.data.numpy()}')\n",
    "y = pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the result\n",
    "def draw(yi, color):\n",
    "    _ = plt.figure(figsize=(20,8))\n",
    "    _ = plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "    _ = plt.xlabel('x', fontsize=16)\n",
    "    _ = plt.ylabel('y', fontsize=16)\n",
    "    _ = plt.xticks(fontsize=16)\n",
    "    _ = plt.yticks(fontsize=16)\n",
    "    _ = plt.plot(np.arange(inputs.size(1)), yi[:inputs.size(1)], color, linewidth=2.)\n",
    "    _ = plt.plot(np.arange(inputs.size(1), inputs.size(1)+future), yi[inputs.size(1):], f'{color}:', linewidth=2.)\n",
    "    plt.show();\n",
    "    \n",
    "draw(y[0], 'r')\n",
    "draw(y[1], 'g')\n",
    "draw(y[2], 'b')\n",
    "\n",
    "# plt.savefig(f'predict_{i}.pdf')\n",
    "# plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
