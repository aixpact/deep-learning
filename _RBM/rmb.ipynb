{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mymods.lauthom import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'movies')\n",
    "get_path('*/*', 'users')\n",
    "get_path('*/*', 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, **kwargs):\n",
    "    \"\"\"Get file path and read file\"\"\"\n",
    "    from pathlib import Path\n",
    "    fpath = list(Path('../../').glob('*/*/' + str(filename)))[0]\n",
    "    return pd.read_csv(fpath, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "movies = read_file('movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "users = read_file('users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "ratings = read_file('ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'u1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = read_file('../../_data/ml-100k/u1.base', delimiter='\\t')\n",
    "test_set = read_file('../../_data/ml-100k/u1.test', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training set and the test set\n",
    "# last feature is set type; 0 = train, 1 = test\n",
    "# concatenate assumes axis = 0\n",
    "# hstack assumes axis = 1 unless inputs are 1d, then axis = 0\n",
    "# vstack assumes axis = 0 after adding an axis if inputs are 1d\n",
    "# append flattens array\n",
    "\n",
    "\n",
    "training_set = np.array(training_set, dtype='int')\n",
    "training_set.head()\n",
    "\n",
    "\n",
    "test_set = np.array(test_set, dtype='int')\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example ratings for user = 1\n",
    "# test set contains all movie ratings for all users\n",
    "# train set contains some blanks, to infer\n",
    "user_id = 1\n",
    "trn = training_set[training_set[:, 0] == user_id, 2:4]\n",
    "tst = test_set[test_set[:, 0] == user_id, 2:4]\n",
    "\n",
    "# check relation train - test\n",
    "for t in tst[:, 1][:10]:\n",
    "    print(t, t in trn, t in tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum # users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
    "nb_users, nb_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into an array \n",
    "\n",
    " - users in lines\n",
    " - movies in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into an array with users in lines and movies in columns\n",
    "# return nested list: user_list(movie_rating_list) - format required by pytorch\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0]==id_users]\n",
    "        id_ratings = data[:,2][data[:,0]==id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if both sets are equal, such that contain all users and all movies\n",
    "assert np.array(training_set).shape==np.array(test_set).shape\n",
    "\n",
    "# number of movies/ratings training set\n",
    "print(len(training_set[0]))\n",
    "\n",
    "# number of movies/ratings test set\n",
    "print(len(test_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set == 0] = -1\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "training_set[training_set >= 3] = 1\n",
    "\n",
    "test_set[test_set == 0] = -1\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "test_set[test_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "args: nv = visual neurons(input layer), nh = hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        \"\"\"initialize weights with random normal\"\"\"\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(1, nh) \n",
    "        self.b = torch.randn(1, nv) \n",
    "        \n",
    "    def probability(self, activation):\n",
    "        \"\"\"get sigmoid probability and sample from a Bernoulli distribution\"\"\"\n",
    "        sigmoid = torch.sigmoid(activation)\n",
    "        return sigmoid, torch.bernoulli(sigmoid)\n",
    "    \n",
    "    def activation(self, input, weight, bias):\n",
    "        \"\"\"get activation\"\"\"\n",
    "        wi = torch.mm(input, weight)\n",
    "        return wi + bias.expand_as(wi)\n",
    "    \n",
    "    def sample_h(self, x):\n",
    "        \"\"\"get prob and binairy activation for hidden layer\"\"\"\n",
    "        return self.probability(self.activation(x, self.W.t(), self.a))\n",
    "    \n",
    "    def sample_v(self, y):\n",
    "        \"\"\"get prob and binairy activation for visual layer\"\"\"\n",
    "        return self.probability(self.activation(y, self.W, self.b))\n",
    "    \n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        \"\"\"update weights for state 0 to k\"\"\"\n",
    "        self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIS = len(training_set[0])\n",
    "N_HID = 100\n",
    "N_EPOCH = 20\n",
    "BATCH_SIZE = 100\n",
    "N_WALKS = 15\n",
    "\n",
    "rbm = RBM(N_VIS, N_HID)\n",
    "print(rbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the RBM\n",
    "# train on rated movies - exclude unrated movies (ratings with value -1)\n",
    "# loss = train set without blanks - test set without blanks from train set\n",
    "for epoch in np.arange(N_EPOCH)+1:\n",
    "    \n",
    "    # average train_loss /users\n",
    "    cum_train_loss = 0\n",
    "    u = 0.\n",
    "    print(list(range(nb_users - BATCH_SIZE, BATCH_SIZE)))\n",
    "    \n",
    "    # batchwise\n",
    "    for u, id_user in enumerate(range(nb_users - BATCH_SIZE, BATCH_SIZE)):\n",
    "        print(u)\n",
    "        vk = training_set[id_user:id_user+BATCH_SIZE]\n",
    "        v0 = vk.clone() # training_set[id_user:id_user+batch_size]\n",
    "        ph0,_ = rbm.sample_h(v0)\n",
    "        \n",
    "        # optimize by blind/random walk\n",
    "        # divergence\n",
    "        # get bernoulli \n",
    "        for k in range(N_WALKS):\n",
    "            _,hk = rbm.sample_h(vk)\n",
    "            _,vk = rbm.sample_v(hk)\n",
    "            vk[v0 < 0] = v0[v0 < 0] # do not update vk for unrated movies\n",
    "        \n",
    "        # get converged hidden probs\n",
    "        phk,_ = rbm.sample_h(vk) \n",
    "        \n",
    "        # update weights\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        \n",
    "        # cum train_loss/users\n",
    "        # vk is v_hat or inferred rating of rated movies\n",
    "        # loss = difference in ratings; 0-1, 1-0, 0-0, 1-1\n",
    "        # 25% loss = 1 out of 4 movies are misqualified\n",
    "        cum_train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
    "#         u += 1. # number of users to average out the cum loss\n",
    "        print('epoch: {} train loss: {} u: {}'.format(epoch, cum_train_loss, u))\n",
    "        print('epoch: ' + str(epoch)+' loss: ' + str(cum_train_loss/u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the RBM\n",
    "# test set contains all users and all ratings\n",
    "# train set has same shape as test set, but contains unrated movies for inferrence\n",
    "cum_test_loss = 0\n",
    "u = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    v = training_set[id_user:id_user+1]\n",
    "    vt = test_set[id_user:id_user+1]\n",
    "    \n",
    "    # infer unrated movies in train set if test set contains rated movies\n",
    "    if len(vt[vt >= 0]) > 0:\n",
    "        _,h = rbm.sample_h(v)\n",
    "        _,v = rbm.sample_v(h)\n",
    "        \n",
    "    # loss = train set with true & inferred ratings -/- test set with all true ratings\n",
    "    u += 1.\n",
    "    cum_test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0])) #/u\n",
    "        \n",
    "    print('test loss: ' + str(cum_test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
