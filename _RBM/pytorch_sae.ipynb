{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mymods.lauthom import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'movies')\n",
    "get_path('*/*', 'users')\n",
    "get_path('*/*', 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, **kwargs):\n",
    "    \"\"\"Get file path and read file\"\"\"\n",
    "    from pathlib import Path\n",
    "    fpath = list(Path('../../').glob('*/*/' + str(filename)))[0]\n",
    "    return pd.read_csv(fpath, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "movies = read_file('movies.dat', sep='::', header=None, encoding='latin-1', names=['id', 'movie', 'cat'])\n",
    "users = read_file('users.dat', sep='::', header=None, encoding='latin-1', names=['id', 'sex', 'unk1', 'unk2', 'unk3'])\n",
    "ratings = read_file('ratings.dat', sep='::', header=None, encoding='latin-1', names=['user_id', 'movie_id', 'rating' , 'unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.sample(10)\n",
    "users.sample(10)\n",
    "ratings.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()\n",
    "users.info()\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'u1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_file('../../_data/ml-100k/u1.base', delimiter='\\t', header=None, names=['user_id', 'movie_id', 'rating', 'unk'])\n",
    "df_test = read_file('../../_data/ml-100k/u1.test', delimiter='\\t', header=None, names=['user_id', 'movie_id', 'rating', 'unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['test'] = False\n",
    "df_test['test'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example ratings for user = 1\n",
    "user_id = 1\n",
    "mask = df['user_id'] == user_id\n",
    "trn = df.loc[mask, :]\n",
    "trn.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique users and movies in both train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set(df['user_id'])\n",
    "movies = set(df['movie_id'])\n",
    "nb_users, nb_movies = len(users), len(movies)\n",
    "nb_users, nb_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for pivot and split\n",
    "df.loc[df['test']==False, 'user_id'] = df.loc[df['test']==False, 'user_id'].values + 99000\n",
    "\n",
    "# Pivot for RBM model\n",
    "pv = df.pivot(index='user_id', columns='movie_id', values='rating')\n",
    "\n",
    "# Change rating: negative/positive:\n",
    "# nan: -1, 1-2: 0, 3-5: 1\n",
    "mask_null = pv.isnull()\n",
    "mask_3 = pv>=3\n",
    "\n",
    "pv[mask_3] = 1\n",
    "pv[~mask_3] = 0\n",
    "pv[mask_null] = -1\n",
    "\n",
    "# Split train test\n",
    "pv_train = pv.loc[pv.index > 99000, :]\n",
    "pv_train.index = pv_train.index - 99000\n",
    "pv_test = pv.loc[pv.index < 99000, :]\n",
    "\n",
    "pv_train.sample(10)\n",
    "pv_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_train.info()\n",
    "pv_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = np.array(pv_train, dtype='int')\n",
    "np_test = np.array(pv_test, dtype='int')\n",
    "\n",
    "np_train.shape\n",
    "np_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check np.array & rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rank train:', 'nan', sum(sum(np_train == -1)))\n",
    "print('rank test:', 'nan', sum(sum(np_train == -1)))\n",
    "    \n",
    "for r in range(6):\n",
    "    print('rank train:', r, sum(sum(np_train == r)))\n",
    "    print('rank test:', r, sum(sum(np_train == r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(np_train)\n",
    "test_set = torch.FloatTensor(np_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.shape\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the Stacked Auto Encoder\n",
    "# inherit from Class nn\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_hl1, n_hl2, n_hl3):\n",
    "        # initialize nn.Module(super of SAE)\n",
    "        super(SAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(nb_movies, n_hl1)\n",
    "        self.fc2 = nn.Linear(n_hl1, n_hl2)\n",
    "        self.fc3 = nn.Linear(n_hl2, n_hl3)\n",
    "        self.fc4 = nn.Linear(n_hl3, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architecture\n",
    "sae = SAE(128, 32, 128)\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "def optimizer(name):\n",
    "    opt = {'RMS': 'optim.RMSprop(sae.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.25)', \n",
    "           'Adam': 'optim.Adam(sae.parameters(), lr=0.01, eps=1e-08, weight_decay=0.5)',\n",
    "           'SGD': 'optim.SGD(sae.parameters(), lr=0.01, momentum=0.95)'}\n",
    "    return eval(opt[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in training_set][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 50\n",
    "# optimiser = optimizer('SGD') # static optimiser\n",
    "\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        inputs = Variable(training_set[id_user]).unsqueeze(0) # [torch.FloatTensor of size 1x1682]\n",
    "        target = inputs[:]\n",
    "        \n",
    "        # train on users with rated movies only\n",
    "        if torch.sum(target.data > -1).item() > 0:\n",
    "            output = sae(inputs)\n",
    "            target.require_grad = False # turn off gradient computation\n",
    "            output[target == 0] = 0     # save computation cost\n",
    "            \n",
    "            # compute (MSE) loss - difference betweeen input and output\n",
    "            # adjust trained/rated movies loss to all movies loss\n",
    "            loss = loss_fn(output, target)\n",
    "            mean_adjust = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            train_loss += np.sqrt(loss.item() * mean_adjust)\n",
    "            s += 1.\n",
    "            \n",
    "            # Backprop loss and optimizer           \n",
    "            optimize = optimizer('SGD') # dynamic optimiser\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "    print('epoch: {:3} loss: {:.3f}'.format(epoch, train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "# pv_train.loc[i,:]\n",
    "np_new = np.array(pv_train.loc[i,:], dtype='int')\n",
    "np_new.shape\n",
    "new_data = torch.FloatTensor(np_new.reshape(1,-1))\n",
    "new_data.shape\n",
    "new_data = Variable(new_data).unsqueeze(0)\n",
    "# new_data[0]\n",
    "pred = sae(new_data).detach().numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_new += 1\n",
    "orig = np_new[:] #.reshape(8, 210) #*255/2\n",
    "pred = pred[:] #.reshape(8, 210)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "_ = plt.figure(figsize=(20,20))\n",
    "_ = plt.plot(range(len(orig)), orig)\n",
    "_ = plt.plot(range(len(orig)), pred)\n",
    "_ = plt.show()\n",
    "\n",
    "# _ = plt.figure(figsize=(20,20))\n",
    "# _ = plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
