{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mymods.lauthom import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'movies')\n",
    "get_path('*/*', 'users')\n",
    "get_path('*/*', 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, **kwargs):\n",
    "    \"\"\"Get file path and read file\"\"\"\n",
    "    from pathlib import Path\n",
    "    fpath = list(Path('../../').glob('*/*/' + str(filename)))[0]\n",
    "    return pd.read_csv(fpath, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "movies = read_file('movies.dat', sep='::', header=None, encoding='latin-1', names=['id', 'movie', 'cat'])\n",
    "users = read_file('users.dat', sep='::', header=None, encoding='latin-1', names=['id', 'sex', 'unk1', 'unk2', 'unk3'])\n",
    "ratings = read_file('ratings.dat', sep='::', header=None, encoding='latin-1', names=['user_id', 'movie_id', 'rating' , 'unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.sample(10)\n",
    "users.sample(10)\n",
    "ratings.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()\n",
    "users.info()\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('*/*', 'u1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_file('../../_data/ml-100k/u1.base', delimiter='\\t', header=None, names=['user_id', 'movie_id', 'rating', 'unk'])\n",
    "df_test = read_file('../../_data/ml-100k/u1.test', delimiter='\\t', header=None, names=['user_id', 'movie_id', 'rating', 'unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['test'] = False\n",
    "df_test['test'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example ratings for user = 1\n",
    "user_id = 1\n",
    "mask = df['user_id'] == user_id\n",
    "trn = df.loc[mask, :]\n",
    "trn.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique users and movies in both train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set(df['user_id'])\n",
    "movies = set(df['movie_id'])\n",
    "nb_users, nb_movies = len(users), len(movies)\n",
    "nb_users, nb_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for pivot and split\n",
    "df.loc[df['test']==False, 'user_id'] = df.loc[df['test']==False, 'user_id'].values + 99000\n",
    "\n",
    "# Pivot for RBM model\n",
    "pv = df.pivot(index='user_id', columns='movie_id', values='rating')\n",
    "\n",
    "# Change rating: negative/positive:\n",
    "# nan: -1, 1-2: 0, 3-5: 1\n",
    "mask_null = pv.isnull()\n",
    "mask_3 = pv>=3\n",
    "\n",
    "pv[mask_3] = 1\n",
    "pv[~mask_3] = 0\n",
    "pv[mask_null] = -1\n",
    "\n",
    "# Split train test\n",
    "pv_train = pv.loc[pv.index > 99000, :]\n",
    "pv_train.index = pv_train.index - 99000\n",
    "pv_test = pv.loc[pv.index < 99000, :]\n",
    "\n",
    "pv_train.sample(10)\n",
    "pv_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_train.info()\n",
    "pv_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = np.array(pv_train, dtype='int')\n",
    "np_test = np.array(pv_test, dtype='int')\n",
    "\n",
    "np_train.shape\n",
    "np_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check np.array & rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rank train:', 'nan', sum(sum(np_train == -1)))\n",
    "print('rank test:', 'nan', sum(sum(np_train == -1)))\n",
    "    \n",
    "for r in range(6):\n",
    "    print('rank train:', r, sum(sum(np_train == r)))\n",
    "    print('rank test:', r, sum(sum(np_train == r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(np_train)\n",
    "test_set = torch.FloatTensor(np_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.shape\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "args: nv = visual neurons(input layer), nh = hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        \"\"\"initialize weights with random normal\"\"\"\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(1, nh) \n",
    "        self.b = torch.randn(1, nv) \n",
    "        \n",
    "    def probability(self, activation):\n",
    "        \"\"\"get sigmoid probability and sample from a Bernoulli distribution\"\"\"\n",
    "        sigmoid = torch.sigmoid(activation)\n",
    "        return sigmoid, torch.bernoulli(sigmoid)\n",
    "    \n",
    "    def activation(self, inputs, weight, bias):\n",
    "        \"\"\"get activation\"\"\"\n",
    "        wi = torch.mm(inputs, weight)\n",
    "        return wi + bias.expand_as(wi)\n",
    "    \n",
    "    def sample_h(self, x):\n",
    "        \"\"\"get prob and binairy activation for hidden layer\"\"\"\n",
    "        return self.probability(self.activation(x, self.W.t(), self.a))\n",
    "    \n",
    "    def sample_v(self, y):\n",
    "        \"\"\"get prob and binairy activation for visual layer\"\"\"\n",
    "        return self.probability(self.activation(y, self.W, self.b))\n",
    "    \n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        \"\"\"update weights for state 0 to k\"\"\"\n",
    "        Wi = torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk) # 1682x100\n",
    "#         print(Wi.shape, self.W.shape)\n",
    "        self.W += Wi.t() # torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIS = len(training_set[0])\n",
    "N_HID = 100\n",
    "N_EPOCH = 20\n",
    "BATCH_SIZE = 100\n",
    "N_WALKS = 15\n",
    "\n",
    "'W.shape:', N_HID, N_VIS\n",
    "\n",
    "rbm = RBM(N_VIS, N_HID)\n",
    "print(rbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "id_ = list(range(nb_users))\n",
    "ran_idx = random.sample(id_, len(id_))\n",
    "batches = range(len(ran_idx)//BATCH_SIZE + 1)\n",
    "loader = [ran_idx[b*BATCH_SIZE:(b+1)*BATCH_SIZE] for b in batches]\n",
    "\n",
    "# Last batch\n",
    "len(list(loader)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on rated movies - exclude unrated movies (ratings with value -1)\n",
    "# loss = train set without blanks - test set without blanks from train set\n",
    "train_loss = 0\n",
    "    \n",
    "for epoch in np.arange(N_EPOCH)+1:\n",
    "    \n",
    "    # average train_loss /users\n",
    "    cum_train_loss = 0\n",
    "    u = 0. \n",
    "    \n",
    "    # batchwise\n",
    "    for b in batches:\n",
    "        vk = training_set[loader[b]]\n",
    "        v0 = training_set[loader[b]] # vk[:] # training_set[id_user:id_user+batch_size]\n",
    "        assert v0 is not vk\n",
    "        ph0, _ = rbm.sample_h(v0)\n",
    "        \n",
    "        # optimize by blind/random walk\n",
    "        # divergence\n",
    "        # get bernoulli \n",
    "        for k in range(N_WALKS):\n",
    "            _, hk = rbm.sample_h(vk)\n",
    "            _, vk = rbm.sample_v(hk)\n",
    "            vk[v0 < 0] = v0[v0 < 0] # do not update vk for unrated movies\n",
    "        \n",
    "        # get converged hidden probs\n",
    "        phk, _ = rbm.sample_h(vk) \n",
    "        \n",
    "        # update weights\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        \n",
    "        # cum train_loss/users\n",
    "        # vk is v_hat or inferred rating of rated movies\n",
    "        # loss = difference in ratings; |0-1|=1, |1-0|=1, |0-0|=0, |1-1|=0\n",
    "        # 25% loss = 1 out of 4 movies are misqualified\n",
    "        cum_train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
    "        train_loss = cum_train_loss/(b+1)\n",
    "        \n",
    "#         u += 1. # number of users to average out the cum loss\n",
    "    print('epoch: {} train loss: {} u: {}'.format(epoch, train_loss, b+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the RBM\n",
    "# test set contains all users and all ratings\n",
    "# train set has same shape as test set, but contains unrated movies for inferrence\n",
    "cum_test_loss = 0\n",
    "u = 0.\n",
    "\n",
    "for id_user in np.arange(nb_users-1):\n",
    "    v = training_set[id_user:id_user+1]\n",
    "    vt = test_set[id_user:id_user+1]\n",
    "    \n",
    "    # infer unrated movies in train set if test set contains rated movies\n",
    "    if len(vt[vt >= 0]) > 0:\n",
    "        _,h = rbm.sample_h(v)\n",
    "        _,v = rbm.sample_v(h)\n",
    "        \n",
    "    # loss = train set with true & inferred ratings -/- test set with all true ratings\n",
    "    u += 1.\n",
    "    cum_test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n",
    "        \n",
    "    print('test loss: ' + str(cum_test_loss.item()/u), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the Stacked Auto Encoder\n",
    "# inherit from Class nn\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, n_hl1, n_hl2, n_hl3):\n",
    "        # initialize nn.Module(super of SAE)\n",
    "        super(SAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(nb_movies, n_hl1)\n",
    "        self.fc2 = nn.Linear(n_hl1, n_hl2)\n",
    "        self.fc3 = nn.Linear(n_hl2, n_hl3)\n",
    "        self.fc4 = nn.Linear(n_hl3, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN\n",
    "sae = SAE(20, 10, 20)\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "def optimizer(name):\n",
    "    opt = {'RMS': 'optim.RMSprop(sae.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0.5)', \n",
    "         'Adam': 'optim.Adam(sae.parameters(), lr=0.01, eps=1e-08, weight_decay=0.25)'}\n",
    "    return eval(opt[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAE\n",
    "N_EPOCH = 50\n",
    "\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) # [torch.FloatTensor of size 1x1682]\n",
    "        target = input.clone()\n",
    "        \n",
    "        # train rated movies only\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False # turn off gradient computation\n",
    "            output[target == 0] = 0 # save computation cost\n",
    "            \n",
    "            # compute (MSE) loss\n",
    "            # adjust trained/rated movies loss to all movies loss\n",
    "            loss = loss_fn(output, target)\n",
    "            mean_adjust = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            train_loss += np.sqrt(loss.item() * mean_adjust)\n",
    "            s += 1.\n",
    "            \n",
    "            # define loss direction and optimizer generator\n",
    "            loss.backward()\n",
    "            optimize = optimizer(\"Adam\")\n",
    "            optimize.step()\n",
    "            \n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
