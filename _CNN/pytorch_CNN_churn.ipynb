{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Import libraries, modules\n",
    "\n",
    "Sequence; core/generic, specific, modules [a-z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from extensive list of imports\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import inspect\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import shutil\n",
    "from subprocess import check_output\n",
    "import sys\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# plot inline\n",
    "%matplotlib inline\n",
    "\n",
    "# set seeds for reproduction\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# interactive mode on\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define data transforms and augmentation before loading\n",
    "\n",
    "Compose augmentation(random), transforms for training and validation/test sets.\n",
    "Set size equal to model \n",
    "Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset\n",
    "data_dir = '.'  # current dir: 'Deep_Learning_A_Z'\n",
    "# print(check_output([\"ls\", data_dir]).decode(\"utf8\"))\n",
    "\n",
    "# Importing the dataset\n",
    "if os.path.exists('Churn_Modelling.csv'):\n",
    "    dataset = pd.read_csv('Churn_Modelling.csv')\n",
    " \n",
    "assert dataset is not None, 'dataframe is loaded'\n",
    "\n",
    "# Generic function to describe inputs of dataset\n",
    "def summary_by_type(dataset):\n",
    "    \"\"\"Describe stats for each input, grouped by dtype\"\"\"\n",
    "    cls_type = {'cat':['object'], 'num':['int64', 'float64' ]}\n",
    "    result = {'cat':None, 'num':None}\n",
    "    for cls in cls_type.keys():\n",
    "        sumry = [[c, dataset[c].describe()] for c in dataset.columns\n",
    "                                            if dataset[c].dtype in cls_type[cls]]\n",
    "        sumry = pd.DataFrame.from_records(sumry)\n",
    "        data = [sumry[1][:][i] for i, _ in enumerate(sumry[1])]\n",
    "        result[cls] = pd.DataFrame(data)\n",
    "    return result\n",
    "\n",
    "# Sanity check\n",
    "print(summary_by_type(dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "X = dataset.iloc[:, 3:13]\n",
    "y = dataset.iloc[:, 13]\n",
    "\n",
    "assert isinstance(X, pd.DataFrame), 'dataframe is loaded'\n",
    "#print(X[:5,],'\\n...\\n', X[-5:])\n",
    "print(X.head(15),y.head(15))\n",
    "print('_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "df_train = X\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "\n",
    "# Make X immutable for hashing (LabelEncoder)\n",
    "if hasattr(X, 'values'):\n",
    "    X = X.values  \n",
    "\n",
    "# instantiate LabelEncoder objects\n",
    "# fit: set levels and transform: set to ints\n",
    "le_X_1 = LabelEncoder()                  \n",
    "X[:, 1] = le_X_1.fit_transform(X[:, 1])  # Geography\n",
    "le_X_2 = LabelEncoder()\n",
    "X[:, 2] = le_X_2.fit_transform(X[:, 2])  # Gender\n",
    "\n",
    "# encode Geography to onehotvector (iso dummyvars?)\n",
    "# # create dummy vars for Geography to avoid ordinal effect on prediction\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]  # remove 1 dummy var (dummy var trap) (need n_levels - 1)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and initializing the ANN\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier = build_classifier(optimizer='adam')\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs=10)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = y_pred > 0.5\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "#\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "acc = (tp+tn)/sum(sum(cm))\n",
    "print(tn, fp, fn, tp)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the ANN\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "\n",
    "parameters = {'batch_size': [8, 16, 32],\n",
    "              'epochs': [10],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build best model for prediction\n",
    "classifier = build_classifier('adam')\n",
    "classifier.fit(X_train, y_train, batch_size = 8, epochs = 20)\n",
    "\n",
    "# Predicting a single new observation\n",
    "\"\"\"Predict if the customer with the following informations will leave the bank:\n",
    "Geography: France\n",
    "Credit Score: 600\n",
    "Gender: Male\n",
    "Age: 40\n",
    "Tenure: 3\n",
    "Balance: 60000\n",
    "Number of Products: 2\n",
    "Has Credit Card: Yes\n",
    "Is Active Member: Yes\n",
    "Estimated Salary: 50000\"\"\"\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction = (new_prediction > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lirnli.wordpress.com/2017/09/03/one-hot-encoding-in-pytorch/\n",
    "# def one_hot(batch, depth):\n",
    "#     ones = torch.sparse.torch.eye(depth)\n",
    "#     return ones.index_select(0,batch)\n",
    "# \n",
    "# t = torch.LongTensor(X[:, 1])\n",
    "# one_hot(t, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All torchvision pre-trained models expect input images normalized in the same way, \n",
    "# i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), \n",
    "# where H and W are expected to be at least 224. \n",
    "# The images have to be loaded into a range of [0, 1] and then normalized using \n",
    "MEAN, SD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Generic function to build transforms\n",
    "def transform_composer(img_size=0, val_size=0, **kwargs):\n",
    "    \"\"\"Build composed data transforms.\n",
    "    \n",
    "    :args: e.g.: img_size=224, val_size=256\n",
    "           kwargs: boolean list of transforms in correct order:\n",
    "           resize=True, c_crop=True, r_crop=True, flip=True, rotate=True, \n",
    "           tensor=True, normalize=True\n",
    "    :return: composed transform\"\"\"\n",
    "    train_dict = {\n",
    "         'r_crop': transforms.RandomResizedCrop(img_size),\n",
    "         'flip': transforms.RandomHorizontalFlip(),\n",
    "         'rotate': transforms.RandomRotation(5),\n",
    "         'tensor': transforms.ToTensor(),      \n",
    "         'normalize': transforms.Normalize(MEAN, SD)\n",
    "    }\n",
    "    val_dict = {\n",
    "         'resize': transforms.Resize(val_size),\n",
    "         'c_crop': transforms.CenterCrop(img_size),\n",
    "         'tensor': transforms.ToTensor(),      \n",
    "         'normalize': transforms.Normalize(MEAN, SD)\n",
    "    }\n",
    "    train_transforms = [train_dict[k] for k, v in kwargs.items() if v and k in train_dict]\n",
    "    val_transforms = [val_dict[k] for k, v in kwargs.items() if v and k in val_dict]\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose(train_transforms),\n",
    "        'val': transforms.Compose(val_transforms)\n",
    "        }\n",
    "    return data_transforms  \n",
    "    \n",
    "# Compose transforms\n",
    "ann_transforms = transform_composer(tensor=True)\n",
    "# Sanity check: list of transforms\n",
    "print(repr(ann_transforms['train'].__dict__))\n",
    "print(repr(ann_transforms['val'].__dict__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check data directory for subfolders and files\n",
    "data_dir = 'churn_data'\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", data_dir]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "        \"\"\"\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\"\"\"\n",
    "        inputs = torch.from_numpy(self.inputs[index])\n",
    "        labels = torch.from_numpy(np.asfarray(self.labels[index]|))\n",
    "        return inputs, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs.index)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders\n",
    "M_BATCH = 4\n",
    "WORKERS = 4\n",
    "PHASES = ['train', 'val']\n",
    "TRANSFORMS = ann_transforms\n",
    "\n",
    "# Split dataset in train and validation sets\n",
    "inputs_train, inputs_val, labels_train, labels_val =\\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "inputs_train = sc.fit_transform(inputs_train)\n",
    "inputs_val = sc.transform(inputs_val)\n",
    "\n",
    "# \n",
    "train_set = MyDataset(inputs_train, labels_train)\n",
    "val_set = MyDataset(inputs_val, labels_val)\n",
    "# dataloaders = DataLoader(dset, batch_size=M_BATCH, shuffle=True, num_workers=WORKERS)\n",
    "dset = {'train': train_set, 'val': val_set}\n",
    "train_set[:5]\n",
    "# dataloaders\n",
    "\n",
    "\n",
    "# dataset_sizes = {x: len(dset[x]) for x in ['train', 'val']}\n",
    "# dataset_sizes\n",
    "# train = TensorDataset(X_train, y_train)\n",
    "# val = TensorDataset(X_test, y_test)\n",
    "# \n",
    "# image_datasets = {'train': train, 'val': val}\n",
    "# \n",
    "# dataloaders = {x: data.DataLoader(dset[x], batch_size=M_BATCH,\n",
    "#                                   shuffle=True, num_workers=WORKERS)\n",
    "#                                   for x in PHASES}\n",
    "# dataloaders['train'][0]\n",
    "# next(iter(dataloaders['train']))\n",
    "\n",
    "\n",
    "\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                           TRANSFORMS[x])\n",
    "#                                           for x in PHASES}\n",
    "# \n",
    "# dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=M_BATCH,\n",
    "#                                   shuffle=True, num_workers=WORKERS)\n",
    "#                                   for x in PHASES}\n",
    "\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "#class_names = image_datasets['train'].classes\n",
    "\n",
    "# Sanity check images, class labels\n",
    "# print((repr(image_datasets['train'].__dict__))[:500])\n",
    "# print(dataset_sizes)\n",
    "#print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image viewer function\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Show images of Tensors in Dataloader.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0)) # convert to np\n",
    "    inp = (SD * inp) + MEAN\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Build model \n",
    "or \n",
    "Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.lin1 = nn.Linear(11, 6)\n",
    "        self.lin2 = nn.Linear(6, 6)\n",
    "        self.lin3 = nn.Linear(6, 1)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = F.relu(self.lin1(inputs))\n",
    "        out = F.relu(self.lin2(out))\n",
    "        out = F.sigmoid(self.lin3(out))\n",
    "        return out\n",
    "        \n",
    "\n",
    "model = ANN()\n",
    "print(model)\n",
    "# or  \n",
    "# ANN_ = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(D_in, H),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(H, D_out),\n",
    "# )      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network Architecture Class\n",
    "# IF NOT PRELOADED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Pytorch pretrained models\n",
    "# model_names = sorted(name for name in models.__dict__\n",
    "#                      if name.islower() \n",
    "#                      and not name.startswith(\"__\")\n",
    "#                      and callable(models.__dict__[name]))\n",
    "# print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to set/define pretrained model\n",
    "# def pre_model(model, pretrained=True, freeze=True):\n",
    "#     \"\"\"\"\"\"\n",
    "#     model = models.__dict__[model](pretrained=pretrained)\n",
    "#     # freeze parameters in backprop\n",
    "#     if freeze:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     num_ftrs = model.fc.in_features    # no. of features in fc layer\n",
    "#     model.fc = nn.Linear(num_ftrs, 2)  # change out_features to 2 (binary loss)\n",
    "#     return model\n",
    "# \n",
    "# \n",
    "# # Define/set model\n",
    "# model_all = pre_model('resnet18', pretrained=True, freeze=False)\n",
    "# model_freeze = pre_model('resnet18', pretrained=True, freeze=True)\n",
    "# \n",
    "# # Sanity check: show model and architecture change\n",
    "# print(model_all)\n",
    "# print(models.resnet18(pretrained=True).fc)\n",
    "# print(model_all.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer functions\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DECAY_STEP = 7  # epoch steps between LR decay\n",
    "DECAY_LR = 0.1\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer functions\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Decay LR\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing model, load and resume\n",
    "def load_model(model, optimizer, num_epochs, resume=True):\n",
    "    \"\"\"Load and resume from existing model.\n",
    "    :return: model path\"\"\"\n",
    "    model_name = os.path.join(data_dir,\n",
    "                              str(model.__class__.__name__)+'_'+\n",
    "                              str(optimizer.__class__.__name__)+'_'+\n",
    "                              str(num_epochs)+'.pk1')\n",
    "    if os.path.exists(model_name) and resume:\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "    return model_name\n",
    "\n",
    "# Sanity check: path\n",
    "load_model(model_all, optimizer_all, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic train helper functions\n",
    "\n",
    "\n",
    "def b_ward(loss, optimizer, scheduler):\n",
    "    \"\"\"Backpropagate loss.\"\"\"\n",
    "    optimizer.zero_grad()   # reset gradients\n",
    "    loss.backward()         # backprop loss\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "\n",
    "def f_ward(model, phase, criterion, inputs, labels):\n",
    "    \"\"\"Forward pass.\n",
    "    \n",
    "    http://pytorch.org/docs/master/notes/autograd.html#volatile\n",
    "    :return: loss and accuracy\n",
    "    \"\"\"\n",
    "    inputs = Variable(inputs, volatile=(phase == 'val'), requires_grad=(phase == 'train'))\n",
    "    labels = Variable(labels)\n",
    "    \n",
    "    # Compute loss and predict label(max log-probability)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    acc = torch.sum(preds == labels.data)\n",
    "    return loss, acc\n",
    "\n",
    "    \n",
    "def train(model, loader, scheduler, criterion, optimizer, phase):\n",
    "    \"\"\"Training, validation for each epoch. Forward, backward props and caching metrics.\n",
    "    \n",
    "    :return: loss and accuracy\"\"\"\n",
    "    model.train(phase == 'train')\n",
    "    cache = {'cum_count': 0, 'cum_loss': 0.0, 'cum_acc': 0.0, \n",
    "             'avg_loss': 0.0, 'avg_acc': 0.0}\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        \n",
    "        # forward\n",
    "        loss, acc = f_ward(model, phase, criterion, inputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        if phase == 'train':\n",
    "            b_ward(loss, optimizer, scheduler)\n",
    "            \n",
    "        # stats\n",
    "        cache['cum_count'] += inputs.size()[0]\n",
    "        cache['cum_loss'] += loss.data[0]\n",
    "        cache['cum_acc'] += acc\n",
    "        cache['avg_loss'] = cache['cum_loss']/cache['cum_count']\n",
    "        cache['avg_acc'] = cache['cum_acc']/cache['cum_count']\n",
    "    return cache['avg_loss'], cache['avg_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function for training and evaluation of validation set\n",
    "def eval_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \"\"\"Running training and validation.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Load last best model saved\n",
    "    model_name = load_model(model, optimizer, num_epochs, resume=True)\n",
    "    print(model_name)\n",
    "    best_model = {'model': model_name, 'best_acc': 0.0, 'best_model_wts': model.state_dict()}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #lap = time.time()\n",
    "        print_header()\n",
    "    \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            lap = time.time()\n",
    "            loss, acc = train(model, dataloaders[phase], scheduler, criterion, optimizer, phase)\n",
    "            \n",
    "            # update LR decay\n",
    "            if phase == 'val':\n",
    "                scheduler.step(loss)\n",
    "            # update and save best_model\n",
    "            if phase == 'val' and acc > best_model['best_acc']:\n",
    "                best_model['best_acc'], best_model['best_model_wts'] = acc, model.state_dict()\n",
    "                torch.save(model.state_dict(), best_model['model'])\n",
    "                \n",
    "            end = time.time()\n",
    "            print_stat(phase, epoch, loss, acc, end-lap)\n",
    "            \n",
    "    finish = time.time()            \n",
    "    print_model_performance(finish-start, best_model)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model['best_model_wts'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic print helper functions\n",
    "\n",
    "\n",
    "# Helper functions for printing stats\n",
    "def time_format(secs):\n",
    "    \"\"\"Convert seconds to h:mm:ss.\"\"\"\n",
    "    m, s = divmod(secs, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "\n",
    "def print_header():\n",
    "    \"\"\"Print header.\"\"\"\n",
    "    h_template = \"\"\"{:8}\\t\\t {:8}\\t\\t    {:12}\\t {:8}\\t\\t {:8}\"\"\"\n",
    "    print()\n",
    "    print(h_template.format('Phase', 'Epoch', 'Loss', 'Accurracy', 'Duration'))\n",
    "       \n",
    "       \n",
    "def print_stat(phase, epoch, loss, acc, duration):\n",
    "    \"\"\"Print loss, accuracy and duration at each epoch/phase.\"\"\"\n",
    "    p_template = \"\"\"{:8}\\t\\t {:8}\\t\\t {:8.4f}\\t\\t    {:8.1f}\\t\\t {:8}\"\"\"\n",
    "    print(p_template.format(phase, epoch, loss, acc*100, time_format(duration)))\n",
    "    \n",
    "        \n",
    "def print_model_performance(duration, best_model):\n",
    "    \"\"\"Print best model performance and total duration.\"\"\"\n",
    "    print('Training and validation complete in: {:8}\\n'\n",
    "          'Best validation Accuracy: {:2.1f}%\\n'\n",
    "          'Learned model saved: {:16}\\n'.format(\n",
    "           time_format(duration), round(best_model['best_acc']*100), 2), best_model['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "# Train and evaluate validation set\n",
    "model_all = eval_model(model, criterion, optimizer, \n",
    "                       exp_lr_scheduler, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluate performance\n",
    "\n",
    "Tune hypermparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict per batch\n",
    "def pred_batch(model):\n",
    "    \"\"\"Predict labels for one batch\"\"\"\n",
    "    inputs, labels = next(iter(dataloaders['val']))\n",
    "    v_inputs, v_labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "    outputs = model(v_inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return zip(inputs, preds, labels)\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "def show_pred_batch(model, n_batches, n_columns=M_BATCH):\n",
    "    \"\"\"Show from n batches n predictions\"\"\"\n",
    "    for _ in range(n_batches):\n",
    "        it_batch = list(pred_batch(model))\n",
    "        title = [(class_names[yhat], (yhat == y)) \n",
    "                  for _, yhat, y in it_batch][:n_columns]\n",
    "        inputs = ([input for input, _, _ in it_batch])[:n_columns]\n",
    "    \n",
    "        # Make a grid from batch\n",
    "        out = torchvision.utils.make_grid(inputs, padding=10)\n",
    "        imshow(out, title)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Show predictions\n",
    "show_pred_batch(model, 5, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
