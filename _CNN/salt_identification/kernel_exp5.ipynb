{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog <a name='A'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "# csum\n",
    "2 .  import imgaug\n",
    "https://arxiv.org/pdf/1706.05587.pdf\n",
    "\n",
    "nadam . = 0.38!\n",
    "adam, 30pochs score = 0.38! - adam werkt niet goed?\n",
    "DEPTH 32x32 = 0.756 (40 epochs)\n",
    "DEPTH 32x32 = 0.753 (10 epochs) . \n",
    "DEPTH 64x64 = 0.736\n",
    "\n",
    "#\n",
    "batch=32, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "/// {'adam': Adam(lr=LR, beta_1=0.9, beta_2=0.999, clipnorm=8.),\n",
    "test score=0.763\n",
    "\n",
    "#\n",
    "batch=48, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "'rmsprop': RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "test score=0.762\n",
    "\n",
    "#\n",
    "batch=16, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "test score=0.753\n",
    "\n",
    "#\n",
    "batch=24, rmsprop, patience=11, lrdecay=4, val_loss  \n",
    "test score=0.764\n",
    "\n",
    "#\n",
    "batch =32\n",
    "metric = val_miou\n",
    "test score=0.745\n",
    "\n",
    "#\n",
    "UP_CONV = False\n",
    "RES = True  \n",
    "'adadelta'\n",
    "\n",
    "21/aug/2018  \n",
    " - initial commit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents: <a name='TOC'></a>\n",
    "[Changelog](#A)  \n",
    "[Sources](#B)  \n",
    "[About](#C)  \n",
    "[U-Net model](#D)  \n",
    "[Setup notebook](#E)  \n",
    "[Data Exploration](#F)  \n",
    "[Visualize images and masks (overlayed)](#G)  \n",
    "[Train validation split](#H)  \n",
    "[Custom metrics](#I)  \n",
    "[Model architecture](#J)  \n",
    "[Build model](#K)  \n",
    "[Train model](#L)  \n",
    "[Learning curves](#M)  \n",
    "[Check performance on validation set](#N)  \n",
    "[Scoring](#O)  \n",
    "[Post-processing: blur and threshold](#Q)   \n",
    "[Predict test set](#P)   \n",
    "[Submission](#R)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources that earn credits <a name='B'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "I got initial inspiration and insights for this notebook from:  \n",
    "\n",
    "[Alexander Liao](https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice/notebook)  -   Model architecture , dice loss  \n",
    "[Jesper Dramsch](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)   - Seismic intro  \n",
    "[Bruno G, do Amaral](https://www.kaggle.com/bguberfain/unet-with-depth)   - Depth  and cumsum  \n",
    "[ABE_](https://www.kaggle.com/jfr311/crf-analysis) - CRF  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "(*Jesper Dramsch*)<a name='C'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Seismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n",
    "\n",
    "Imaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n",
    "\n",
    "Seismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\n",
    "You'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n",
    "\n",
    "[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\n",
    "Caption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n",
    "\n",
    "Interpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n",
    "\n",
    "One common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n",
    "\n",
    "Recently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n",
    "\n",
    "+ https://github.com/waldeland/CNN-for-ASI\n",
    "+ https://github.com/bolgebrygg/MalenoV\n",
    "+ https://github.com/crild/facies_net\n",
    "\n",
    "You will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net model<a name='D'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "The seismic images resemble X-rays and Ultrasound scans. The U-Net model is a CNN used for Biomedical Image Segmentation.   \n",
    "\n",
    "Please read the paper: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\"> U-Net: Convolutional Networks for Biomedical Image Segmentation</a>) \n",
    "\n",
    "<a href=\"https://github.com/jocicmarko/ultrasound-nerve-segmentation\">Another interesting read</a> from the Kaggle Ultrasound Nerve Segmentation competition.</p>\n",
    "\n",
    "<p><img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setup notebook<a name='E'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Commit/Development mode\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 5\n",
    "COMMIT = True\n",
    "DEV = not COMMIT\n",
    "DBG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_used():\n",
    "    \"\"\"Memory used\"\"\"\n",
    "    import resource\n",
    "    return round(resource.getrusage(resource.RUSAGE_SELF)[2] * 10/1028 / 10, 1)\n",
    "\n",
    "def mem_fun(fun, **kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    mem_start = mem_used()\n",
    "    _ = fun(**kwargs)\n",
    "    print(f'memory used by function: {(mem_used() - mem_start):.1f}mb')\n",
    "\n",
    "def test_alloc_size(n, m, c):\n",
    "    \"\"\"Check memory allocation needed for train/test set\"\"\"\n",
    "    imgs_test_alloc = list(range(n))\n",
    "    for i in range(n):\n",
    "        imgs_test_alloc[i] = np.ones((m, TGT_SIZE, TGT_SIZE, c)) * random.randint(0,100)\n",
    "    return imgs_test_alloc\n",
    "    \n",
    "if DBG:\n",
    "    mem_fun(test_alloc_size, n=1, m=18000, c=2)\n",
    "    print(f'notebook memory used: {mem_used()}mb')\n",
    "    ## memory used by function: 4482.3mb for (1800, 128, 1288, 2)\n",
    "    ## notebook memory used: 4805.7mb\n",
    "\n",
    "nb_mem = mem_used()\n",
    "f'Initial memory used by this notebook: {nb_mem}mb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "from memory_profiler import profile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "import ipywidgets as ipy\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, AtrousConvolution2D\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Nadam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set file locations\n",
    "For convenience when working from different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.'\n",
    "path = '../input'\n",
    "path_train = f'{path}/train'\n",
    "path_test = f'{path}/test'\n",
    "imgs_train = f'{path}/train/images'\n",
    "masks_train = f'{path}/train/masks'\n",
    "imgs_test = f'{path}/test/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path names\n",
    "train_img_filepaths = sorted(glob.glob(f'{imgs_train}/*'))\n",
    "train_mask_filepaths = sorted(glob.glob(f'{masks_train}/*'))\n",
    "test_img_filepaths = sorted(glob.glob(f'{imgs_test}/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipy.interact(idx=ipy.IntSlider(min=1, max=len(train_img_filepaths)), value=10, step=1)\n",
    "def plot(idx):\n",
    "    \"\"\"Navigate thru images and masks\"\"\"\n",
    "    img = np.array(load_img(train_img_filepaths[idx], grayscale=True))\n",
    "    mask = np.array(load_img(train_mask_filepaths[idx], grayscale=True))\n",
    "    fig, axs = plt.subplots(1,2, figsize=(20,10))\n",
    "    axs[0].imshow(img, cmap=\"Greys\")\n",
    "    axs[1].imshow(mask, cmap=\"Greens\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/61720\n",
    "BAD_MASKS =[\n",
    "    '1eaf42beee','33887a0ae7','33dfce3a76','3975043a11','39cd06da7d','483b35d589','49336bb17b','4ef0559016',\n",
    "    '4fbda008c7','4fdc882e4b','50d3073821','53e17edd83','5b217529e7','5f98029612','608567ed23','62aad7556c',\n",
    "    '62d30854d7','6460ce2df7','6bc4c91c27','7845115d01','7deaf30c4a','80a458a2b6','81fa3d59b8','8367b54eac',\n",
    "    '849881c690','876e6423e6','90720e8172','916aff36ae','919bc0e2ba','a266a2a9df','a6625b8937','a9ee40cf0d',\n",
    "    'aeba5383e4','b63b23fdc9','baac3469ae','be7014887d','be90ab3e56','bfa7ee102e','bfbb9b9149','c387a012fc',\n",
    "    'c98dfd50ba','caccd6708f','cb4f7abe67','d0bbe4fd97','d4d2ed6bd2','de7202d286','f0c401b64b','f19b7d20bb',\n",
    "    'f641699848','f75842e215','00950d1627','0280deb8ae','06d21d76c4','09152018c4','09b9330300','0b45bde756',\n",
    "    '130229ec15','15d76f1672','182bfc6862','23afbccfb5','24522ec665','285f4b2e82','2bc179b78c','2f746f8726',\n",
    "    '3cb59a4fdc','403cb8f4b3','4f5df40ab2','50b3aef4c4','52667992f8','52ac7bb4c1','56f4bcc716','58de316918',\n",
    "    '640ceb328a','71f7425387','7c0b76979f','7f0825a2f0','834861f1b6','87afd4b1ca','88a5c49514','9067effd34',\n",
    "    '93a1541218','95f6e2b2d1','96216dae3b','96523f824a','99ee31b5bc','9a4b15919d','9b29ca561d','9eb4a10b98',\n",
    "    'ad2fa649f7','b1be1fa682','b24d3673e1','b35b1b412b','b525824dfc','b7b83447c4','b8a9602e21','ba1287cb48',\n",
    "    'be18a24c49','c27409a765','c2973c16f1','c83d9529bd','cef03959d8','d4d34af4f7','d9a52dc263','dd6a04d456',\n",
    "    'ddcb457a07','e12cd094a6','e6e3e58c43','e73ed6e7f2','f6e87c1458','f7380099f6','fb3392fee0','fb47e8e74e','febd1d2a67']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set/initiate image constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 101   # original/raw image size\n",
    "TGT_SIZE = 128   # tunable hyperparam: model/input image size {128, 256, 512} - for this 14gb RAM kernel 128 is max. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def upsample(img, dim=TGT_SIZE, interpolation=cv2.INTER_LINEAR):\n",
    "    \"\"\"Resize image to target shape(model_input) or back to original shape\n",
    "    \n",
    "    INTER_NEAREST - a nearest-neighbor interpolation\n",
    "    INTER_LINEAR - a bilinear interpolation (used by default)\n",
    "    INTER_AREA - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moire’-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.\n",
    "    INTER_CUBIC - a bicubic interpolation over 4x4 pixel neighborhood\n",
    "    INTER_LANCZOS4 - a Lanczos interpolation over 8x8 pixel neighborhood\n",
    "    To shrink an image, it will generally look best with CV_INTER_AREA interpolation, whereas to enlarge an image, \n",
    "    it will generally look best with CV_INTER_CUBIC(slow) or CV_INTER_LINEAR(faster but still looks OK).\n",
    "    \"\"\"\n",
    "    if img.shape[0] == dim:\n",
    "        return img\n",
    "    return cv2.resize(img, (dim, dim), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def adaptive_gaus_thres(img, n=5, m=0):\n",
    "    \"\"\"Adaptive Gaussian thresholding\n",
    "    cv2.threshold requires a 1 layer uint8 image\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, n, m)\n",
    "\n",
    "\n",
    "def adaptive_mean_thres(img, n=5, m=0):\n",
    "    \"\"\"Adaptive Mean thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.adaptiveThreshold(img, 1, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, n, m)\n",
    "\n",
    "\n",
    "def otsu_thres(img, n=0, m=255):\n",
    "    \"\"\"Otsu thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "def gaus_thres(img, n=0, m=255):\n",
    "    \"\"\"Gaussian thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.threshold(img, n, m, cv2.ADAPTIVE_THRESH_GAUSSIAN_C)[1]\n",
    "\n",
    "\n",
    "def plot_mask_thres(fn, img, n, m, cmap='seismic', title=''):\n",
    "    \"\"\"Plot thresholding images\"\"\"\n",
    "    plt.imshow(fn(img, n, m), cmap=cmap); plt.title(title)\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_masks(imgs, masks, **kwargs):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n",
    "    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n",
    "    Arguments:\n",
    "    imgs: vector(-1, h, w, ch)\n",
    "    masks: vector(-1, h, w, ch)\n",
    "    \"\"\"\n",
    "    depth = kwargs.get('depth', None)\n",
    "    coverage = kwargs.get('coverage', None)\n",
    "    cov_class = kwargs.get('cov_class', None)\n",
    "    preds_valid = kwargs.get('preds_valid', None)\n",
    "    thres = kwargs.get('thres', None)\n",
    "    grid_width = kwargs.get('grid_width', 10)\n",
    "    zoom = kwargs.get('zoom', 1.5)\n",
    "    \n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, (img, mask) in enumerate(zip(imgs, masks)):\n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            pred = preds_valid[i]\n",
    "            pred = pred[..., 0]\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(pred > thres), dtype=np.float32)\n",
    "                iou = f'IoU: {IoU(mask, pred)}'\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "                _ = ax.text(2, img.shape[0]-2, iou, color=\"k\")\n",
    "            else:\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "            \n",
    "        if depth is not None:\n",
    "            _ = ax.text(2, img.shape[0]-2, f'depth: {depth[i].round(3)}', color=\"k\")\n",
    "        if (coverage is not None) and (cov_class is not None):   \n",
    "            _ = ax.text(2, 2, f'{coverage[i].round(3)}({cov_class[i]})', color=\"k\", ha=\"left\", va=\"top\")\n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration<a name='F'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Load and build train and test set.\n",
    "\n",
    " - contemplating to use depth as a layer, this explains the stacking in `images_d`.\n",
    " - checking depth\n",
    " - checking coverage\n",
    " - checking depth - coverage relationship\n",
    " - visualize seismic images with masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv files\n",
    "\n",
    "These files contain image filenames(without .png) and seismic depths of the images.  \n",
    " - train.csv - filenames of train set  \n",
    " - depths.csv - depths of all images test & train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(f'{path}/train.csv', index_col=\"id\", usecols=[0])\n",
    "depths_df_ = pd.read_csv(f'{path}/depths.csv', index_col=\"id\") # train and test\n",
    "train_df_ = train_df_.join(depths_df_)\n",
    "test_df = depths_df_[~depths_df_.index.isin(train_df_.index.values)]\n",
    "\n",
    "# Indices\n",
    "train_indices = train_df_.index.values\n",
    "test_indices = test_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some RAM\n",
    "del depths_df_\n",
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image and mask vectors\n",
    "\n",
    "1. check for NULL/white images and filter out if any.  \n",
    "2. load all images and masks\n",
    "3. augment images/masks:\n",
    "  - flip all masks (coverage > 0)\n",
    "4. merge original and augmented images/masks\n",
    "5. sanity checks on flip and merge\n",
    "6. convert dictionary to arrays\n",
    "7. sanity check array shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    empty_imgs = {name: (\n",
    "                  upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE) / 255,\n",
    "                  upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255,\n",
    "                  train_df_.loc[name, 'z'])\n",
    "                  for name in tqdm_notebook(train_indices) if np.sum(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)))==0}\n",
    "\n",
    "    print(f'number of empty/white images: {len(empty_imgs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Visual sanity check white images and corresponding white masks\n",
    "    for img, tup in empty_imgs.items():\n",
    "        _ = plt.imshow(tup[0], cmap=\"Greys\")\n",
    "        _ = plt.imshow(tup[1], alpha=.3, cmap=\"Greens\")\n",
    "        _ = plt.title(f'White image: {img}, depth: {tup[2]}')\n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(imgs, dim=64):\n",
    "    \"\"\"\"\"\"\n",
    "    return [resize(img, (dim, dim), mode='constant', preserve_range=True) for img in imgs]\n",
    "\n",
    "def img_func(images, random_state, parents, hooks):\n",
    "    \"\"\"Lambda cumsum transform\"\"\"\n",
    "    for i, img in enumerate(images):\n",
    "        images[i] = (np.float32(img) - img.mean()).cumsum(axis=0)\n",
    "    return images\n",
    "\n",
    "def keypoint_func(keypoints_on_images, random_state, parents, hooks):\n",
    "    return keypoints_on_images\n",
    "\n",
    "def augment_xy(X, Y, dim=128):\n",
    "    \"\"\"Augment X and Y same way\"\"\"\n",
    "    # Augmenting\n",
    "    # https://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    seq = iaa.Sequential([\n",
    "#         iaa.Scale(dim), # scales to (dim, dim)\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of images\n",
    "        iaa.OneOf([\n",
    "            iaa.Sometimes(0.5, iaa.Noop()), # iaa.Lambda(img_func, keypoint_func)), returns negative values on masks\n",
    "            iaa.SomeOf(2, [\n",
    "            iaa.Noop(),\n",
    "            iaa.Noop(),\n",
    "            iaa.GaussianBlur(sigma=(0.0, 2.0)),\n",
    "            iaa.Affine(rotate=(-10, 10), translate_percent={\"x\": (-0.25, 0.25)},  mode='symmetric',  cval=(0)),\n",
    "            iaa.PiecewiseAffine(scale=(0.02, 0.06), mode='edge', cval=(0)),\n",
    "        ]) \n",
    "                  ]),\n",
    "        # More as you want ...\n",
    "    ])\n",
    "    \n",
    "    # Convert the stochastic sequence of augmenters to a deterministic one.\n",
    "    # The deterministic sequence will always apply the exactly same effects to the images.\n",
    "    seq_det = seq.to_deterministic()\n",
    "    \n",
    "    X_aug = np.array(seq_det.augment_images(X))[...,np.newaxis]\n",
    "#     print(X_aug[:, :64, :64, :].shape)\n",
    "#     X_aug2 = np.array(downsample(seq_det.augment_images(X)))[...,np.newaxis]\n",
    "    Y_aug = np.array(seq_det.augment_images(Y))[...,np.newaxis]\n",
    "    X_dict = {'img': X_aug, \n",
    "              'csum': X_aug[:, :dim, :dim, :], # np.ones(len(X)*dim*dim).reshape(-1, dim, dim, 1) * 0.001,\n",
    "              'feat': np.ones(len(X)).reshape(-1, 1) * 0.01 * np.random.normal(0, 0.1, 1)}\n",
    "    return X_dict, Y_aug\n",
    "        \n",
    "def data_gen(batch=BATCH_SIZE, indices=train_indices, dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train):\n",
    "    \"\"\"Unlimited Stochastic Augmentation\"\"\"\n",
    "    while True:\n",
    "#         rand_indices = random.sample(indices, batch)    # limited, no replacement (batch <= len(indices))\n",
    "        rand_indices = np.random.choice(indices, batch) # unlimited with replacement - stochastic\n",
    "        X = [upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        Y = [upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        yield augment_xy(X, Y, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = next(data_gen(batch=100, indices=train_indices.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train))\n",
    "X_, Y_ = XY[0]['img'], XY[1]\n",
    "# Show image/mask\n",
    "plot_imgs_masks(X_, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(Y_true, Y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean(IoU) metric\n",
    "    IoU = intersection / union\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - convert probability mask to boolean mask based on given threshold\n",
    "     - score the mask 1 if(IoU > score_threshold(0.5))\n",
    "    Take the mean of the scoress\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n",
    "        \n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            score = tf.identity(score) #!! use identity to transform score to tensor\n",
    "        prec.append(score) \n",
    "        \n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model block - helpers\n",
    "def conv_block(m, ch_dim, acti, bn, res, do=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    # http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review - dilation explained\n",
    "    # https://arxiv.org/pdf/1706.05587.pdf - DeepLab\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=1)(n)\n",
    "    o = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=3)(n) \n",
    "#     p = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=5)(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "#     return Concatenate()([m, n, o, p]) if res else n\n",
    "    return Concatenate()([m, n, o]) if res else n\n",
    "\n",
    "def input_feature(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    f_repeat = RepeatVector(xx*xx)(f)\n",
    "    f_conv = Reshape((xx, xx, n_features))(f_repeat)\n",
    "    n = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n\n",
    "\n",
    "def input_csum(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    n = Concatenate(axis=-1, name=f'csum_{2}')([n, f])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_block(m, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum):\n",
    "    \"\"\"Recursive CNN builder\"\"\"\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, ch_dim, acti, bn, res) # add/remove drop-out: ,do\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n",
    "        # Input csum at 3rd layer (32x32)\n",
    "        if (inp_csum is not None) and (depth==DEPTH):\n",
    "            m = Concatenate()([m, input_csum(inp_csum, m)])\n",
    "        # Input depth\n",
    "        if (inp_feat is not None) and (depth==1):\n",
    "            m = Concatenate()([m, input_feature(inp_feat, m)])\n",
    "        m = level_block(m, int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum)\n",
    "        \n",
    "        # Unwind recursive stack calls - creating the upscaling part of the model\n",
    "        if up:\n",
    "            # Repeat the rows and columns of the data by 2 and 2 respectively - untrainable like reverse pooling\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            # Transposed convolutions are going in the opposite direction of a normal convolution - trainable\n",
    "            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, ch_dim, acti, bn, res)\n",
    "    else:\n",
    "        # Depth == 0 - deepest conv_block\n",
    "        m = conv_block(m, ch_dim, acti, bn, res, do) # add/remove drop-out: ,do\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n",
    "    \"\"\"Returns model\"\"\"\n",
    "    inputs = Input(shape=img_shape, name='img')\n",
    "    inp_csum = None # Input(shape=(64, 64,1), name='csum') ###\n",
    "    inp_feat = None # Input(shape=(1,), name='feat') # or None\n",
    "    outputs = level_block(inputs, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, inp_feat, inp_csum)\n",
    "    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "#     return Model(inputs=[inputs, inp_csum, inp_feat], outputs=outputs)\n",
    "    return Model(inputs=[inputs], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CH = 1                  # X_train.shape[-1]  # layers of image\n",
    "CONV_CH = 16                # number of channels to start/end UNet with\n",
    "DEPTH = 4                   # number of CONV blocks to max model depth - outstride 8: 128/8 = 16(depth=4), outstride 16: 128/16 = 8(depth=3), depth 3 doesnot work well\n",
    "D_OUT = 0.2                 # small random effect can be effective against overfitting. \n",
    "                            # Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n",
    "BN = True\n",
    "UP_CONV = False             # UPSampling avoids coarse checkerboard effect in images\n",
    "RES = True                  # residuals skip layers for deep networks (gradient evaporation)\n",
    "\n",
    "model = UNet((TGT_SIZE, TGT_SIZE, IMG_CH), \n",
    "             start_ch=CONV_CH, \n",
    "             depth=DEPTH, \n",
    "             dropout=D_OUT,\n",
    "             batchnorm=BN, \n",
    "             upconv=UP_CONV,\n",
    "             residual=RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "# Define optimizer\n",
    "# Clip gradients to norm 1., \n",
    "optimizer = {'adam': Adam(lr=LR, beta_1=0.9, beta_2=0.999, clipnorm=8.),\n",
    "             'sgd': SGD(lr=LR, decay=LR/100, momentum=0.99, nesterov=True, clipnorm=5.),\n",
    "             'rmsprop': RMSprop(lr=LR, rho=0.9, epsilon=None, decay=0.0),\n",
    "             'adadelta': Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
    "             'nadam': Nadam(lr=LR, beta_1=0.9, beta_2=0.999)} # , epsilon=None, schedule_decay=0.004\n",
    "opt = 'nadam'\n",
    "\n",
    "# Define loss\n",
    "loss = [\"binary_crossentropy\", \"kullback_leibler_divergence\"]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss[0], optimizer=optimizer[opt], metrics=[\"accuracy\", mean_iou]) #accuracy\n",
    "\n",
    "model_name = f'TGS_UNet_{opt}_CH-{IMG_CH}-{CONV_CH}_D-{DEPTH}_DO-{D_OUT>0}_BN-{BN}_UP-{UP_CONV}_RES-{RES}.h5'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=15, verbose=1), # bias-variance tradeof\n",
    "    ReduceLROnPlateau(patience=4, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, verbose=1)] # continue/save based on monitor metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train val indices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_ids, val_ids = train_test_split(train_indices, test_size=0.2, random_state=1) #TODO, stratify=classes)\n",
    "\n",
    "# First fit\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "history1 = model.fit_generator(data_gen(batch=BATCH_SIZE, indices=train_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              epochs=EPOCHS,\n",
    "                              steps_per_epoch=len(train_ids) //BATCH_SIZE,\n",
    "                              validation_data=data_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              validation_steps=len(val_ids) //BATCH_SIZE,\n",
    "                              verbose=1, \n",
    "                              callbacks=callbacks,\n",
    "                              workers=-1)    \n",
    "\n",
    "# Second fit - can change the augmentation, batch size\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_2 = 60 # total no of epochs incl. first training session\n",
    "history2 = model.fit_generator(data_gen(batch=BATCH_SIZE, indices=train_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              initial_epoch=EPOCHS,  # resume from earlier training\n",
    "                              epochs=EPOCHS_2,\n",
    "                              steps_per_epoch=len(train_ids) //BATCH_SIZE,\n",
    "                              validation_data=data_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              validation_steps=len(val_ids) //BATCH_SIZE,\n",
    "                              verbose=1, \n",
    "                              callbacks=callbacks,\n",
    "                              workers=-1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "for history in [history1, history2]:\n",
    "    fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(20,8))\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    _ = ax_loss.legend()\n",
    "    _ = ax_loss.set_title('Loss')\n",
    "\n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "    _ = ax_acc.legend()\n",
    "    _ = ax_acc.set_title('Accuracy')\n",
    "\n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")\n",
    "    _ = ax_iou.legend()\n",
    "    _ = ax_iou.set_title('IoU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (non-empty) images and masks\n",
    "# Keep empty images as test set also has empty images\n",
    "train_set = {name: (\n",
    "              upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE) / 255,                          # image vector 128**\n",
    "              upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255,                        # mask vector 128**\n",
    "              csum(upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE//2) / 255),                 # cumsum image vector 32**\n",
    "              upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE//2) // 255,                     # mask vector 32**\n",
    "              train_df_.loc[name, 'z'],                                                                                          # depth\n",
    "              np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2,  # salt coverage\n",
    "              (np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2  # salt coverage class\n",
    "                  - .01) * 100//10 + 1)\n",
    "              for name in tqdm_notebook(train_indices)} # if np.sum(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True))) > 0}\n",
    "\n",
    "n_train_samples = len(train_set.keys())\n",
    "print(f'Number of training samples: {n_train_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non-empty flip images and masks\n",
    "train_flip = {f'{name}_': (\n",
    "              np.fliplr(upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE) / 255),               # flipped image vector\n",
    "              np.fliplr(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255),             # flipped mask vector\n",
    "              csum(np.fliplr(upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE//2) / 255)),      # csum flipped image vector\n",
    "              np.fliplr(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE//2) // 255),          # csum flipped mask vector\n",
    "              train_df_.loc[name, 'z'],                                                                                          # depth\n",
    "              np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2,  # salt coverage\n",
    "              (np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2  # salt coverage class\n",
    "                  - .01) * 100//10 + 1)\n",
    "              for name in tqdm_notebook(train_indices) if np.sum(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True))) > 0}\n",
    "\n",
    "n_flip_samples = len(train_flip.keys())\n",
    "print(f'Number of flipped training samples: {n_flip_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train set with flips\n",
    "train_dict = {**train_set, **train_flip}\n",
    "\n",
    "# Sanity check 1 entry\n",
    "if DEV:\n",
    "    print(train_dict[list(train_dict)[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Visual sanity check flipped images and corresponding depths, coverages\n",
    "    i, max_i = 0, 5\n",
    "    fig, (ax1, ax2) = plt.subplots(2,max_i, figsize=(20,10))\n",
    "    for img, tup in train_dict.items():\n",
    "        if img[:-1] in list(train_dict):\n",
    "            _ = ax1[i].imshow(tup[0], cmap=\"Greys\")\n",
    "            _ = ax1[i].imshow(tup[1], alpha=.3, cmap=\"Greens\")\n",
    "            _ = ax1[i].set_title(f'Flipped: {img}, \\ndepth: {tup[4]} \\ncoverage: {tup[5]} \\ncoverage class: {tup[6]}'); \n",
    "            _ = ax1[i].axis('off')\n",
    "\n",
    "            _ = ax2[i].imshow(train_dict[img[:-1]][0], cmap=\"Greys\")\n",
    "            _ = ax2[i].imshow(train_dict[img[:-1]][1], alpha=.3, cmap=\"Greens\")\n",
    "            _ = ax2[i].set_title(f'Original: {img}, \\ndepth: {train_dict[img[:-1]][4]} \\ncoverage: {train_dict[img[:-1]][5]}'); \n",
    "            _ = ax2[i].axis('off')\n",
    "            i += 1\n",
    "            if i==max_i: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to arrays\n",
    "X_train = np.array([v[0] for k, v in train_dict.items()])[...,np.newaxis]\n",
    "Y_train = np.array([v[1] for k, v in train_dict.items()])[...,np.newaxis]\n",
    "\n",
    "X_train_csum = np.array([v[2] for k, v in train_dict.items()])[...,np.newaxis]\n",
    "Y_train_csum = np.array([v[3] for k, v in train_dict.items()])[...,np.newaxis]\n",
    "\n",
    "X_depth = np.array([v[4] for k, v in train_dict.items()])\n",
    "Y_coverages = np.array([v[5] for k, v in train_dict.items()])\n",
    "Y_cov_class = np.array([v[6] for k, v in train_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DBG:\n",
    "    # Sanity check array shapes\n",
    "#     X_train.shape, Y_train.shape, X_depth.shape, Y_coverages.shape, Y_cov_class.shape,n_train_samples, n_flip_samples\n",
    "    print(X_train.shape, Y_train.shape, X_train_csum.shape, Y_train_csum.shape, X_depth.shape, Y_coverages.shape, Y_cov_class.shape,n_train_samples, n_flip_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize depth\n",
    "print('Computing normalized seismic dept...')\n",
    "depth = X_depth\n",
    "mean_depth, std_depth, max_depth = depth.mean(), depth.std(), depth.max()\n",
    "X_norm_depth = (depth - mean_depth) / std_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    " - checking the distribution\n",
    " \n",
    "*As per below: depth is 'normal' distributed and train and test set have same distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    _ = sns.distplot(X_depth, label=\"Train\")\n",
    "    _ = sns.distplot(test_df.z, label=\"Test\")\n",
    "    _ = plt.legend()\n",
    "    _ = plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth vs. coverage\n",
    "\n",
    " - checking the correlation between depth and coverage\n",
    " \n",
    " Stretching and offsetting the normalized salt coverage for visual comparison reason only\n",
    " \n",
    "*As per below: no pattern or correlation visible, depth and coverage are unrelated. Depth might still be a factor in the prediction, e.g. the structure/grain might relate to depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Pairplot\n",
    "    salt_cover_norm = (Y_coverages - np.mean(Y_coverages)) / np.std(Y_coverages)\n",
    "    sns.pairplot(pd.DataFrame(np.stack((X_norm_depth, salt_cover_norm)).T, columns=['depth', 'coverage']), size=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage class\n",
    "\n",
    "*As per below: there are 8-10 times more seismic images with 0-10% salt areas. Stratification in train-validation split must prevent overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    bincount = np.bincount(list(Y_cov_class))\n",
    "    _ = plt.figure(figsize=(15, 6))\n",
    "    _ = sns.distplot(Y_cov_class, label=\"Train\", kde=False, bins=len(bincount))\n",
    "    _ = plt.legend()\n",
    "    _ = plt.title(\"Coverage distribution\")\n",
    "    print(bincount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images and masks (overlayed)<a name='G'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Helper function for using throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_masks(imgs, masks, **kwargs):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n",
    "    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n",
    "    Arguments:\n",
    "    imgs: vector(-1, h, w, ch)\n",
    "    masks: vector(-1, h, w, ch)\n",
    "    \"\"\"\n",
    "    depth = kwargs.get('depth', None)\n",
    "    coverage = kwargs.get('coverage', None)\n",
    "    cov_class = kwargs.get('cov_class', None)\n",
    "    preds_valid = kwargs.get('preds_valid', None)\n",
    "    thres = kwargs.get('thres', None)\n",
    "    grid_width = kwargs.get('grid_width', 10)\n",
    "    zoom = kwargs.get('zoom', 1.5)\n",
    "    \n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, (img, mask) in enumerate(zip(imgs, masks)):\n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            pred = preds_valid[i]\n",
    "            pred = pred[..., 0]\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(pred > thres), dtype=np.float32)\n",
    "                iou = f'IoU: {IoU(mask, pred)}'\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "                _ = ax.text(2, img.shape[0]-2, iou, color=\"k\")\n",
    "            else:\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "            \n",
    "        if depth is not None:\n",
    "            _ = ax.text(2, img.shape[0]-2, f'depth: {depth[i].round(3)}', color=\"k\")\n",
    "        if (coverage is not None) and (cov_class is not None):   \n",
    "            _ = ax.text(2, 2, f'{coverage[i].round(3)}({cov_class[i]})', color=\"k\", ha=\"left\", va=\"top\")\n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise sample - get a feel of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    N = 100\n",
    "    sample = random.sample(list(range(N)), N)\n",
    "    plot_imgs_masks(X_train[sample], Y_train[sample], coverage=Y_coverages[sample], cov_class=Y_cov_class[sample], depth=X_norm_depth[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split<a name='H'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Validation set will be used for saving model checkpoints and early stopping. \n",
    "Tradeoff:\n",
    " - larger validation set for more validation accuracy and saving better generalizing model, which means less training samples and thus less generalizing model.\n",
    "\n",
    "Using images with depth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(f'notebook memory used before split: {mem_used()}mb')\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid, X_train_csum, X_valid_csum, Y_train_csum, Y_valid_csum, depth_train, depth_valid = train_test_split(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_train_csum,\n",
    "    Y_train_csum,\n",
    "    np.array(X_norm_depth).reshape(-1, 1),\n",
    "    test_size=VAL_SIZE, \n",
    "    stratify=Y_cov_class, \n",
    "    random_state=1)\n",
    "\n",
    "gc.collect()\n",
    "print(f'notebook memory used after split: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check \n",
    "\n",
    "Shape and images/masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DBG:\n",
    "    print(X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, depth_train.shape, depth_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Sanity check X_valid, Y_valid are correct unary/boolean masks\n",
    "    N = 20\n",
    "    plot_imgs_masks(X_valid[:N], Y_valid[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metrics<a name='I'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Custom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(Y_true, Y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean(IoU) metric\n",
    "    IoU = intersection / union\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - convert probability mask to boolean mask based on given threshold\n",
    "     - score the mask 1 if(IoU > score_threshold(0.5))\n",
    "    Take the mean of the scoress\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n",
    "        \n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            score = tf.identity(score) #!! use identity to transform score to tensor\n",
    "        prec.append(score) \n",
    "        \n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    beta = 1.25\n",
    "    bb = beta * beta\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = (1 + bb) * K.sum(y_true_f * y_pred_f)\n",
    "    union = K.sum(y_true_f) + bb * K.sum(y_pred_f)\n",
    "    return -(intersection + smooth) / (union + smooth)\n",
    "\n",
    "# Similar to IoU metric\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "\n",
    "def as_keras_metric(method):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\"\"\"\n",
    "    import functools\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self_, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self_, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "# recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture<a name='J'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Below functions create the UNet model in a functional and recursive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(m, ch_dim, acti, bn, res, do=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=1)(n)\n",
    "    o = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=2)(n)  ## \n",
    "    p = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=4)(n)\n",
    "#     n = AtrousConvolution2D(ch_dim, 3, 3, atrous_rate=(2,2), border_mode='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n, o, p]) if res else n\n",
    "\n",
    "def input_feature(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    f_repeat = RepeatVector(xx*xx)(f)\n",
    "    f_conv = Reshape((xx, xx, n_features))(f_repeat)\n",
    "    n = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n\n",
    "\n",
    "def input_csum(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "#     f_repeat = RepeatVector(xx*xx)(f)\n",
    "#     f_conv = Reshape((xx, xx, n_features))(f)\n",
    "    n = Concatenate(axis=-1, name=f'csum_{2}')([n, f])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_block(m, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum):\n",
    "    \"\"\"Recursive CNN builder\"\"\"\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, ch_dim, acti, bn, res) # add/remove drop-out: ,do\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n",
    "        # Input csum at 3rd layer (32x32)\n",
    "        if (inp_csum is not None) and (depth==DEPTH):\n",
    "            m = Concatenate()([m, input_csum(inp_csum, m)])\n",
    "        # Input depth\n",
    "        if (inp_feat is not None) and (depth==1):\n",
    "            m = Concatenate()([m, input_feature(inp_feat, m)])\n",
    "        m = level_block(m, int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum)\n",
    "        \n",
    "        # Unwind recursive stack calls - creating the upscaling part of the model\n",
    "        if up:\n",
    "            # Repeat the rows and columns of the data by 2 and 2 respectively - untrainable like reverse pooling\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            # Transposed convolutions are going in the opposite direction of a normal convolution - trainable\n",
    "            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, ch_dim, acti, bn, res)\n",
    "    else:\n",
    "        # Depth == 0 - deepest conv_block\n",
    "        m = conv_block(m, ch_dim, acti, bn, res, do) # add/remove drop-out: ,do\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n",
    "    \"\"\"Returns model\"\"\"\n",
    "    inputs = Input(shape=img_shape, name='img')\n",
    "    inp_csum = Input(shape=(64, 64,1), name='csum') ###\n",
    "    inp_feat = Input(shape=(1,), name='feat') # or None\n",
    "    outputs = level_block(inputs, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, inp_feat, inp_csum)\n",
    "    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "    return Model(inputs=[inputs, inp_csum, inp_feat], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Build model<a name='K'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Interesting reads:  \n",
    "[orginal paper](https://arxiv.org/pdf/1505.04597.pdf)  \n",
    "[Upsampling2D vs. Convolution2DTranspose](#https://distill.pub/2016/deconv-checkerboard/)  \n",
    "[Optimizers](http://ruder.io/optimizing-gradient-descent/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CH = 1 #X_train.shape[-1]  # layers of image\n",
    "CONV_CH = 16                # number of channels to start/end UNet with\n",
    "DEPTH = 5                   # number of CONV blocks to max model depth\n",
    "D_OUT = 0.2                 # small random effect can be effective against overfitting. \n",
    "                            # Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n",
    "BN = True\n",
    "UP_CONV = False              # UPSampling avoids coarse checkerboard effect in images\n",
    "RES = True                 # residuals skip layers for deep networks (gradient evaporation)\n",
    "\n",
    "model = UNet((TGT_SIZE, TGT_SIZE, IMG_CH), \n",
    "             start_ch=CONV_CH, \n",
    "             depth=DEPTH, \n",
    "             dropout=D_OUT,\n",
    "             batchnorm=BN, \n",
    "             upconv=UP_CONV,\n",
    "             residual=RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "# Define optimizer\n",
    "# Clip gradients to norm 1., \n",
    "optimizer = {'adam': Adam(lr=LR, beta_1=0.9, beta_2=0.999, clipnorm=8.),\n",
    "             'sgd': SGD(lr=LR, decay=LR/100, momentum=0.99, nesterov=True, clipnorm=5.),\n",
    "             'rmsprop': RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "             'adadelta': Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
    "             'nadam': Nadam(lr=LR, beta_1=0.9, beta_2=0.999)} # , epsilon=None, schedule_decay=0.004\n",
    "opt = 'rmsprop'\n",
    "\n",
    "# Define loss\n",
    "loss = [\"binary_crossentropy\", \"kullback_leibler_divergence\"]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss[0], optimizer=optimizer[opt], metrics=[\"accuracy\", precision, mean_iou]) #accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'TGS_UNet_{opt}_CH-{IMG_CH}-{CONV_CH}_D-{DEPTH}_DO-{D_OUT>0}_BN-{BN}_UP-{UP_CONV}_RES-{RES}.h5'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model<a name='L'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Model can be trained succesively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 24  # larger will have more stable learning, but needs more GPU, smaller is more stochastic in nature\n",
    "# EPOCHS = 5      # default EPOCHS when commit mode\n",
    "fit = True       # True:fit model, False:load model\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, verbose=1), # bias-variance tradeof\n",
    "    ReduceLROnPlateau(patience=4, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, verbose=1)] # continue/save based on monitor metric\n",
    "\n",
    "X_train_dict = {'img': X_train, \n",
    "                'csum': X_train_csum,\n",
    "                'feat': depth_train}\n",
    "\n",
    "X_val_dict = {'img': X_valid, \n",
    "              'csum': X_valid_csum,\n",
    "              'feat': depth_valid}\n",
    "\n",
    "# Train when needed - input EPOCHS\n",
    "if DEV:\n",
    "    train_yn = input('train y/n?: ')\n",
    "    if train_yn[0] == 'n':\n",
    "        fit = False\n",
    "    else:\n",
    "        EPOCHS = int(input('Number of training epochs?: '))\n",
    "\n",
    "if fit:        \n",
    "    history = model.fit(X_train_dict, \n",
    "            Y_train, \n",
    "            validation_data=(X_val_dict, Y_valid),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Learning curves<a name='M'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "1. Check the curves for errors  \n",
    "2. Check the curves for roughness and convergence to tune the hyperparameters:  \n",
    " - learning rate and decay\n",
    " - batch size\n",
    " - model architecture\n",
    " - epochs and earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fit:\n",
    "    fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(20,8))\n",
    "\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    _ = ax_loss.legend()\n",
    "    _ = ax_loss.set_title('Loss')\n",
    "    \n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "    _ = ax_acc.legend()\n",
    "    _ = ax_acc.set_title('Accuracy')\n",
    "    \n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")\n",
    "    _ = ax_iou.legend()\n",
    "    _ = ax_iou.set_title('IoU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on validation set <a name='N'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = load_model(model_name, custom_objects={'mean_iou': mean_iou})\n",
    "print('model loading done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[next(data_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train)) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on validation set\n",
    "# DEV = True\n",
    "if DEV:\n",
    "    val_scores = model.evaluate_generator(data_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train), \n",
    "                                          steps=50,\n",
    "                                          workers=-1)\n",
    "    print(f'model validation scores: \\nval_loss: {val_scores[0]}, \\nval_acc: {val_scores[1]}, \\nval_miou: {val_scores[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "preds_valid = model.predict_generator(data_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                                      steps=50,\n",
    "                                      workers=-1).reshape(-1, TGT_SIZE, TGT_SIZE)\n",
    "preds_valid = preds_valid.reshape(-1, TGT_SIZE, TGT_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    print(preds_valid.shape) #, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize validation(probability masks) layed over GT masks\n",
    "\n",
    "Grount truth: green mask\n",
    "Validation: pink mask\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold = .6\n",
    "if DEV:\n",
    "    N = np.arange(100)\n",
    "    plot_imgs_masks(X_valid[N], Y_valid[N], preds_valid=preds_valid[N]>.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: blur and threshold<a name='Q'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Compare masks with different blur\n",
    "    for i in random.sample(range(Y_valid.shape[0]), 5):\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y = Y_valid[...,0][i]\n",
    "        blurr = 7 # restricted in medianBlur - 5 is max\n",
    "        thres = .7\n",
    "        fig, ax = plt.subplots(1,4, figsize=(24,6))\n",
    "\n",
    "        _ = ax[0].imshow(y>thres, cmap=\"seismic\"); ax[0].set_title('Ground Truth mask')\n",
    "\n",
    "        _ = ax[1].imshow(y_pred>thres, cmap=\"seismic\"); ax[1].set_title('Prediction mask')\n",
    "        _ = ax[1].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        img_blur = cv2.medianBlur(y_pred, 5)\n",
    "        _ = ax[2].imshow(img_blur>thres, cmap=\"seismic\"); ax[2].set_title('Median blur')\n",
    "        _ = ax[2].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        img_blur = cv2.blur(y_pred, (blurr, blurr))\n",
    "        _ = ax[3].imshow(img_blur>thres, cmap=\"seismic\"); plt.title('Gaussian blur')\n",
    "        _ = ax[3].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Combinations of blur and threshold\n",
    "    sample = random.sample(range(Y_valid.shape[0]), 5)\n",
    "    for i in sample:\n",
    "        thres = .7\n",
    "        blur_kernel = (7, 7)\n",
    "        Y_pred_gaus = np.array([cv2.blur(y, blur_kernel) for y in preds_valid]).reshape(preds_valid.shape)\n",
    "        \n",
    "        y = Y_valid[...,0][i]\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y_gaus = Y_pred_gaus[...,0][i]\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(1,7, figsize=(25,5), sharey=True)\n",
    "        # GT mask\n",
    "        _ = ax1.imshow(y, cmap=\"seismic\"); \n",
    "        _ = ax1.set_title('Ground truth mask')\n",
    "\n",
    "        # Probability mask\n",
    "        _ = ax2.imshow(y_pred, cmap=\"seismic\"); \n",
    "        _ = ax2.set_title('Probability mask')\n",
    "        _ = ax2.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Prediction mask\n",
    "        img = y_pred > thres\n",
    "        _ = ax3.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax3.set_title('Prediction mask')\n",
    "        _ = ax3.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Opencv thresh prediction mask\n",
    "        thres, img = cv2.threshold(y_pred, thres, 1, cv2.THRESH_BINARY)\n",
    "        _ = ax4.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax4.set_title('Opencv threshold')\n",
    "        _ = ax4.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_gaus, thres, 1, cv2.THRESH_BINARY)\n",
    "        _ = ax5.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax5.set_title('Blur>thres')\n",
    "        _ = ax5.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Thres>blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_pred, thres, 1, cv2.THRESH_BINARY)\n",
    "        img = cv2.blur(img, blur_kernel) > thres\n",
    "        _ = ax6.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax6.set_title('Thres>blur>thres')\n",
    "        _ = ax6.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Blur>thres>blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_gaus, thres, 1, cv2.THRESH_BINARY)\n",
    "        img = cv2.blur(img, blur_kernel) > thres\n",
    "        _ = ax7.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax7.set_title('Blur>thres>blur>thres')\n",
    "        _ = ax7.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Experiment with special thresholding algo's\n",
    "    for i in random.sample(range(Y_valid.shape[0]), 5):\n",
    "        thres = .7\n",
    "        blur_kernel, blurr = (3, 3), 3\n",
    "        Y_pred_gaus = np.array([cv2.blur(y, blur_kernel) for y in preds_valid]).reshape(preds_valid.shape)\n",
    "        \n",
    "        y = Y_valid[...,0][i]\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y_pred = (255 * y_pred).astype(np.uint8)\n",
    "        y_gaus = Y_pred_gaus[...,0][i]\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1,6, figsize=(25,5), sharey=True)\n",
    "        # GT mask\n",
    "        _ = ax1.imshow(y, cmap=\"seismic\"); \n",
    "        _ = ax1.set_title('Ground truth mask')\n",
    "        \n",
    "        # Prediction mask\n",
    "        Y_ = y_pred > thres\n",
    "        _ = ax2.imshow(Y_, cmap=\"seismic\"); \n",
    "        _ = ax2.set_title('Prediction mask')\n",
    "        _ = ax2.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Otsu prediction mask\n",
    "        _, Y_otsu = cv2.threshold(y_pred, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        _ = ax3.imshow(Y_otsu, cmap=\"seismic\"); \n",
    "        _ = ax3.set_title('Otsu mask')\n",
    "        _ = ax3.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        # Adaptive threshold\n",
    "        _, Y_gaus = cv2.threshold(y_pred, 60, 240, cv2.ADAPTIVE_THRESH_GAUSSIAN_C)\n",
    "        _ = ax4.imshow(cv2.bitwise_not(Y_gaus), cmap=\"seismic\"); \n",
    "        _ = ax4.set_title('Gaussian threshold')  # reverse image: cv2.bitwise_not()\n",
    "        _ = ax4.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        # Compbined Otsu and Adaptive\n",
    "        Y_mean = (Y_otsu+Y_gaus)/2\n",
    "        _ = ax5.imshow(cv2.bitwise_not(Y_mean), cmap=\"seismic\"); \n",
    "        _ = ax5.set_title('IoU Otsu and Gaussian') \n",
    "        _ = ax5.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        #\n",
    "        img_thresh_Gaussian = cv2.adaptiveThreshold(y_pred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 1)\n",
    "        _ = ax6.imshow(img_thresh_Gaussian, cmap=\"seismic\"); \n",
    "        _ = ax6.set_title('Adaptive Gaussian')\n",
    "        _ = ax6.imshow(y, alpha=.3, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Otsu masking\n",
    "def pred_otsu(y, thresh, blurr=3):\n",
    "    \"\"\"Preprocess mask for otsu threshold\n",
    "    Returns:\n",
    "    binary mask\"\"\"\n",
    "    blur_kernel = (blurr, blurr)\n",
    "    try:\n",
    "        im = cv2.blur(y, blur_kernel) > thresh\n",
    "        im = np.array(im * 255, dtype=np.uint8)\n",
    "        _, Y_otsu = cv2.threshold(im, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    except:\n",
    "        Y_otsu = np.zeros_like(y)\n",
    "    return Y_otsu//255\n",
    "\n",
    "def preds_otsu(Y, threshold, blurr):\n",
    "    \"\"\"Preprocess masks for otsu threshold\"\"\"\n",
    "    Y_otsu = [pred_otsu(y, threshold, blurr) for y in Y[...,0]]\n",
    "    return np.array(Y_otsu, dtype=np.int8).reshape(Y.shape)\n",
    "\n",
    "# Sanity check function\n",
    "if DEV:\n",
    "    for i in [6, 13, 17, 27]:\n",
    "        thres = .7\n",
    "        blurr = 3\n",
    "        _ = plt.imshow(preds_otsu(preds_valid[:28], threshold=thres, blurr=blurr)[i,...,0], cmap=\"seismic\")\n",
    "        _ = plt.imshow(Y_valid[...,0][i], alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.title('Otsu'); \n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring<a name='O'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Score the model and do a threshold optimization by the best IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(Y_true, Y_pred):\n",
    "    \"\"\"IoU of unary/boolean mask using confusion matrix\"\"\"\n",
    "    CM, _, _ = np.histogram2d(Y_true.ravel(), Y_pred.ravel(), bins=(2,2))\n",
    "    TN, FN, FP, TP = np.split(CM.ravel(), 4)\n",
    "    if TP==0:\n",
    "        return 0\n",
    "    else:\n",
    "        iou = TP / (FN + FP + TP)\n",
    "    return np.mean(np.arange(0.5, 1.0, 0.05) < iou)\n",
    "\n",
    "def miou(Y_trues, Y_preds):\n",
    "    \"\"\"Mean intersection over union of all masks\"\"\"\n",
    "    return np.mean([IoU(Y_trues[i], Y_preds[i]) for i in range(Y_trues.shape[0])])\n",
    "\n",
    "def best_miou(mious):\n",
    "    \"\"\"Returns tuple of (best IoU, corresponding threshold)\"\"\"\n",
    "    best_idx = np.argmax(mious)\n",
    "    return mious[best_idx], thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold for best IoU score (submission)\n",
    "\n",
    "Checking which threshold delivers the best IoU score, so this threshold will be used for Test set prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare blur effect on IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Explore best params giving best IoU: blur kernel size, threshold\n",
    "    blur_range = [1, 3, 5, 7, 9]\n",
    "    mious_otsu = {}\n",
    "    for i, blurr in enumerate(blur_range):\n",
    "        print(f'computing best IoU @blur: {blurr}')\n",
    "        thresholds = np.linspace(0.1, 0.9, 80)\n",
    "        mious_otsu = {**mious_otsu, **{blurr: {threshold: miou(Y_valid, preds_otsu(preds_valid, threshold, blurr=blurr))\n",
    "                     for threshold in tqdm_notebook(thresholds)}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "if DEV:\n",
    "    # Visualize IoU by blur and as function of thresholds\n",
    "    _ = plt.figure(figsize=(20,10))\n",
    "    _ = plt.xlabel(\"Threshold\")\n",
    "    _ = plt.ylabel(\"IoU\")\n",
    "    _ = plt.title(f\"Mean-IoU\")\n",
    "    \n",
    "    for i in blur_range:\n",
    "        best_idx = np.argmax(list(mious_otsu[i].values()))\n",
    "        threshold_best_ = list(mious_otsu[i])[best_idx]\n",
    "        miou_best_ = list(mious_otsu[i].values())[best_idx]\n",
    "        _ = plt.plot(list(mious_otsu[i]), mious_otsu[i].values(), label=f'Blur: {i}')\n",
    "        _ = plt.plot(threshold_best_, miou_best_, \"ok\", label=f\"Best threshold: {threshold_best_.round(3)}\")\n",
    "        _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU scores for best params\n",
    "# Blur 3 gives same reuslts as more blur, this is the savest bet\n",
    "blurr = 3 \n",
    "thresholds = np.linspace(0.1, 0.9, 80)\n",
    "mious_otsu = np.array([miou(Y_valid, preds_otsu(preds_valid, threshold, blurr=blurr))\n",
    "                 for threshold in tqdm_notebook(thresholds)])\n",
    "\n",
    "# Get best scores by threshold\n",
    "miou_best_otsu, threshold_best_otsu = best_miou(mious_otsu)\n",
    "\n",
    "if DEV:\n",
    "    mious_no = np.array([miou(Y_valid, np.int8(preds_valid > threshold)) \n",
    "                     for threshold in tqdm_notebook(thresholds)])\n",
    "    miou_best, threshold_best = best_miou(mious_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #DEV:\n",
    "    # Visualize best IoU as function of thresholds\n",
    "    plt.figure(figsize=(20,10))\n",
    "    _ = plt.plot(thresholds, mious_otsu, label=f'Otsu threshold with blur {blurr}')\n",
    "    _ = plt.plot(thresholds, mious_no, label='No blur')\n",
    "    _ = plt.plot(threshold_best, miou_best, \"xr\", label=f\"Best threshold: {threshold_best.round(3)}\")\n",
    "    _ = plt.plot(threshold_best_otsu, miou_best_otsu, \"xk\", label=f\"Best threshold otsu: {threshold_best_otsu.round(3)}\")\n",
    "    _ = plt.xlabel(\"Threshold\")\n",
    "    _ = plt.ylabel(\"IoU\")\n",
    "    _ = plt.title(f\"Mean-IoU: {miou_best.round(3)}(default) - {miou_best_otsu.round(3)}(otsu)\")\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize binary masks based on best threshold\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    plot_imgs_masks(X_valid[N], Y_valid[N], preds_valid=preds_valid[N], thres=threshold_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    blurr = 5\n",
    "    thres = .5\n",
    "    plot_imgs_masks(X_valid[N], Y_valid[N], preds_valid=preds_otsu(preds_valid, threshold_best_otsu, blurr=blurr)[N], thres=threshold_best_otsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of using Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    blurr = 5\n",
    "\n",
    "    Y_pred_non = np.array([y for y in preds_valid[N]]).reshape(preds_valid[N].shape)\n",
    "    Y_pred_otsu = preds_otsu(preds_valid, threshold_best_otsu, blurr=blurr)[N]\n",
    "\n",
    "    plot_imgs_masks((np.zeros_like(Y_pred_non)), Y_pred_non, preds_valid=Y_pred_otsu, thres=threshold_best_otsu)\n",
    "    diff = abs(Y_pred_non - Y_pred_otsu) - np.ones_like(Y_pred_non)\n",
    "    plot_imgs_masks((np.zeros_like(Y_pred_non)), diff, preds_valid=diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory: need ~7gb for test set\n",
    "gc.collect()\n",
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory - delete abundant objects\n",
    "abundant_objects = [train_set, train_flip, train_dict, X_train, Y_train, X_valid, Y_valid]\n",
    "for obj in abundant_objects:\n",
    "    try:\n",
    "        del obj\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(f\"couldn't delete {str(obj)}\")\n",
    "\n",
    "gc.collect()\n",
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set<a name='P'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check if all indices and images match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    test_ids = next(os.walk(f\"{path_test}/images\"))[2]\n",
    "    print(set(test_ids) ^ set(test_df.index+'.png'))\n",
    "    assert len(set(test_ids) ^ set(test_df.index+'.png')) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Convert test set images and depths to layers\n",
    "\n",
    " - Upsample images to arrays\n",
    " - Reshape for modeling\n",
    " - Create depth layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set empty images\n",
    "if DEV:\n",
    "    test_dict_empty = {name: (\n",
    "                  upsample(np.array(load_img(f'{imgs_test}/{name}.png', grayscale=True)), TGT_SIZE) / 255,              # image vector\n",
    "                  test_df.loc[name, 'z'])                                                                               # depth\n",
    "                  for name in tqdm_notebook(test_indices) if np.sum(np.array(load_img(f'{imgs_test}/{name}.png', grayscale=True))) == 0}\n",
    "    \n",
    "    # TODO\n",
    "    X_test_csum = np.array([upsample(csum(img_to_array(load_img(f'{imgs_test}/{name}.png', grayscale=True))), TGT_SIZE//2) / 255  # csum image vector\n",
    "              for name in tqdm_notebook(test_indices) if np.sum(np.array(load_img(f'{imgs_test}/{name}.png', grayscale=True))) == 0])[...,np.newaxis]\n",
    "\n",
    "    # Convert dictionary to arrays\n",
    "    X_test_empty = np.array([v[0] for k, v in test_dict_empty.items()])[...,np.newaxis]\n",
    "\n",
    "    # Normalize depth - !use train mean and std\n",
    "    X_test_depth_empty = np.array([v[1] for k, v in test_dict_empty.items()])\n",
    "    X_test_norm_depth_empty = (X_test_depth_empty - mean_depth) / std_depth\n",
    "\n",
    "    # Predict empty test set as sanity check - improve score with 1/97,6%\n",
    "    X_test_dict_empty = {'img': X_test_empty, \n",
    "                         'csum': X_test_csum , # TODO\n",
    "                         'feat': X_test_norm_depth_empty} \n",
    "\n",
    "    Y_test_empty = model.predict(X_test_dict_empty)\n",
    "    Y_test_empty_otsu = preds_otsu(Y_test_empty, threshold_best_otsu, 3) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual sanity check - empty images\n",
    "if DEV:\n",
    "    N = np.arange(30)\n",
    "    plot_imgs_masks(X_test_empty[N], np.zeros_like(X_test_empty[N]), preds_valid=Y_test_empty[N], thres=threshold_best)\n",
    "    plot_imgs_masks(X_test_empty[N], np.zeros_like(X_test_empty[N]), preds_valid=Y_test_empty_otsu[N], thres=threshold_best_otsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set (keep >5gb free RAM!)\n",
    "# Test set has 17570/18000 = 97,6% non-empty images\n",
    "X_test = np.array([upsample(np.array(load_img(f'{imgs_test}/{name}.png', grayscale=True)), TGT_SIZE) / 255  # image vector\n",
    "              for name in tqdm_notebook(test_indices)])[...,np.newaxis]\n",
    "X_test_csum = np.array([upsample(csum(img_to_array(load_img(f'{imgs_test}/{name}.png', grayscale=True))), TGT_SIZE//2) / 255  # csum image vector\n",
    "              for name in tqdm_notebook(test_indices)])[...,np.newaxis]\n",
    "\n",
    "X_test_depth = np.array([test_df.loc[name, 'z'] for name in tqdm_notebook(test_indices)])\n",
    "\n",
    "# Normalize depth - !use train mean and std\n",
    "X_test_norm_depth = (X_test_depth - mean_depth) / std_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict X_test\n",
    "X_test_dict = {'img': X_test, \n",
    "               'csum': X_test_csum,\n",
    "               'feat': X_test_norm_depth}\n",
    "\n",
    "Y_test = model.predict(X_test_dict) # probability\n",
    "Y_test_otsu = preds_otsu(Y_test, threshold_best_otsu, 3) # is already unary/boolean!\n",
    "print('prediction done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually check of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of predictions without/with applying Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    plot_imgs_masks(X_test[N], np.zeros_like(X_test[N]), preds_valid=Y_test[N], thres=threshold_best)\n",
    "    plot_imgs_masks(X_test[N], np.zeros_like(X_test[N]), preds_valid=Y_test_otsu[N], thres=threshold_best_otsu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission  <a name='R'></a> \n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "Submission is in csv form:\n",
    " - `id`: index (equals filename)\n",
    " - `rle_mask`: run-length format (down-then-right): `masked_pixel_start` `<space>` `length_of_masked_pixels` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F'):\n",
    "    \"\"\"Convert binary mask image to run-length array or string.\n",
    "    \n",
    "    Args:\n",
    "    img: image in shape [n, m]\n",
    "    order: is down-then-right, i.e. Fortran(F)\n",
    "    string: return in string or array\n",
    "\n",
    "    Return:\n",
    "    run-length as a string: <start[1s] length[1s] ... ...>\n",
    "    \"\"\"\n",
    "    bytez = img.reshape(img.shape[0]*img.shape[1], order=order)\n",
    "    bytez = np.concatenate([[0], bytez, [0]])\n",
    "    runs = np.where(bytez[1:] != bytez[:-1])[0] + 1 # pos start at 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Use for sanity check the encode function\n",
    "def RLdec(rl_string, shape=(101, 101), order='F'):\n",
    "    \"\"\"Convert run-length string to binary mask image.\n",
    "    \n",
    "    Args:\n",
    "    rl_string: \n",
    "    shape: target shape of array\n",
    "    order: decode order is down-then-right, i.e. Fortran(F)\n",
    "\n",
    "    Return:\n",
    "    binary mask image as array\n",
    "    \"\"\"\n",
    "    s = rl_string.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img, dim=101):\n",
    "    \"\"\"Downsize an image\"\"\"\n",
    "    return cv2.resize(img, (dim, dim), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def downsample_(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode predictions on Otsu validation threshold - upsample doesnot work (??)\n",
    "pred_dict = {idx: RLenc(downsample(Y_test_otsu[i], dim=101))\n",
    "             for i, idx in enumerate(tqdm_notebook(test_indices))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    pd.DataFrame.from_dict(pred_dict, orient='index').sample(10)\n",
    "    \n",
    "pd.DataFrame.from_dict(pred_dict, orient='index').sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')\n",
    "print('submission saved!')\n",
    "\n",
    "if DEV:\n",
    "    sub.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for upvoting and sharing your thoughts!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
