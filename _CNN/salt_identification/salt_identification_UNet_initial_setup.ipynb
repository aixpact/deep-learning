{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)\n",
    "by: frank@aixpact.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "- experimenting with cumsum and depth\n",
    "  - did not improve the scores\n",
    "  - will explore data augmenting and EDA in seperate notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits to\n",
    "\n",
    "I based this notebook and insights on:\n",
    "\n",
    "[Alexander Liao](https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice/notebook)  -- Dice and augmentation  \n",
    "[Jesper Dramsch](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics) -- About session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "Seismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n",
    "\n",
    "Imaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n",
    "\n",
    "Seismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\n",
    "You'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n",
    "\n",
    "[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\n",
    "Caption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n",
    "\n",
    "Interpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n",
    "\n",
    "One common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n",
    "\n",
    "Recently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n",
    "\n",
    "+ https://github.com/waldeland/CNN-for-ASI\n",
    "+ https://github.com/bolgebrygg/MalenoV\n",
    "+ https://github.com/crild/facies_net\n",
    "\n",
    "You will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input'\n",
    "path_train = f'{path}/train'\n",
    "path_test = f'{path}/test'\n",
    "imgs_train = f'{path}/train/images'\n",
    "masks_train = f'{path}/train/masks'\n",
    "imgs_test = f'{path}/test/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set/initiate constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 101\n",
    "TGT_SIZE = 128\n",
    "MAX_DEPTH = None # set after loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(img, img_size_target=TGT_SIZE):\n",
    "    \"\"\"Resize image to target shape\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def downsample(img, img_size_orig=IMG_SIZE):\n",
    "    \"\"\"Resize image to original shape\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_orig:\n",
    "        return img\n",
    "    return resize(img, (img_size_orig, img_size_orig), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def imgs_2_array(path, img_names, ftype='.png', size=TGT_SIZE):\n",
    "    \"\"\"Load images as arrays in target shape: (-1, target, target, 1)\"\"\"\n",
    "    imgs = [upsample(np.array(load_img(f'{path}/{name}{ftype}', grayscale=True))) / 255\n",
    "                      for name in tqdm_notebook(img_names)]\n",
    "    imgs.extend([np.fliplr(img) for img in imgs]) # extend set with flips\n",
    "    imgs = np.array(imgs).reshape(-1, size, size, 1)\n",
    "    return imgs\n",
    "        \n",
    "    \n",
    "def csum(img, border=2):\n",
    "    \"\"\"Create image cumsum from image\n",
    "    Sort of image bleeding downwards\"\"\"\n",
    "    center_mean = img[border:-border, border:-border].mean()\n",
    "    csum = (np.float32(img)-center_mean).cumsum(axis=0)         \n",
    "    csum -= csum[border:-border, border:-border].mean()\n",
    "    csum /= max(1e-3, csum[border:-border, border:-border].std())\n",
    "    return csum\n",
    "\n",
    "\n",
    "def imgs_2_csum(path, img_names, ftype='.png', size=TGT_SIZE):\n",
    "    \"\"\"Load images and transform to array with image and cumsum layer\"\"\"\n",
    "    imgs = imgs_2_array(path, img_names, ftype, size)\n",
    "    img_csums = [csum(img) for img in tqdm_notebook(imgs)]\n",
    "    return np.concatenate((np.array(imgs), np.array(img_csums)) , axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Load and build train and test set.\n",
    "\n",
    " - contemplating to use depth as a layer, this explains the stacking in `images_d`.\n",
    " - checking depth\n",
    " - checking coverage\n",
    " - checking depth - coverage relationship\n",
    " - visualize seismic images with masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(f'{path}/train.csv', index_col=\"id\", usecols=[0])\n",
    "depths_df_ = pd.read_csv(f'{path}/depths.csv', index_col=\"id\")\n",
    "train_df_ = train_df_.join(depths_df_)\n",
    "train_indices = train_df_.index\n",
    "\n",
    "# Flip(augment) train images -> duplicate df: images and depth\n",
    "train_df = pd.concat([train_df_, train_df_])\n",
    "\n",
    "test_df = depths_df_[~depths_df_.index.isin(train_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs_2_array(imgs_train, train_indices, '.png', TGT_SIZE)\n",
    "train_df[\"images\"] = [img for img in imgs]\n",
    "train_df[\"images\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs_2_csum(imgs_train, train_indices, '.png', TGT_SIZE)\n",
    "train_df[\"images_d\"] = [img for img in imgs]\n",
    "train_df[\"images_d\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = imgs_2_array(masks_train, train_indices, '.png', TGT_SIZE)\n",
    "train_df[\"masks\"] = [mask for mask in masks]\n",
    "train_df[\"masks\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = train_df[\"z\"]\n",
    "mean_depth, std_depth, max_depth = depth.mean(), depth.std(), depth.max()\n",
    "norm_depth = (depth - mean_depth) / std_depth\n",
    "train_df[\"depth\"] = norm_depth\n",
    "train_df[\"depth\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "train_df[:5]\n",
    "train_df[4000:4005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    " - checking the distribution\n",
    " \n",
    "*As per below: depth is 'normal' distributed and train and test set have same distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(train_df.z, label=\"Train\")\n",
    "_ = sns.distplot(test_df.z, label=\"Test\")\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask coverage\n",
    "\n",
    "*As per below: there are 8-10 times more seismic images with 0-10% salt areas. Stratification in train-validation split must prevent overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(mask):\n",
    "    \"\"\"Compute salt mask coverage\"\"\"\n",
    "    return np.sum(mask) / (mask.shape[0]*mask.shape[1])\n",
    "\n",
    "\n",
    "def norm_coverage(masks):\n",
    "    \"\"\"Compute salt mask coverage\"\"\"\n",
    "    coverages = masks.map(coverage)\n",
    "    mean_cov, std_cov, max_cov = coverages.mean(), coverages.std(), coverages.max()\n",
    "    return(coverages - mean_cov) / std_cov\n",
    "\n",
    "\n",
    "def coverage_class(mask):\n",
    "    \"\"\"Compute salt mask coverage class\"\"\"\n",
    "    if coverage(mask) == 0:\n",
    "            return 0\n",
    "    return (coverage(mask) * 100 //10).astype(np.int8) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(train_df.masks.map(coverage_class), label=\"Train\", kde=False)\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Coverage distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth vs. coverage\n",
    "\n",
    " - checking the correlation between depth and coverage\n",
    " \n",
    "*As per below: no pattern or correlation visible, depth and coverage are unrelated. Depth might still be a factor in the prediction, e.g. the structure/grain might relate to depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salt_cover = train_df.masks.map(coverage)\n",
    "salt_cover = norm_coverage(train_df.masks)\n",
    "# depth = train_df.z / MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_cover = norm_coverage(train_df.masks)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(len(norm_depth)), norm_depth, alpha=.5)\n",
    "plt.scatter(range(len(salt_cover)), salt_cover, color='r', alpha=.5)\n",
    "plt.title('Normalized:\\nDepth as % of maximum depth(blue) vs. Salt coverage as % of image size(red)', fontsize=20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images and masks (overlayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_masks(imgs, masks, preds_valid=None, thres=None, grid_width=10, zoom=1.5):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(orange). \n",
    "    The prediction mask can be either in float-mask or binary-mask form(based on threshold)\n",
    "    \"\"\"\n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        mask = masks[i]\n",
    "        depth, MAX_DEPTH = 1, 1 # img[0, 0, 1] # TODO\n",
    "        \n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(img[..., 1], alpha=0.15, cmap=\"seismic\") # TODO\n",
    "        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(preds_valid[i] > thres), dtype=np.float32)\n",
    "            else:\n",
    "                pred = preds_valid[i]\n",
    "            _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "        \n",
    "        _ = ax.text(2, img.shape[0]-2, depth * MAX_DEPTH//1, color=\"k\")\n",
    "        _ = ax.text(img.shape[0]-2, 2, round(coverage(mask), 2), color=\"k\", ha=\"right\", va=\"top\")\n",
    "        _ = ax.text(2, 2, coverage_class(mask), color=\"k\", ha=\"left\", va=\"top\")\n",
    "        \n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "plot_imgs_masks(train_df.iloc[:N].images_d, train_df.iloc[:N].masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, depth_train, depth_valid = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images_d.map(upsample).tolist()).reshape(-1, TGT_SIZE, TGT_SIZE, 2), # FJE images_d\n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, TGT_SIZE, TGT_SIZE, 1), \n",
    "    train_df.depth.values,\n",
    "    test_size=0.15, \n",
    "    stratify=train_df.masks.map(coverage_class), \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many examples without salt, as you can see by the masks that are entirely dark. That's great, an algorithm we build will then know that patches exist entirely without salt. Talk about biasing your data.\n",
    "\n",
    "We can draw heavily on other work, instead of regurgitating the geophysics work that has been done before. I mentioned that seismic is kind of like ultrasound. So I had a look at https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n",
    "\n",
    "Let's throw a Unet at our data. I am blatanly stealing from Ketil at this point. All credit goes to him and his nice code.\n",
    "First we'll need to get our data into a shape that works for U-Nets. That means, it should be a power of 2. Let's do it quick and dirty for now, but eventually, consider aliasing and all that fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation and preprocessing\n",
    "\n",
    "To address overfitting, to increase the training set and as result to boost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "# LAST_BATCH = len(x_train)%BATCH_SIZE\n",
    "# N_BATCHES = int(LAST_BATCH > 0) + len(x_train)//BATCH_SIZE\n",
    "# print(LAST_BATCH, N_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "\n",
    "As the mask is related to the image, both the mask and the image must be augmented identically, by creating identical generators and using the same seed.\n",
    "\n",
    "**Note:**  \n",
    " - Shifting alters the edges and does more harm than good.  \n",
    " - Zoom-out alters the edges and does more harm than good. \n",
    " - Vertical flip seems to halt the loss/preformance from start.\n",
    " - Standardization changes mask boolean values to negative values, this will raise error at training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two instances with the same arguments\n",
    "data_gen_args = dict(\n",
    "    rotation_range=2.,\n",
    "    zoom_range=[.8, 1],\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "image_datagen.fit(x_train, seed=1)\n",
    "mask_datagen.fit(y_train, seed=1)\n",
    "\n",
    "image_generator = image_datagen.flow(\n",
    "    x_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=1)\n",
    "\n",
    "mask_generator = mask_datagen.flow(\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=1)\n",
    "\n",
    "# combine generators into one which yields image and masks\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate augmented dataset\n",
    "\n",
    "Set the size of the training set as you like, generating(bootstrapping) more augmented samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l = []\n",
    "Y_train_l = []\n",
    "    \n",
    "# add examples to list by batch\n",
    "for batch_id, (x_batch, y_batch) in tqdm_notebook(enumerate(train_generator)):\n",
    "    # Add full batches only - prevent odd array shapes\n",
    "    if x_batch.shape[0] == BATCH_SIZE:\n",
    "        X_train_l.append(x_batch)\n",
    "        Y_train_l.append(y_batch)\n",
    "    # Break infinite loop manually when required number of batches is reached\n",
    "    if len(X_train_l) == N_BATCHES: break\n",
    "\n",
    "# Sanity check all arrays are same shape\n",
    "assert len(set(arr.shape for arr in X_train_l)) == 1\n",
    "assert len(set(arr.shape for arr in Y_train_l)) == 1\n",
    "\n",
    "# Stack list of arrays\n",
    "X_train_augm = np.vstack(X_train_l)\n",
    "Y_train_augm = np.vstack(Y_train_l)\n",
    "\n",
    "# Sanity check stacking over first dimension\n",
    "assert X_train_augm.shape[0] == BATCH_SIZE * N_BATCHES\n",
    "assert Y_train_augm.shape[0] == BATCH_SIZE * N_BATCHES\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augm[:60].shape\n",
    "Y_train_augm[:60].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "plot_imgs_masks(X_train_augm[:N], Y_train_augm[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean-IoU metric(mean-intersect-over-union-score).\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - create boolean mask (from float mask) based on threshold\n",
    "     - score the mask 1 if IoU > score_threshold(0.5)\n",
    "    Take the mean of the scores\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_bool = tf.to_int32(y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(y_true, y_pred_bool, 2) # mean score over batch(=1)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "#             tf.cast(score > score_thres, tf.int32) # provided threshold for iou-score\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "        \n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(m, ch_dim, acti, bn, res, do=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_feature(f, n):\n",
    "    \"\"\"\"\"\"\n",
    "    # We put the depth somewhere in the middle\n",
    "    nn, features = 1, 1\n",
    "    xx       = K.int_shape(n)[1]\n",
    "    f_repeat = RepeatVector(xx*xx*nn)(f)\n",
    "    f_conv   = Reshape((xx,xx,nn*features))(f_repeat)\n",
    "    n        = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n",
    "    n        = BatchNormalization()(n)            \n",
    "#     n        = Activation('relu')(n) ## <-------------- May be\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_block(m, input_depth, inp_feat, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res):\n",
    "    \"\"\"Recursive CNN builder\"\"\"\n",
    "    featureOK = True\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, ch_dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n",
    "        if featureOK and (depth==DEPTH-1):\n",
    "            m = Concatenate()([m, input_feature(inp_feat, m)])\n",
    "        m = level_block(m, input_depth, inp_feat, int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        # \n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, ch_dim, acti, bn, res)\n",
    "    else:\n",
    "        # lowest/middle block\n",
    "        m = conv_block(m, ch_dim, acti, bn, res, do)\n",
    "        m = Concatenate()([m, input_depth])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPT_SIZE = 4\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n",
    "    \"\"\"Returns model\"\"\"\n",
    "    inputs = Input(shape=img_shape, name='img')\n",
    "    input_depth = Input(shape=(DPT_SIZE, DPT_SIZE, 1), name='depth')\n",
    "    inp_feat = Input(shape=(1,), name='feat')\n",
    "    outputs = level_block(inputs, input_depth, inp_feat, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "    return Model(inputs=[inputs, input_depth, inp_feat], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "#         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "#     \"\"\"Returns model\"\"\"\n",
    "#     inputs = Input(shape=img_shape)\n",
    "#     outputs = level_block(inputs, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "#     outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "#     return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 5\n",
    "model = UNet((TGT_SIZE, TGT_SIZE, 2), \n",
    "             start_ch=16, \n",
    "             depth=DEPTH, \n",
    "             batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay=0.01)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\", mean_iou])\n",
    "# model.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=7, verbose=1),\n",
    "    ReduceLROnPlateau(patience=3, verbose=1),\n",
    "    ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True)]\n",
    "#                     save_weights_only=True)\n",
    "\n",
    "history = model.fit({'img': x_train, \n",
    "                     'feat': depth_train, \n",
    "                     'depth': np.ones((x_train.shape[0], 4, 4, 1))}, # only use first layer\n",
    "                    y_train, \n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=1, #EPOCHS, # only test 1st batch\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n",
    "# Epoch 20/50\n",
    "# 5440/5440 [==============================] - 32s 6ms/step \n",
    "# - loss: 0.1346 \n",
    "# - mean_iou: 0.6909 \n",
    "# - val_loss: 0.2147 \n",
    "# - val_mean_iou: 0.6940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "_ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "_ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "\n",
    "_ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "_ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "\n",
    "_ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "_ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "First we'll get the test data. This takes a while, it's 18000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train, val and test\n",
    "model = load_model('model-tgs-salt-1.h5', custom_objects={'mean_iou': mean_iou})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model.predict(x_valid[...,:1], verbose=1).reshape(-1, TGT_SIZE, TGT_SIZE)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "imgs = train_df.loc[ids_valid[:N]].images_d\n",
    "masks = train_df.loc[ids_valid[:N]].masks\n",
    "preds = preds_valid[:N]\n",
    "plot_imgs_masks(imgs, masks, preds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Score the model and do a threshold optimization by the best IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    print(f'area_true: {area_true}, area_pred: {area_pred}')\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "    print(f'area_true: {area_true[0][0]}, area_pred: {area_pred[0][0]}, union: {union[0][0]}')\n",
    "    \n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "    print(f'iou: {iou}')\n",
    "    print(f'iou: {iou[0][0]}, intersect: {intersection[0][0]}, union: {union[0][0]}')\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        mask = iou > threshold\n",
    "        tp = np.sum(mask, axis=1) == 1   # Correct objects\n",
    "        fp = np.sum(mask, axis=0) == 0  # Missed objects\n",
    "        fn = np.sum(mask, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    \"\"\"Compute IoU batchwise\"\"\"\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    return np.mean([iou_metric(y_true_in[b], y_pred_in[b]) for b in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _union(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return sum((y_true.ravel() + y_pred.ravel()) > 0)\n",
    "    \n",
    "_union(y_valid_ori[0], np.int32(preds_valid[0] > .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return sum(y_true.ravel() * y_pred.ravel())\n",
    "    \n",
    "_intersect(y_valid_ori[0], np.int32(preds_valid[0] > .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iou(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return _intersect(y_true, y_pred)/ max(1e-7, _union(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return sum(y_true.ravel() * y_pred.ravel())\n",
    "\n",
    "def _union(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return sum((y_true.ravel() + y_pred.ravel()) > 0)\n",
    "\n",
    "def _iou(y_true, y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return _intersect(y_true, y_pred)/ max(1e-7, _union(y_true, y_pred))\n",
    "\n",
    "def miou(y_true, y_pred, score_thres=.5):\n",
    "    \"\"\"\"\"\"\n",
    "    score = []\n",
    "    for mask_thres in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_bool = y_pred > mask_thres\n",
    "        score.append(_iou(y_true, y_pred_bool)> score_thres)\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    y_true, y_pred = y_valid_ori[i], np.int32(preds_valid[i] > .5)\n",
    "    f'area_true: {np.sum(y_true)}, area_pred: {np.sum(y_pred)}'\n",
    "    f'miou: {miou(y_true, y_pred)}, intersect: {_intersect(y_true, y_pred)}, union: {_union(y_true, y_pred)}'\n",
    "    iou_metric(y_true, y_pred);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_valid_ori[i], np.int32(preds_valid[i] > .5)\n",
    "\n",
    "def iou_pix(y_true, y_pred, thres_mask, thres_score):\n",
    "    \"\"\"IoU implemented as tp/(tp + fp + fn) at pixel level\n",
    "    \"\"\"\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = (y_pred > thres_mask).ravel()\n",
    "    \n",
    "    tp = sum(y_true * y_pred)\n",
    "    tn = sum((y_true + y_pred) == 0)\n",
    "    fp = sum(y_true < y_pred)\n",
    "    fn = sum(y_true > y_pred)\n",
    "    return int((tp / max(1e-6, (tp + fp + fn))) > thres_score)\n",
    "\n",
    "iou_prec(y_true, y_pred, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_valid_ori[i], np.int32(preds_valid[i] > .5)\n",
    "\n",
    "def iou_msk(y_true, y_pred, thres_mask, thres_score):\n",
    "    \"\"\"IoU implemented as tp/(tp + fp + fn) at mask level\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = int(np.sum(y_true) > 0)\n",
    "    y_pred = int(np.sum(y_pred > thres_mask) > 0)\n",
    "    print(y_true, y_pred)\n",
    "#     tp = sum(y_true * y_pred)\n",
    "#     tn = sum((y_true + y_pred) == 0)\n",
    "#     fp = sum(y_true < y_pred)\n",
    "#     fn = sum(y_true > y_pred)\n",
    "#     return int((tp / max(1e-6, (tp + fp + fn))) > thres_score)\n",
    "\n",
    "iou_prec(y_true, y_pred, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_pix(y_trues, y_preds, thres_score):\n",
    "    \"\"\"\"\"\"\n",
    "    for i in tqdm_notebook(range(len(y_trues))):\n",
    "        np.mean([iou_pix(y_trues[i], y_preds[i], thres_mask, thres_score) for thres_mask in np.linspace(0.1, 0.9, 40)])\n",
    "# \n",
    "# mean_iou_pix(y_valid_ori, preds_valid, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_msk(y_trues, y_preds, thres_score):\n",
    "    \"\"\"\"\"\"\n",
    "    for i in tqdm_notebook(range(len(y_trues))):\n",
    "        print(y_trues[i], y_preds[i])\n",
    "        np.mean([iou_msk(y_trues[i], y_preds[i], thres_mask, thres_score) for thres_mask in np.linspace(0.1, 0.9, 40)])\n",
    "# \n",
    "# mean_iou_msk(y_valid_ori, preds_valid, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_ori[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_metric_batch(y_valid_ori, np.int32(preds_valid > .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold for best IoU score (submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(thresholds, ious)\n",
    "_ = plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "_ = plt.xlabel(\"Threshold\")\n",
    "_ = plt.ylabel(\"IoU\")\n",
    "_ = plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best.round(), iou_best.round()))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "imgs = train_df.loc[ids_valid[:N]].images_d\n",
    "masks = train_df.loc[ids_valid[:N]].masks\n",
    "preds = preds_valid[:N]\n",
    "plot_imgs_masks(imgs, masks, preds_valid, threshold_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check if all indices and images match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = next(os.walk(path_test+\"/images\"))[2]\n",
    "test_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(test_ids) ^ set(test_df.index+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge layers image and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [upsample(np.array(load_img(f\"{path_test}/images/{idx}.png\", grayscale=True))) / 255 \n",
    "                   for idx in tqdm_notebook(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test).reshape(-1, TGT_SIZE, TGT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_d = [np.ones_like(x_test[0]) * test_df.loc[i][\"z\"] / MAX_DEPTH\n",
    "                     for i in tqdm_notebook(test_df.index)] \n",
    "x_test_d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_m = np.dstack((np.array(x_test), np.array(x_test_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_m = np.array([np.dstack((x_test[i], x_test_d[i])) for i, idx in tqdm_notebook(enumerate(test_df.index))])\n",
    "x_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test_m[..., :1]) # only use first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission  \n",
    "\n",
    "Submission is in csv form:\n",
    " - `id`: index (equals filename)\n",
    " - `rle_mask`: run-length format (down-then-right): `masked_pixel_start` `<space>` `length_of_masked_pixels` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', string=True):\n",
    "    \"\"\"Convert binary mask image to run-length array or string.\n",
    "    \n",
    "    pixel==1 locations are returned in format: <start length> ...\n",
    "    \n",
    "    Args:\n",
    "    img: image in shape [n, m]\n",
    "    order: is down-then-right, i.e. Fortran(F)\n",
    "    string: return in string or array\n",
    "\n",
    "    Return:\n",
    "    run-length as an array or string\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if string:\n",
    "        z = ''\n",
    "        for rr in runs:\n",
    "            z += f'{rr[0]} {rr[1]} '\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) \n",
    "             for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encode(text):\n",
    "    '''\n",
    "    Doctest:\n",
    "        >>> encode('WWWWWWWWWWWWBWWWWWWWWWWWWBBBWWWWWWWWWWWWWWWWWWWWWWWWBWWWWWWWWWWWWWW')\n",
    "        '12W1B12W3B24W1B14W'    \n",
    "    '''\n",
    "    from re import sub\n",
    "    return sub(r'(.)\\1*', lambda m: f'{text.index(m.group(0))} {len(m.group(0))} {\".\"} ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '0000111101011'\n",
    "run_length_encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_enc(text):\n",
    "    \"\"\"\"\"\"\n",
    "    idx, rls = 0, ''\n",
    "    text = text + '0'\n",
    "    \n",
    "    def RLE(text):\n",
    "        \"\"\"Run length encoding\n",
    "        add '0' to start text to mark eof\"\"\"\n",
    "        global idx, rls\n",
    "        if idx==0: text += '0'\n",
    "        start = text.index('1')\n",
    "        idx += start\n",
    "        end = text[start:].index('0')\n",
    "        rls += f'{idx} {end} '\n",
    "        idx += start\n",
    "        print(rls)\n",
    "        try:\n",
    "#             print('*', text[start+end:])\n",
    "            RLE(text[start+end:])\n",
    "        except:\n",
    "            print('done!')\n",
    "\n",
    "    RLE(text)\n",
    "    return rls\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(im, order='F'):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    Starts at position 1?\n",
    "    '''\n",
    "    pixels = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(rle_mask, shape=(101, 101), order='F'):\n",
    "    '''\n",
    "    rle_mask: run-length as string formated (start length)\n",
    "    shape: (height, width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = rle_mask.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([[0,1,1,1,0], [0,1,1,0,1], [0,1,1,1,0], [0,1,1,0,1], [0,1,1,0,1]])\n",
    "img.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix = img.reshape(img.shape[0] * img.shape[1], order='F')\n",
    "pix = np.concatenate([[0], pix, [0]])\n",
    "pix\n",
    "# runs = np.where(pix[1:] != pix[:-1])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = np.where(pix[1:] != pix[:-1])[0] + 1 # index shifted by 1?\n",
    "runs\n",
    "runs[1::2] -= runs[::2]\n",
    "runs\n",
    "' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix[1:] * pix[:-1] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_decode(rle_encode(img), img.shape, order='F') == img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLE(text, idx=0, rl=''):\n",
    "    \"\"\"Run length encoding\n",
    "    add '0' to start text to mark eof\"\"\"\n",
    "#         global idx, rl\n",
    "    if idx==0: text += '0'\n",
    "    start = text.index('1')\n",
    "    idx += start\n",
    "    end = text[start:].index('0')\n",
    "    rl += f'{idx} {end} '\n",
    "    idx += start\n",
    "    print(rl)\n",
    "    try:\n",
    "#         print('*', text[start+end:])\n",
    "        RLE(text[start+end:], idx, rl)\n",
    "    except:\n",
    "        print('done!')\n",
    "RLE(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
