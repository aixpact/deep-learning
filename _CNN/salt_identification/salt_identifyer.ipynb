{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Changelog\n",
    "\n",
    "14/aug/18:\n",
    " - standardize depth and input earlier in model\n",
    " - use image cumsum as second layer\n",
    " - deleted data generator for augemnting and replaced by np.fliplr()\n",
    "\n",
    "\n",
    "13/aug/18:\n",
    " - update coverage_class - typo\n",
    " - to prevent kernel from dying(and unable to commit), added code to delete unused lists, features  \n",
    " - update scoring - best iou prediction threshold\n",
    " \n",
    " 12/aug/18:  \n",
    " - Added tuning options:\n",
    "   - SGD optimizer\n",
    "   - kullback_leibler_divergence loss function (doesnot perform well)\n",
    "   - gradient clipping, to control beginning, plateaus and shoulders\n",
    " - update RLenc function\n",
    " - update dice_coef_loss\n",
    " - update coverage_class\n",
    "\n",
    "11/aug/18: \n",
    "- Start UNet with 16 channels (iso 8)\n",
    "- Cleaning code and comments\n",
    "\n",
    "10/aug/18: \n",
    "- Changed hyperparameters\n",
    "- Added depth to middle of model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources that earn credits\n",
    "\n",
    "I got inspiration and insights for this notebook from:  \n",
    "\n",
    "[Alexander Liao](https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice/notebook)   \n",
    "[Jesper Dramsch](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)   \n",
    "[Bruno G, do Amaral](https://www.kaggle.com/bguberfain/unet-with-depth)   \n",
    "[Bartek](https://www.kaggle.com/melgor/u-net-batchnorm-augmentation-stratificat-b0026c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "Seismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n",
    "\n",
    "Imaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n",
    "\n",
    "Seismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\n",
    "You'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n",
    "\n",
    "[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\n",
    "Caption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n",
    "\n",
    "Interpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n",
    "\n",
    "One common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n",
    "\n",
    "Recently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n",
    "\n",
    "+ https://github.com/waldeland/CNN-for-ASI\n",
    "+ https://github.com/bolgebrygg/MalenoV\n",
    "+ https://github.com/crild/facies_net\n",
    "\n",
    "You will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net model\n",
    "\n",
    "The seismic images resemble X-rays and Ultrasound scans. The U-Net model is a CNN used for Biomedical Image Segmentation.   \n",
    "\n",
    "Please read the paper: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\"> U-Net: Convolutional Networks for Biomedical Image Segmentation</a>) \n",
    "\n",
    "<a href=\"https://github.com/jocicmarko/ultrasound-nerve-segmentation\">Another interesting read</a> from the Kaggle Ultrasound Nerve Segmentation competition.</p>\n",
    "\n",
    "<p><img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Set file locations\n",
    "For convenience when working from different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input'\n",
    "path_train = f'{path}/train'\n",
    "path_test = f'{path}/test'\n",
    "imgs_train = f'{path}/train/images'\n",
    "masks_train = f'{path}/train/masks'\n",
    "imgs_test = f'{path}/test/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set/initiate image constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 101   # original/raw image size\n",
    "TGT_SIZE = 128   # model/input image size\n",
    "DPT_SIZE = 4     # CONV filter size when depth is input in model\n",
    "MAX_DEPTH = None # maximum depth('z') of seismic image, set after loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def upsample(img, img_size_target=TGT_SIZE):\n",
    "    \"\"\"Resize image to target shape(model_input)\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def downsample(img, img_size_orig=IMG_SIZE):\n",
    "    \"\"\"Resize image to original shape\"\"\"\n",
    "    img_size = img.shape[0]\n",
    "    if img_size == img_size_orig:\n",
    "        return img\n",
    "    return resize(img, (img_size_orig, img_size_orig), mode='constant', preserve_range=True)\n",
    "\n",
    "\n",
    "def imgs_2_array(path, img_names, ftype='.png', size=TGT_SIZE):\n",
    "    \"\"\"Load images as arrays in target shape: (-1, target, target, 1)\"\"\"\n",
    "    imgs = [upsample(np.array(load_img(f'{path}/{name}{ftype}', grayscale=True))) / 255\n",
    "                      for name in tqdm_notebook(img_names)]\n",
    "    imgs.extend([np.fliplr(img) for img in imgs]) # extend set with flips\n",
    "    imgs = np.array(imgs).reshape(-1, size, size, 1)\n",
    "    return imgs\n",
    "        \n",
    "    \n",
    "def csum(img, border=2):\n",
    "    \"\"\"Create image cumsum from image\n",
    "    Sort of image bleeding downwards\"\"\"\n",
    "    center_mean = img[border:-border, border:-border].mean()\n",
    "    csum = (np.float32(img)-center_mean).cumsum(axis=0)         \n",
    "    csum -= csum[border:-border, border:-border].mean()\n",
    "    csum /= max(1e-3, csum[border:-border, border:-border].std())\n",
    "    return csum\n",
    "\n",
    "\n",
    "def imgs_2_csum(path, img_names, ftype='.png', size=TGT_SIZE):\n",
    "    \"\"\"Load images and transform to array with image and cumsum layer\"\"\"\n",
    "    imgs = imgs_2_array(path, img_names, ftype, size)\n",
    "    img_csums = [csum(img) for img in tqdm_notebook(imgs)]\n",
    "    return np.concatenate((np.array(imgs), np.array(img_csums)) , axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Load and build train and test set.\n",
    "\n",
    " - contemplating to use depth as a layer, this explains the stacking in `images_d`.\n",
    " - checking depth\n",
    " - checking coverage\n",
    " - checking depth - coverage relationship\n",
    " - visualize seismic images with masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(f'{path}/train.csv', index_col=\"id\", usecols=[0])\n",
    "depths_df_ = pd.read_csv(f'{path}/depths.csv', index_col=\"id\")\n",
    "train_df_ = train_df_.join(depths_df_)\n",
    "train_indices = train_df_.index.values  # TODO\n",
    "\n",
    "# Flip(augment) train images -> first duplicate train_df: images & depth\n",
    "train_df = pd.concat([train_df_, train_df_])\n",
    "train_df.index = np.concatenate([train_indices, train_indices+'_'])\n",
    "\n",
    "# Test set\n",
    "test_df = depths_df_[~depths_df_.index.isin(train_indices)]\n",
    "\n",
    "# Free up some RAM\n",
    "del depths_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.index[:5], train_df.index[4000:4005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use without cumsum layer\n",
    "# imgs = imgs_2_array(imgs_train, train_indices, '.png', TGT_SIZE)\n",
    "# train_df[\"images\"] = [img for img in imgs]\n",
    "\n",
    "# Use with cumsum layer\n",
    "X_imgs = imgs_2_csum(imgs_train, train_indices, '.png', TGT_SIZE)\n",
    "train_df[\"images_d\"] = [img for img in X_imgs]\n",
    "\n",
    "X_masks = imgs_2_array(masks_train, train_indices, '.png', TGT_SIZE)\n",
    "train_df[\"masks\"] = [mask for mask in X_masks]\n",
    "\n",
    "# Normalize depth\n",
    "depth = train_df[\"z\"]\n",
    "mean_depth, std_depth, max_depth = depth.mean(), depth.std(), depth.max()\n",
    "X_norm_depth = (depth - mean_depth) / std_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check flip image have same depth\n",
    "X_norm_depth[:5].values == X_norm_depth[4000:4005].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[4000:4005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    " - checking the distribution\n",
    " \n",
    "*As per below: depth is 'normal' distributed and train and test set have same distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(train_df.z, label=\"Train\")\n",
    "_ = sns.distplot(test_df.z, label=\"Test\")\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask coverage\n",
    "\n",
    "*As per below: there are 8-10 times more seismic images with 0-10% salt areas. Stratification in train-validation split must prevent overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(mask):\n",
    "    \"\"\"Compute salt mask coverage\"\"\"\n",
    "    return np.sum(mask) / (mask.shape[0]*mask.shape[1])\n",
    "\n",
    "\n",
    "def norm_coverage(masks):\n",
    "    \"\"\"Compute salt mask coverage\"\"\"\n",
    "    coverages = np.array([coverage(mask) for mask in masks])\n",
    "    mean_cov, std_cov, max_cov = coverages.mean(), coverages.std(), coverages.max()\n",
    "    return (coverages - mean_cov) / std_cov\n",
    "\n",
    "\n",
    "def coverage_class(mask):\n",
    "    \"\"\"Compute salt mask coverage class\"\"\"\n",
    "    if coverage(mask) == 0:\n",
    "        return 0\n",
    "    return (coverage(mask) * 100 //10).astype(np.int8) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = [(coverage(mask)-.01) * 100//10 + 1 for mask in X_masks]\n",
    "_ = sns.distplot(coverage_label, label=\"Train\", kde=False)\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Coverage distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth vs. coverage\n",
    "\n",
    " - checking the correlation between depth and coverage\n",
    " \n",
    "*As per below: no pattern or correlation visible, depth and coverage are unrelated. Depth might still be a factor in the prediction, e.g. the structure/grain might relate to depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_cover = norm_coverage(train_df.masks)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(len(X_norm_depth)), X_norm_depth, alpha=.5, label='Normalized Seismic Depth')\n",
    "plt.scatter(range(len(salt_cover)), salt_cover, color='r', alpha=.5, label='Normalized Salt Coverage')\n",
    "plt.title('Normalized Depth vs. Salt coverage as % of image size', fontsize=20)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, fontsize=16);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images and masks (overlayed)\n",
    "\n",
    "Helper function for using throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_masks(imgs, masks, **kwargs):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n",
    "    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n",
    "    \"\"\"\n",
    "    depth = kwargs.get('depth', None)\n",
    "    preds_valid = kwargs.get('preds_valid', None)\n",
    "    thres = kwargs.get('thres', None)\n",
    "    grid_width = kwargs.get('grid_width', 10)\n",
    "    zoom = kwargs.get('zoom', 1.5)\n",
    "    \n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, (img, mask) in enumerate(zip(imgs, masks)):\n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(img[..., 1], alpha=0.15, cmap=\"seismic\") # TODO\n",
    "        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            pred = preds_valid[i]\n",
    "            pred = pred[..., 0]\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(pred > thres), dtype=np.float32)\n",
    "                iou = f'IoU: {_iou(mask, pred).round(3)}'\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "                _ = ax.text(2, img.shape[0]-2, iou, color=\"k\")\n",
    "            else:\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n",
    "            \n",
    "        if depth is not None:\n",
    "            _ = ax.text(2, img.shape[0]-2, f'depth: {depth[i]}', color=\"k\")\n",
    "#         _ = ax.text(img.shape[0]-2, 2, coverage_class(mask), color=\"k\", ha=\"right\", va=\"top\")\n",
    "        _ = ax.text(2, 2, f'{coverage(mask).round(3)}({coverage_class(mask)})', color=\"k\", ha=\"left\", va=\"top\")\n",
    "        \n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "plot_imgs_masks(train_df.iloc[:N].images_d, train_df.iloc[:N].masks, depth=train_df.iloc[:N].z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split\n",
    "\n",
    "Validation set will be used for saving model checkpoints and early stopping. \n",
    "Tradeoff:\n",
    " - larger validation set for more validation accuracy and saving better generalizing model, which means less training samples and thus less generalizing model.\n",
    "\n",
    "Using images with depth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.15\n",
    "\n",
    "ids_train, ids_valid, X_train, X_valid, Y_train, Y_valid, depth_train, depth_valid = train_test_split(\n",
    "    train_df.index.values,\n",
    "    X_imgs,\n",
    "    X_masks,\n",
    "    np.array(X_norm_depth).reshape(-1, 1),\n",
    "    test_size=VAL_SIZE, \n",
    "    stratify=train_df.masks.map(coverage_class), \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, depth_train.shape, depth_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "imgs = train_df.loc[ids_valid[:N]].images_d\n",
    "masks = train_df.loc[ids_valid[:N]].masks\n",
    "plot_imgs_masks(imgs, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metric(s)\n",
    "\n",
    "Custom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(Y_true, Y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean(IoU) metric\n",
    "    IoU = intersection / union\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - convert probability mask to boolean mask based on given threshold\n",
    "     - score the mask 1 if(IoU > score_threshold(0.5))\n",
    "    Take the mean of the scoress\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            score = tf.identity(score) #!! use identity to transform score to tensor\n",
    "        prec.append(score) \n",
    "        \n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - check below loss function\n",
    "# smooth=K.epsilon()\n",
    "def dice_coef(Y_true, Y_pred, smooth=1):\n",
    "    \"\"\"\"\"\"\n",
    "#     Y_true_f = K.flatten(Y_true)\n",
    "#     Y_pred_f = K.flatten(Y_pred)\n",
    "    assert Y_true.shape == Y_pred.shape\n",
    "    intersection = K.dot(Y_true, K.transpose(Y_pred))\n",
    "    union = K.dot(Y_true, K.transpose(Y_true)) + K.dot(Y_pred, K.transpose(Y_pred))\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "def dice_coef_loss(Y_true, Y_pred):\n",
    "    \"\"\"\"\"\"\n",
    "    return K.mean(1. - dice_coef(Y_true, Y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "Below functions create the UNet model in a functional and recursive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(m, ch_dim, acti, bn, res, do=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "def input_feature(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    f_repeat = RepeatVector(xx*xx)(f)\n",
    "    f_conv = Reshape((xx, xx, n_features))(f_repeat)\n",
    "    n = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_block(m, input_depth, inp_feat, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res):\n",
    "    \"\"\"Recursive CNN builder\"\"\"\n",
    "    featureOK = True\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, ch_dim, acti, bn, res) # no drop-out\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n",
    "        if featureOK and (depth==DEPTH-1):\n",
    "            m = Concatenate()([m, input_feature(inp_feat, m)])\n",
    "        m = level_block(m, input_depth, inp_feat,\n",
    "                        int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res)\n",
    "        \n",
    "        # Unwind recursive stack calls - creating the upscaling part of the model\n",
    "        if up:\n",
    "            # Repeat the rows and columns of the data by 2 and 2 respectively\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            # Transposed convolutions are going in the opposite direction of a normal convolution\n",
    "            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, ch_dim, acti, bn, res)\n",
    "    else:\n",
    "        # Depth == 0 - deepest conv_block\n",
    "        m = conv_block(m, ch_dim, acti, bn, res, do)\n",
    "        # Input and concat depth information in the middle layer\n",
    "        m = Concatenate()([m, input_depth])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n",
    "    \"\"\"Returns model\"\"\"\n",
    "    inputs = Input(shape=img_shape, name='img')\n",
    "    input_depth = Input(shape=(DPT_SIZE, DPT_SIZE, 1), name='depth')\n",
    "    inp_feat = Input(shape=(1,), name='feat')\n",
    "    outputs = level_block(inputs, input_depth, inp_feat, \n",
    "                          start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "    return Model(inputs=[inputs, input_depth, inp_feat], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CH = 2    # layers of image\n",
    "CONV_CH = 16  # number of channels to start/end UNet with\n",
    "DEPTH = 5     # number of CONV blocks to max model depth\n",
    "D_OUT = 0.3\n",
    "BN = True\n",
    "UP_CONV = False\n",
    "RES = True\n",
    "\n",
    "model = UNet((TGT_SIZE, TGT_SIZE, IMG_CH), \n",
    "             start_ch=CONV_CH, \n",
    "             depth=DEPTH, \n",
    "             dropout=D_OUT,\n",
    "             batchnorm=BN, \n",
    "             upconv=UP_CONV,\n",
    "             residual=RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-3\n",
    "# Define optimizer\n",
    "# Clip gradients to norm 1., \n",
    "optimizer = [Adam(lr=LR, beta_1=0.9, beta_2=0.9999, decay=LR/100, clipvalue=.5),\n",
    "             SGD(lr=LR, decay=LR/100, momentum=0.9, nesterov=True, clipnorm=1.)]\n",
    "\n",
    "# Define loss\n",
    "loss = [dice_coef_loss, \"binary_crossentropy\", \"kullback_leibler_divergence\"]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss[1], optimizer=optimizer[0], metrics=[\"accuracy\", mean_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'TGS_salt_UNet_{IMG_CH}_{CONV_CH}_{DEPTH}_{D_OUT>0}_{BN}_{UP_CONV}_{RES}.h5'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_train.shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=7, verbose=1),\n",
    "    ReduceLROnPlateau(patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, verbose=1)]\n",
    "\n",
    "X_train_dict = {'img': X_train, \n",
    "              'feat': depth_train, \n",
    "              'depth': np.ones((X_train.shape[0], DPT_SIZE, DPT_SIZE, 1))} # depth for later use?\n",
    "\n",
    "X_val_dict = {'img': X_valid, \n",
    "              'feat': depth_valid, \n",
    "              'depth': np.ones((X_valid.shape[0], DPT_SIZE, DPT_SIZE, 1))} # depth for later use?\n",
    "\n",
    "history = model.fit(X_train_dict, \n",
    "                    Y_train, \n",
    "                    validation_data=(X_val_dict, Y_valid),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Learning curves\n",
    "\n",
    "1. Check the curves for errors  \n",
    "2. Check the curves for roughness and convergence to tune the hyperparameters:  \n",
    " - learning rate and decay\n",
    " - batch size\n",
    " - model architecture\n",
    " - epochs and earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "_ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "_ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "_ = ax_loss.legend()\n",
    "_ = ax_loss.set_title('Loss')\n",
    "_ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "_ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "_ = ax_acc.legend()\n",
    "_ = ax_acc.set_title('Accuracy')\n",
    "_ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "_ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")\n",
    "_ = ax_iou.legend()\n",
    "_ = ax_iou.set_title('IoU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = load_model(model_name, custom_objects={'mean_iou': mean_iou})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on validation set\n",
    "model.evaluate(X_val_dict, Y_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "preds_valid = model.predict(X_val_dict, verbose=1).reshape(-1, TGT_SIZE, TGT_SIZE)\n",
    "preds_valid = preds_valid.reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n",
    "# preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "# Y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid]) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# cm = confusion_matrix(y_test, pred)\n",
    "# cm\n",
    "# # Accuracy\n",
    "# np.sum(np.eye(2) * cm) / np.sum(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize probability('float') masks\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "imgs = train_df.loc[ids_valid[:N]].images_d\n",
    "masks = train_df.loc[ids_valid[:N]].masks\n",
    "preds = [pred for pred in preds_valid[:N]]\n",
    "plot_imgs_masks(imgs, masks, preds_valid=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Score the model and do a threshold optimization by the best IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iou(Y_true, Y_pred):\n",
    "    \"\"\"IoU\"\"\"\n",
    "    Y_true_f, Y_pred_f = Y_true.ravel(), Y_pred.ravel()\n",
    "    intersection = np.sum(Y_true_f * Y_pred_f)\n",
    "    union = np.sum((Y_true_f + Y_pred_f) > 0)\n",
    "    return intersection/ max(1e-9, union)\n",
    "\n",
    "def miou(Y_trues, Y_preds):\n",
    "    \"\"\"Mean intersection over union\"\"\"\n",
    "    return np.mean([_iou(Y_trues[i], Y_preds[i]) for i in range(Y_trues.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold for best IoU score (submission)\n",
    "\n",
    "Checking which threshold delivers the best IoU score, so this threshold will be used for Test set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 80)\n",
    "ious = np.array([miou(Y_valid, np.int32(preds_valid > threshold)) \n",
    "                 for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious)\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(thresholds, ious)\n",
    "_ = plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "_ = plt.xlabel(\"Threshold\")\n",
    "_ = plt.ylabel(\"IoU\")\n",
    "_ = plt.title(\"Threshold: {} delivers best mean-IoU: {} \".format(threshold_best.round(3), iou_best.round(3)))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize binary masks based on best threshold\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "imgs = train_df.loc[ids_valid[:N]].images_d\n",
    "masks = train_df.loc[ids_valid[:N]].masks\n",
    "preds = preds_valid[:N]\n",
    "plot_imgs_masks(imgs, masks, preds_valid=preds_valid, thres=threshold_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check if all indices and images match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = next(os.walk(path_test+\"/images\"))[2]\n",
    "assert len(set(test_ids) ^ set(test_df.index+'.png')) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Convert test set images and depths to layers\n",
    "\n",
    " - Upsample images to arrays\n",
    " - Reshape for modeling\n",
    " - Create depth layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [upsample(np.array(load_img(f\"{path_test}/images/{idx}.png\", grayscale=True))) / 255 \n",
    "                   for idx in tqdm_notebook(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test).reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create depth layer\n",
    "X_test_d = [np.ones((4,4,1)) * (test_df.loc[i][\"z\"] / MAX_DEPTH)\n",
    "                     for i in tqdm_notebook(test_df.index)] \n",
    "X_test_d = np.array(X_test_d).reshape(-1, DPT_SIZE, DPT_SIZE, 1)\n",
    "X_test_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict({'img': X_test, 'depth': X_test_d}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission  \n",
    "\n",
    "Submission is in csv form:\n",
    " - `id`: index (equals filename)\n",
    " - `rle_mask`: run-length format (down-then-right): `masked_pixel_start` `<space>` `length_of_masked_pixels` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F'):\n",
    "    \"\"\"Convert binary mask image to run-length array or string.\n",
    "    \n",
    "    Args:\n",
    "    img: image in shape [n, m]\n",
    "    order: is down-then-right, i.e. Fortran(F)\n",
    "    string: return in string or array\n",
    "\n",
    "    Return:\n",
    "    run-length as a string: <start[1s] length[1s] ... ...>\n",
    "    \"\"\"\n",
    "    bytez = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    bytez = np.concatenate([[0], bytez, [0]])\n",
    "    runs = np.where(bytez[1:] != bytez[:-1])[0] + 1 # pos start at 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Use for sanity check the encode function\n",
    "def RLdec(rl_string, shape=(101, 101), order='F'):\n",
    "    \"\"\"Convert run-length string to binary mask image.\n",
    "    \n",
    "    Args:\n",
    "    rl_string: \n",
    "    shape: target shape of array\n",
    "    order: decode order is down-then-right, i.e. Fortran(F)\n",
    "\n",
    "    Return:\n",
    "    binary mask image as array\n",
    "    \"\"\"\n",
    "    s = rl_string.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) \n",
    "             for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')\n",
    "sub.head()\n",
    "print('submission saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for sharing your thoughts!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
