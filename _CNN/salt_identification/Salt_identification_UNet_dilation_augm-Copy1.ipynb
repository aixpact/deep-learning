{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog <a name='A'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "# csum\n",
    "2 .  import imgaug\n",
    "https://arxiv.org/pdf/1706.05587.pdf\n",
    "\n",
    "nadam . = 0.38!\n",
    "adam, 30pochs score = 0.38! - adam werkt niet goed?\n",
    "DEPTH 32x32 = 0.756 (40 epochs)\n",
    "DEPTH 32x32 = 0.753 (10 epochs) . \n",
    "DEPTH 64x64 = 0.736\n",
    "\n",
    "#\n",
    "batch=32, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "/// {'adam': Adam(lr=LR, beta_1=0.9, beta_2=0.999, clipnorm=8.),\n",
    "test score=0.763\n",
    "\n",
    "#\n",
    "batch=48, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "'rmsprop': RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "test score=0.762\n",
    "\n",
    "#\n",
    "batch=16, rmsprop, patience=13, lrdecay=4, val_loss  \n",
    "test score=0.753\n",
    "\n",
    "#\n",
    "batch=24, rmsprop, patience=11, lrdecay=4, val_loss  \n",
    "test score=0.764\n",
    "\n",
    "#\n",
    "batch =32\n",
    "metric = val_miou\n",
    "test score=0.745\n",
    "\n",
    "#\n",
    "UP_CONV = False\n",
    "RES = True  \n",
    "'adadelta'\n",
    "\n",
    "21/aug/2018  \n",
    " - initial commit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents: <a name='TOC'></a>\n",
    "[Changelog](#A)  \n",
    "[Sources](#B)  \n",
    "[About](#C)  \n",
    "[U-Net model](#D)  \n",
    "[Setup notebook](#E)  \n",
    "[Data Exploration](#F)  \n",
    "[Visualize images and masks (overlayed)](#G)  \n",
    "[Train validation split](#H)  \n",
    "[Custom metrics](#I)  \n",
    "[Model architecture](#J)  \n",
    "[Build model](#K)  \n",
    "[Train model](#L)  \n",
    "[Learning curves](#M)  \n",
    "[Check performance on validation set](#N)  \n",
    "[Scoring](#O)  \n",
    "[Post-processing: blur and threshold](#Q)   \n",
    "[Predict test set](#P)   \n",
    "[Submission](#R)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources that earn credits <a name='B'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "I got initial inspiration and insights for this notebook from:  \n",
    "\n",
    "[Alexander Liao](https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice/notebook)  -   Model architecture , dice loss  \n",
    "[Jesper Dramsch](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)   - Seismic intro  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "(*Jesper Dramsch*)<a name='C'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Seismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n",
    "\n",
    "Imaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n",
    "\n",
    "Seismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\n",
    "You'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n",
    "\n",
    "[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\n",
    "Caption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n",
    "\n",
    "Interpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n",
    "\n",
    "One common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n",
    "\n",
    "Recently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n",
    "\n",
    "+ https://github.com/waldeland/CNN-for-ASI\n",
    "+ https://github.com/bolgebrygg/MalenoV\n",
    "+ https://github.com/crild/facies_net\n",
    "\n",
    "You will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net model<a name='D'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "The seismic images resemble X-rays and Ultrasound scans. The U-Net model is a CNN used for Biomedical Image Segmentation.   \n",
    "\n",
    "Please read the paper: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\"> U-Net: Convolutional Networks for Biomedical Image Segmentation</a>) \n",
    "\n",
    "<a href=\"https://github.com/jocicmarko/ultrasound-nerve-segmentation\">Another interesting read</a> from the Kaggle Ultrasound Nerve Segmentation competition.</p>\n",
    "\n",
    "<p><img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setup notebook<a name='E'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Commit/Development mode\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 5\n",
    "COMMIT = False\n",
    "DEV = not COMMIT\n",
    "DBG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import resource\n",
    "from memory_profiler import profile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "import ipywidgets as ipy\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, AtrousConvolution2D\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Nadam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_used():\n",
    "    \"\"\"Memory used\"\"\"\n",
    "    return round(resource.getrusage(resource.RUSAGE_SELF)[2] * 10/1028 / 10, 1)\n",
    "\n",
    "def mem_fun(fun, **kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    mem_start = mem_used()\n",
    "    _ = fun(**kwargs)\n",
    "    print(f'memory used by function: {(mem_used() - mem_start):.1f}mb')\n",
    "    \n",
    "# \n",
    "def test_alloc_size(n, m, c):\n",
    "    \"\"\"Check memory allocation needed for train/test set\"\"\"\n",
    "    imgs_test_alloc = list(range(n))\n",
    "    for i in range(n):\n",
    "        imgs_test_alloc[i] = np.ones((m, TGT_SIZE, TGT_SIZE, c)) * random.randint(0,100)\n",
    "    return imgs_test_alloc\n",
    "    \n",
    "if DBG:\n",
    "    mem_fun(test_alloc_size, n=1, m=18000, c=2)\n",
    "    print(f'notebook memory used: {mem_used()}mb')\n",
    "    ## memory used by function: 4482.3mb for (1800, 128, 1288, 2)\n",
    "    ## notebook memory used: 4805.7mb\n",
    "\n",
    "nb_mem = mem_used()\n",
    "f'Initial memory used by this notebook: {nb_mem}mb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set file locations\n",
    "For convenience when working from different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.'\n",
    "path = './input'\n",
    "path_train = f'{path}/train'\n",
    "path_test = f'{path}/test'\n",
    "imgs_train = f'{path}/train/images'\n",
    "masks_train = f'{path}/train/masks'\n",
    "imgs_test = f'{path}/test/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path names\n",
    "train_img_filepaths = sorted(glob.glob(f'{imgs_train}/*'))\n",
    "train_mask_filepaths = sorted(glob.glob(f'{masks_train}/*'))\n",
    "test_img_filepaths = sorted(glob.glob(f'{imgs_test}/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first glimpse at images and masks\n",
    "if DEV:\n",
    "    @ipy.interact(idx=ipy.IntSlider(min=1, max=len(train_img_filepaths)), value=10, step=1)\n",
    "    def plot(idx):\n",
    "        \"\"\"Navigate thru images and masks\"\"\"\n",
    "        img = np.array(load_img(train_img_filepaths[idx], grayscale=True))\n",
    "        mask = np.array(load_img(train_mask_filepaths[idx], grayscale=True))\n",
    "        fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "        axs[0].imshow(img, cmap=\"Greys\")\n",
    "        axs[1].imshow(mask, cmap=\"Greens\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/61720\n",
    "# To consider exclude from training, not knowing if such masks are in LB test set\n",
    "BAD_MASKS =[\n",
    "    '1eaf42beee','33887a0ae7','33dfce3a76','3975043a11','39cd06da7d','483b35d589','49336bb17b','4ef0559016',\n",
    "    '4fbda008c7','4fdc882e4b','50d3073821','53e17edd83','5b217529e7','5f98029612','608567ed23','62aad7556c',\n",
    "    '62d30854d7','6460ce2df7','6bc4c91c27','7845115d01','7deaf30c4a','80a458a2b6','81fa3d59b8','8367b54eac',\n",
    "    '849881c690','876e6423e6','90720e8172','916aff36ae','919bc0e2ba','a266a2a9df','a6625b8937','a9ee40cf0d',\n",
    "    'aeba5383e4','b63b23fdc9','baac3469ae','be7014887d','be90ab3e56','bfa7ee102e','bfbb9b9149','c387a012fc',\n",
    "    'c98dfd50ba','caccd6708f','cb4f7abe67','d0bbe4fd97','d4d2ed6bd2','de7202d286','f0c401b64b','f19b7d20bb',\n",
    "    'f641699848','f75842e215','00950d1627','0280deb8ae','06d21d76c4','09152018c4','09b9330300','0b45bde756',\n",
    "    '130229ec15','15d76f1672','182bfc6862','23afbccfb5','24522ec665','285f4b2e82','2bc179b78c','2f746f8726',\n",
    "    '3cb59a4fdc','403cb8f4b3','4f5df40ab2','50b3aef4c4','52667992f8','52ac7bb4c1','56f4bcc716','58de316918',\n",
    "    '640ceb328a','71f7425387','7c0b76979f','7f0825a2f0','834861f1b6','87afd4b1ca','88a5c49514','9067effd34',\n",
    "    '93a1541218','95f6e2b2d1','96216dae3b','96523f824a','99ee31b5bc','9a4b15919d','9b29ca561d','9eb4a10b98',\n",
    "    'ad2fa649f7','b1be1fa682','b24d3673e1','b35b1b412b','b525824dfc','b7b83447c4','b8a9602e21','ba1287cb48',\n",
    "    'be18a24c49','c27409a765','c2973c16f1','c83d9529bd','cef03959d8','d4d34af4f7','d9a52dc263','dd6a04d456',\n",
    "    'ddcb457a07','e12cd094a6','e6e3e58c43','e73ed6e7f2','f6e87c1458','f7380099f6','fb3392fee0','fb47e8e74e','febd1d2a67']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images and masks (overlayed)<a name='G'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Helper functions:\n",
    "- upsample image\n",
    "- ploting overlayed images in grid - for using throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 101   # original/raw image size\n",
    "TGT_SIZE = 128   # tunable hyperparam: model/input image size {128, 256, 512} - for this 14gb RAM kernel 128 is max. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(img, dim=TGT_SIZE, interpolation=cv2.INTER_LINEAR):\n",
    "    \"\"\"Resize image to target shape(model_input) or back to original shape\n",
    "    \n",
    "    INTER_NEAREST - a nearest-neighbor interpolation\n",
    "    INTER_LINEAR - a bilinear interpolation (used by default)\n",
    "    INTER_AREA - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moireâ€™-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.\n",
    "    INTER_CUBIC - a bicubic interpolation over 4x4 pixel neighborhood\n",
    "    INTER_LANCZOS4 - a Lanczos interpolation over 8x8 pixel neighborhood\n",
    "    To shrink an image, it will generally look best with CV_INTER_AREA interpolation, whereas to enlarge an image, \n",
    "    it will generally look best with CV_INTER_CUBIC(slow) or CV_INTER_LINEAR(faster but still looks OK).\n",
    "    \"\"\"\n",
    "    if img.shape[0] == dim:\n",
    "        return img\n",
    "    return cv2.resize(img, (dim, dim), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_masks(imgs, masks, **kwargs):\n",
    "    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n",
    "    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n",
    "    Arguments:\n",
    "    imgs: vector(-1, h, w, ch)\n",
    "    masks: vector(-1, h, w, ch)\n",
    "    \"\"\"\n",
    "    depth = kwargs.get('depth', None)\n",
    "    coverage = kwargs.get('coverage', None)\n",
    "    cov_class = kwargs.get('cov_class', None)\n",
    "    preds_valid = kwargs.get('preds_valid', None)\n",
    "    thres = kwargs.get('thres', None)\n",
    "    grid_width = kwargs.get('grid_width', 10)\n",
    "    zoom = kwargs.get('zoom', 1.5)\n",
    "    title = kwargs.get('title', \"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\")\n",
    "    \n",
    "    grid_height = 1 + (len(imgs)-1) // grid_width\n",
    "    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n",
    "    axes = axs.ravel()\n",
    "    \n",
    "    for i, (img, mask) in enumerate(zip(imgs, masks)):\n",
    "        \n",
    "        ax = axes[i] #//grid_width, i%grid_width]\n",
    "        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n",
    "        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n",
    "        \n",
    "        if preds_valid is not None:\n",
    "            pred = preds_valid[i]\n",
    "            pred = pred[..., 0]\n",
    "            if thres is not None:\n",
    "                pred = np.array(np.round(pred > thres), dtype=np.float32)\n",
    "                iou = f'IoU: {IoU(mask, pred)}'\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "                _ = ax.text(2, img.shape[0]-2, iou, color=\"k\")\n",
    "            else:\n",
    "                _ = ax.imshow(pred, alpha=0.3, cmap=\"Oranges\")\n",
    "            \n",
    "        if depth is not None:\n",
    "            _ = ax.text(2, img.shape[0]-2, f'depth: {depth[i].round(3)}', color=\"k\")\n",
    "        if (coverage is not None) and (cov_class is not None):   \n",
    "            _ = ax.text(2, 2, f'{coverage[i].round(3)}({cov_class[i]})', color=\"k\", ha=\"left\", va=\"top\")\n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = plt.axis('off')\n",
    "    plt.suptitle(title, y=1+.5/grid_height, fontsize=20)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise sample - get a feel of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    N = 100\n",
    "    samples = random.sample(list(range(N)), N)\n",
    "    X = [np.array(load_img(train_img_filepaths[sample], grayscale=True))[...,np.newaxis] for sample in samples]\n",
    "    Y = [np.array(load_img(train_mask_filepaths[sample], grayscale=True))[...,np.newaxis] for sample in samples]\n",
    "    plot_imgs_masks(X, Y) #, coverage=Y_coverages[sample], cov_class=Y_cov_class[sample], depth=X_norm_depth[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration<a name='F'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Load and build train and test set.\n",
    "\n",
    " - contemplating to use depth as a layer, this explains the stacking in `images_d`.\n",
    " - checking depth\n",
    " - checking coverage\n",
    " - checking depth - coverage relationship\n",
    " - visualize seismic images with masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv files\n",
    "\n",
    "These files contain image filenames(without .png) and seismic depths of the images.  \n",
    " - train.csv - filenames of train set  \n",
    " - depths.csv - depths of all images test & train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(f'{path}/train.csv', index_col=\"id\", usecols=[0])\n",
    "depths_df_ = pd.read_csv(f'{path}/depths.csv', index_col=\"id\") # train and test\n",
    "train_df = train_df_.join(depths_df_)\n",
    "test_df = depths_df_[~depths_df_.index.isin(train_df_.index.values)]\n",
    "\n",
    "# Indices\n",
    "train_indices = train_df.index.values\n",
    "test_indices = test_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some RAM\n",
    "del depths_df_\n",
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    " - checking the distribution\n",
    " \n",
    "*As per below: depth is 'normal' distributed and train and test set have same distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = train_df.z\n",
    "mean_depth, std_depth, max_depth = depth.mean(), depth.std(), depth.max()\n",
    "X_norm_depth = (depth - mean_depth) / std_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    _ = sns.distplot(train_df.z, label=\"Train\")\n",
    "    _ = sns.distplot(test_df.z, label=\"Test\")\n",
    "    _ = plt.legend()\n",
    "    _ = plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA dept, coverage, std, mean\n",
    "\n",
    " - checking the correlation between depth and coverage\n",
    " \n",
    " Stretching and offsetting the normalized salt coverage for visual comparison reason only\n",
    " \n",
    "*As per below: no pattern or correlation visible, depth and coverage are unrelated. Depth might still be a factor in the prediction, e.g. the structure/grain might relate to depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage\n",
    "Y_cov = [(np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2,      # salt coverage\n",
    "              (np.sum(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) // 255) / TGT_SIZE**2  # salt coverage class\n",
    "                  - .01) * 100//10 + 1) for name in tqdm_notebook(train_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_coverages, Y_cov_class = zip(*Y_cov)\n",
    "Y_coverages = np.array(Y_coverages)\n",
    "Y_cov_class = np.array(Y_cov_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation in images\n",
    "X_std = [np.std(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)))  \n",
    "         for name in tqdm_notebook(train_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean in images\n",
    "X_mean = [np.mean(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)))  \n",
    "         for name in tqdm_notebook(train_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_stack = np.stack([train_indices, np.array(X_mean), np.array(X_std), Y_coverages, Y_cov_class]).T\n",
    "np_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np_stack, columns=['id', 'mean', 'stddev', 'coverage', 'cov_class']).set_index('id')\n",
    "df.stddev = pd.to_numeric(df.stddev) #.astype(np.float)\n",
    "df['mean'] = pd.to_numeric(df['mean'])\n",
    "df.coverage = pd.to_numeric(df.coverage)\n",
    "df['cov_norm'] = (df.coverage - np.mean(df.coverage)) / np.std(df.coverage)\n",
    "df.cov_class = pd.to_numeric(df.cov_class).astype(np.int8)\n",
    "df['depth'] = depth\n",
    "df['depth_norm'] = X_norm_depth\n",
    "df['std_class'] = np.digitize(df.stddev, [0., 5., 10., 20., 30., 40., 50., 60., 70, 1000.])\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lmplot('stddev', 'cov_norm', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Pairplot\n",
    "    _ = sns.pairplot(df.loc[:, ['depth', 'cov_norm']], size=6)\n",
    "    _ = sns.pairplot(df.loc[df.cov_class!=0, ['depth', 'cov_norm']], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Pairplot\n",
    "    _ = sns.pairplot(df.loc[df.cov_class!=0, ['mean', 'cov_norm']], size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('stddev', 'coverage', data=df, color='b', kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('mean', 'coverage', data=df, color='b', kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    plt.figure(figsize=(20,8))\n",
    "    with sns.axes_style(style=None):\n",
    "        sns.violinplot('cov_class', 'stddev', data=df, inner=\"quartile\") #\n",
    "#                        split=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    plt.figure(figsize=(20,8))\n",
    "    with sns.axes_style(style=None):\n",
    "        sns.violinplot('std_class', 'coverage', data=df, inner=\"quartile\") #\n",
    "#                        split=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    plt.figure(figsize=(20,8))\n",
    "    with sns.axes_style(style=None):\n",
    "        sns.violinplot('std_class', 'mean', data=df, inner=\"quartile\") #\n",
    "#                        split=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "g = sns.PairGrid(df, vars=['depth_norm', 'cov_norm', 'mean', 'stddev'], hue='cov_class', palette='RdBu_r')\n",
    "g.map(plt.scatter, alpha=0.5)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    std_zero = set(df[df.stddev<5].index) | set(df[df.stddev>45].index) | set(df[df['mean']<50].index) | set(df[df['mean']>200].index)\n",
    "    N = 200\n",
    "    samples = random.sample(std_zero, min(len(std_zero), N))\n",
    "    X = [np.array(load_img(f'{imgs_train}/{sample}.png', grayscale=True))[...,np.newaxis] for sample in samples]\n",
    "    Y = [np.array(load_img(f'{masks_train}/{sample}.png', grayscale=True))[...,np.newaxis] for sample in samples]\n",
    "    plot_imgs_masks(X, Y, title='Stand dev is zero') #, coverage=Y_coverages[sample], cov_class=Y_cov_class[sample], depth=X_norm_depth[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage class\n",
    "\n",
    "*As per below: there are 8-10 times more seismic images with 0-10% salt areas. Stratification in train-validation split must prevent overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    bincount = np.bincount(list(Y_cov_class))\n",
    "    _ = plt.figure(figsize=(15, 6))\n",
    "    _ = sns.distplot(Y_cov_class, label=\"Train\", kde=False, bins=len(bincount))\n",
    "    _ = plt.legend()\n",
    "    _ = plt.title(\"Coverage distribution\")\n",
    "    print(bincount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(imgs, dim=64):\n",
    "    \"\"\"\"\"\"\n",
    "    return [resize(img, (dim, dim), mode='constant', preserve_range=True) for img in imgs]\n",
    "\n",
    "def img_func(images, random_state, parents, hooks):\n",
    "    \"\"\"Lambda cumsum transform\"\"\"\n",
    "    for i, img in enumerate(images):\n",
    "        images[i] = (np.float32(img) - img.mean()).cumsum(axis=0)\n",
    "    return images\n",
    "\n",
    "def keypoint_func(keypoints_on_images, random_state, parents, hooks):\n",
    "    return keypoints_on_images\n",
    "\n",
    "def augment_xy(X, Y, dim=128):\n",
    "    \"\"\"Augment X and Y same way\"\"\"\n",
    "    # Augmenting\n",
    "    # https://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    seq = iaa.Sequential([\n",
    "#         iaa.Scale(dim), # scales to (dim, dim)\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of images\n",
    "        iaa.OneOf([\n",
    "            iaa.Sometimes(0.5, iaa.Noop()), # iaa.Lambda(img_func, keypoint_func)), returns negative values on masks\n",
    "            iaa.SomeOf(2, [\n",
    "            iaa.Noop(),\n",
    "            iaa.Noop(),\n",
    "            iaa.GaussianBlur(sigma=(0.0, 2.0)),\n",
    "            iaa.Affine(rotate=(-10, 10), translate_percent={\"x\": (-0.25, 0.25)},  mode='symmetric',  cval=(0)),\n",
    "            iaa.PiecewiseAffine(scale=(0.02, 0.06), mode='edge', cval=(0)),\n",
    "        ]) \n",
    "                  ]),\n",
    "        # More as you want ...\n",
    "    ])\n",
    "    \n",
    "    # Convert the stochastic sequence of augmenters to a deterministic one.\n",
    "    # The deterministic sequence will always apply the exactly same effects to the images.\n",
    "    seq_det = seq.to_deterministic()\n",
    "    \n",
    "    X_aug = np.array(seq_det.augment_images(X))[...,np.newaxis]\n",
    "#     print(X_aug[:, :64, :64, :].shape)\n",
    "#     X_aug2 = np.array(downsample(seq_det.augment_images(X)))[...,np.newaxis]\n",
    "    Y_aug = np.array(seq_det.augment_images(Y))[...,np.newaxis]\n",
    "    X_dict = {'img': X_aug, \n",
    "              'csum': X_aug[:, :dim, :dim, :], # np.ones(len(X)*dim*dim).reshape(-1, dim, dim, 1) * 0.001,\n",
    "              'feat': np.ones(len(X)).reshape(-1, 1) * 0.01 * np.random.normal(0, 0.1, 1)}\n",
    "    return X_dict, Y_aug\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader - generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(batch=BATCH_SIZE, indices=train_indices, dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train):\n",
    "    \"\"\"Unlimited stochastic augmentation from training set\"\"\"\n",
    "    while True:\n",
    "        rand_indices = np.random.choice(indices, batch)  # unlimited sampling with replacement - stochastic\n",
    "        X = [upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        Y = [upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        yield augment_xy(X, Y, 64)\n",
    "        \n",
    "def val_gen(batch=BATCH_SIZE, indices=train_indices, dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train):\n",
    "    \"\"\"Unlimited stochastic sampling from validation set\"\"\"\n",
    "    while True:\n",
    "        rand_indices = np.random.choice(indices, batch)  # unlimited sampling with replacement - stochastic\n",
    "        X = [upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        Y = [upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), dim) / 255 for name in rand_indices]\n",
    "        yield np.array(X)[...,np.newaxis], np.array(Y)[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual sanity check\n",
    "XY = next(data_gen(batch=40, indices=train_indices.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train))\n",
    "plot_imgs_masks(XY[0]['img'], XY[1], title='Visual sanity check augmenting images and masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metrics and loss functions<a name='I'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Custom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(Y_true, Y_pred, score_thres=0.5):\n",
    "    \"\"\"Compute mean(IoU) metric\n",
    "    IoU = intersection / union\n",
    "    \n",
    "    For each (mask)threshold in provided range:\n",
    "     - convert probability mask to boolean mask based on given threshold\n",
    "     - score the mask 1 if(IoU > score_threshold(0.5))\n",
    "    Take the mean of the scoress\n",
    "\n",
    "    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n",
    "    \"\"\"\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n",
    "        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n",
    "        \n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            score = tf.identity(score) #!! use identity to transform score to tensor\n",
    "        prec.append(score) \n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "# Implementing a parameterized custom loss functions in Keras. \n",
    "# 1. writing a method for the coefficient/metric. \n",
    "# 2. writing a wrapper function to format things the way Keras needs them to be.\n",
    "\n",
    "# # import keras.backend as K\n",
    "# def dice_coef(y_true, y_pred, smooth, thresh):\n",
    "#     \"\"\"Dice coefficient\n",
    "\n",
    "#     Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "#          =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "#     ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "#     \"\"\"\n",
    "#     y_pred = y_pred > thresh\n",
    "#     y_pred = K.cast(y_pred, dtype='float32')\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     intersection = K.sum(y_true * y_pred, axis=-1)\n",
    "# #     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "#     return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "\n",
    "# def dice_loss_(smooth, thresh):\n",
    "#     \"\"\"Dice loss factory as Keras loss functions must only take (y_true, y_pred) as parameters.\"\"\"\n",
    "#     def dice(y_true, y_pred):\n",
    "#         return -dice_coef(y_true, y_pred, smooth, thresh)\n",
    "#     return dice\n",
    "\n",
    "\n",
    "# def dice_mean_loss(smooth, thresh):\n",
    "#     \"\"\"Dice loss factory as Keras loss functions must only take (y_true, y_pred) as parameters.\"\"\"\n",
    "#     def dice(y_true, y_pred):\n",
    "#         return 1 - dice_coef(y_true, y_pred, smooth, thresh)\n",
    "#     return dice\n",
    "\n",
    "# Finally, define the loss function to use in model compile\n",
    "# dice = dice_mean_loss(smooth=1e-5, thresh=0.5) # smooth=1\n",
    "\n",
    "# Jaccard loss algo\n",
    "# intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "#     sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "#     jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "#     return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth = 1\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32') # threshold 0.5\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture<a name='J'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Below functions create the UNet model in a functional and recursive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model block - helpers\n",
    "def conv_block(m, ch_dim, acti, bn, res, do=0):\n",
    "    \"\"\"CNN block\"\"\"\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    # http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review - dilation explained\n",
    "    # https://arxiv.org/pdf/1706.05587.pdf - DeepLab\n",
    "    n = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=1)(n)\n",
    "    o = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=3)(n) \n",
    "#     p = Conv2D(ch_dim, 3, activation=acti, padding='same', dilation_rate=5)(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n, o]) if res else n\n",
    "\n",
    "def input_feature(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    f_repeat = RepeatVector(xx*xx)(f)\n",
    "    f_conv = Reshape((xx, xx, n_features))(f_repeat)\n",
    "    n = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n\n",
    "\n",
    "def input_csum(f, n, n_features=1):\n",
    "    \"\"\"Input block\"\"\"\n",
    "    features = 1\n",
    "    xx = K.int_shape(n)[1]\n",
    "    n = Concatenate(axis=-1, name=f'csum_{2}')([n, f])\n",
    "    n = BatchNormalization()(n)            \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_block(m, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum):\n",
    "    \"\"\"Recursive CNN builder\"\"\"\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, ch_dim, acti, bn, res) # add/remove drop-out: ,do\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n",
    "        # Input csum at 3rd layer (32x32)\n",
    "        if (inp_csum is not None) and (depth==DEPTH):\n",
    "            m = Concatenate()([m, input_csum(inp_csum, m)])\n",
    "        # Input depth\n",
    "        if (inp_feat is not None) and (depth==1):\n",
    "            m = Concatenate()([m, input_feature(inp_feat, m)])\n",
    "        m = level_block(m, int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res, inp_feat, inp_csum)\n",
    "        \n",
    "        # Unwind recursive stack calls - creating the upscaling part of the model\n",
    "        if up:\n",
    "            # Repeat the rows and columns of the data by 2 and 2 respectively - untrainable like reverse pooling\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            # Transposed convolutions are going in the opposite direction of a normal convolution - trainable\n",
    "            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, ch_dim, acti, bn, res)\n",
    "    else:\n",
    "        # Depth == 0 - deepest conv_block\n",
    "        m = conv_block(m, ch_dim, acti, bn, res, do) # add/remove drop-out: ,do\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n",
    "    \"\"\"Returns model\"\"\"\n",
    "    inputs = Input(shape=img_shape, name='img')\n",
    "    inp_csum = None # Input(shape=(64, 64,1), name='csum') ###\n",
    "    inp_feat = None # Input(shape=(1,), name='feat') # or None\n",
    "    outputs = level_block(inputs, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, inp_feat, inp_csum)\n",
    "    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n",
    "#     return Model(inputs=[inputs, inp_csum, inp_feat], outputs=outputs)\n",
    "    return Model(inputs=[inputs], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model<a name='K'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Interesting reads:  \n",
    "[orginal paper](https://arxiv.org/pdf/1505.04597.pdf)  \n",
    "[Upsampling2D vs. Convolution2DTranspose](#https://distill.pub/2016/deconv-checkerboard/)  \n",
    "[Optimizers](http://ruder.io/optimizing-gradient-descent/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CH = 1                  # X_train.shape[-1]  # layers of image\n",
    "CONV_CH = 16                # number of channels to start/end UNet with\n",
    "DEPTH = 4                   # number of CONV blocks to max model depth - outstride 8: 128/8 = 16(depth=4), outstride 16: 128/16 = 8(depth=3), depth 3 doesnot work well\n",
    "D_OUT = 0.2                 # small random effect can be effective against overfitting. \n",
    "                            # Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n",
    "BN = True\n",
    "UP_CONV = False             # UPSampling avoids coarse checkerboard effect in images\n",
    "RES = True                  # residuals skip layers for deep networks (gradient evaporation)\n",
    "\n",
    "model = UNet((TGT_SIZE, TGT_SIZE, IMG_CH), \n",
    "             start_ch=CONV_CH, \n",
    "             depth=DEPTH, \n",
    "             dropout=D_OUT,\n",
    "             batchnorm=BN, \n",
    "             upconv=UP_CONV,\n",
    "             residual=RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "# Define optimizer\n",
    "# Clip gradients to norm 1., \n",
    "optimizer = {'adam': Adam(lr=LR, beta_1=0.9, beta_2=0.999, clipnorm=8.),\n",
    "             'sgd': SGD(lr=LR, decay=LR/100, momentum=0.99, nesterov=True, clipnorm=5.),\n",
    "             'rmsprop': RMSprop(lr=LR, rho=0.9, epsilon=None, decay=0.0),\n",
    "             'adadelta': Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
    "             'nadam': Nadam(lr=LR, beta_1=0.9, beta_2=0.999)} # , epsilon=None, schedule_decay=0.004\n",
    "opt = 'nadam'\n",
    "\n",
    "# Define loss\n",
    "# TODO Dice needs debugging\n",
    "loss = [\"binary_crossentropy\", \"kullback_leibler_divergence\", dice_loss, bce_dice_loss, bce_logdice_loss]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss[3], optimizer=optimizer[opt], metrics=[\"accuracy\", mean_iou]) #accuracy\n",
    "\n",
    "model_name = f'TGS_UNet_{opt}_CH-{IMG_CH}-{CONV_CH}_D-{DEPTH}_DO-{D_OUT>0}_BN-{BN}_UP-{UP_CONV}_RES-{RES}.h5'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=15, verbose=1), # bias-variance tradeof\n",
    "    ReduceLROnPlateau(patience=4, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(model_name, monitor='val_mean_iou', mode='max', save_best_only=True, verbose=1)] # continue/save based on monitor metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split<a name='H'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Validation set will be used for saving model checkpoints and early stopping. \n",
    "Tradeoff:\n",
    " - larger validation set for more validation accuracy and saving better generalizing model, which means less training samples and thus less generalizing model.\n",
    "\n",
    "Using images with depth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train val indices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_ids, val_ids = train_test_split(train_indices, test_size=0.15, random_state=1, stratify=Y_cov_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model<a name='L'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Model can be trained succesively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First fit\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 5\n",
    "history1 = model.fit_generator(data_gen(batch=BATCH_SIZE, indices=train_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              epochs=EPOCHS,\n",
    "                              steps_per_epoch=len(train_ids) //BATCH_SIZE,\n",
    "                              validation_data=val_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                              validation_steps=len(val_ids) //BATCH_SIZE,\n",
    "                              verbose=1, \n",
    "                              callbacks=callbacks,\n",
    "                              workers=-1)    \n",
    "\n",
    "# Second fit - can change the augmentation, batch size\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_2 = 0 # total no of epochs incl. first training session\n",
    "if EPOCHS_2 > EPOCHS:\n",
    "    history2 = model.fit_generator(data_gen(batch=BATCH_SIZE, indices=train_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                                  initial_epoch=EPOCHS,  # resume from earlier training\n",
    "                                  epochs=EPOCHS_2,\n",
    "                                  steps_per_epoch=len(train_ids) //BATCH_SIZE,\n",
    "                                  validation_data=val_gen(batch=BATCH_SIZE, indices=val_ids.tolist(), dim=TGT_SIZE, x_path=imgs_train, y_path=masks_train),\n",
    "                                  validation_steps=len(val_ids) //BATCH_SIZE,\n",
    "                                  verbose=1, \n",
    "                                  callbacks=callbacks,\n",
    "                                  workers=-1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves<a name='M'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "1. Check the curves for errors  \n",
    "2. Check the curves for roughness and convergence to tune the hyperparameters:  \n",
    " - learning rate and decay\n",
    " - batch size\n",
    " - model architecture\n",
    " - epochs and earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "for history in [history1,]: # history2]:\n",
    "    fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(20,8))\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    _ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    _ = ax_loss.legend()\n",
    "    _ = ax_loss.set_title('Loss')\n",
    "\n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "    _ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "    _ = ax_acc.legend()\n",
    "    _ = ax_acc.set_title('Accuracy')\n",
    "\n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n",
    "    _ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")\n",
    "    _ = ax_iou.legend()\n",
    "    _ = ax_iou.set_title('IoU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on validation set <a name='N'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = load_model(model_name, custom_objects={'mean_iou': mean_iou})\n",
    "print('model loading done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation set\n",
    "X_val = np.array([np.array(upsample(np.array(load_img(f'{imgs_train}/{name}.png', grayscale=True)), TGT_SIZE) / 255)[...,np.newaxis] for name in val_ids])\n",
    "Y_val = np.array([np.array(upsample(np.array(load_img(f'{masks_train}/{name}.png', grayscale=True)), TGT_SIZE) / 255)[...,np.newaxis] for name in val_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on validation set\n",
    "# DEV = True\n",
    "if DEV:\n",
    "    val_scores = model.evaluate(X_val, Y_val, verbose=1)\n",
    "    print(f'model validation scores: \\nval_loss: {val_scores[0]}, \\nval_acc: {val_scores[1]}, \\nval_miou: {val_scores[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "preds_valid = model.predict(X_val, verbose=1).reshape(-1, TGT_SIZE, TGT_SIZE)\n",
    "preds_valid = preds_valid.reshape(-1, TGT_SIZE, TGT_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    print(preds_valid.shape) #, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize validation(probability masks) layed over GT masks\n",
    "\n",
    "Grount truth: green mask\n",
    "Validation: pink mask\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold = .6\n",
    "if DEV:\n",
    "    N = np.arange(100)\n",
    "    plot_imgs_masks(X_val[N], Y_val[N], preds_valid=preds_valid[N]>.6, title='GT(green) vs.Validation mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: blur and threshold<a name='Q'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing helper functions\n",
    "def adaptive_gaus_thres(img, n=5, m=0):\n",
    "    \"\"\"Adaptive Gaussian thresholding\n",
    "    cv2.threshold requires a 1 layer uint8 image\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, n, m)\n",
    "\n",
    "\n",
    "def adaptive_mean_thres(img, n=5, m=0):\n",
    "    \"\"\"Adaptive Mean thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.adaptiveThreshold(img, 1, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, n, m)\n",
    "\n",
    "\n",
    "def otsu_thres(img, n=0, m=255):\n",
    "    \"\"\"Otsu thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "def gaus_thres(img, n=0, m=255):\n",
    "    \"\"\"Gaussian thresholding\"\"\"\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return cv2.threshold(img, n, m, cv2.ADAPTIVE_THRESH_GAUSSIAN_C)[1]\n",
    "\n",
    "\n",
    "def plot_mask_thres(fn, img, n, m, cmap='seismic', title=''):\n",
    "    \"\"\"Plot thresholding images\"\"\"\n",
    "    plt.imshow(fn(img, n, m), cmap=cmap); plt.title(title)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Compare masks with different blur\n",
    "    for i in random.sample(range(Y_val.shape[0]), 2):\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y = Y_val[...,0][i]\n",
    "        blurr = 7 # restricted in medianBlur - 5 is max\n",
    "        thres = .7\n",
    "        fig, ax = plt.subplots(1,4, figsize=(24,6))\n",
    "\n",
    "        _ = ax[0].imshow(y>thres, cmap=\"seismic\"); ax[0].set_title('Ground Truth mask')\n",
    "\n",
    "        _ = ax[1].imshow(y_pred>thres, cmap=\"seismic\"); ax[1].set_title('Prediction mask')\n",
    "        _ = ax[1].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        img_blur = cv2.medianBlur(y_pred, 5)\n",
    "        _ = ax[2].imshow(img_blur>thres, cmap=\"seismic\"); ax[2].set_title('Median blur')\n",
    "        _ = ax[2].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        img_blur = cv2.blur(y_pred, (blurr, blurr))\n",
    "        _ = ax[3].imshow(img_blur>thres, cmap=\"seismic\"); plt.title('Gaussian blur')\n",
    "        _ = ax[3].imshow(y>thres, alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Combinations of blur and threshold\n",
    "    sample = random.sample(range(Y_val.shape[0]), 2)\n",
    "    for i in sample:\n",
    "        thres = .7\n",
    "        blur_kernel = (7, 7)\n",
    "        Y_pred_gaus = np.array([cv2.blur(y, blur_kernel) for y in preds_valid]).reshape(preds_valid.shape)\n",
    "        \n",
    "        y = Y_val[...,0][i]\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y_gaus = Y_pred_gaus[...,0][i]\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(1,7, figsize=(25,5), sharey=True)\n",
    "        # GT mask\n",
    "        _ = ax1.imshow(y, cmap=\"seismic\"); \n",
    "        _ = ax1.set_title('Ground truth mask')\n",
    "\n",
    "        # Probability mask\n",
    "        _ = ax2.imshow(y_pred, cmap=\"seismic\"); \n",
    "        _ = ax2.set_title('Probability mask')\n",
    "        _ = ax2.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Prediction mask\n",
    "        img = y_pred > thres\n",
    "        _ = ax3.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax3.set_title('Prediction mask')\n",
    "        _ = ax3.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Opencv thresh prediction mask\n",
    "        thres, img = cv2.threshold(y_pred, thres, 1, cv2.THRESH_BINARY)\n",
    "        _ = ax4.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax4.set_title('Opencv threshold')\n",
    "        _ = ax4.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_gaus, thres, 1, cv2.THRESH_BINARY)\n",
    "        _ = ax5.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax5.set_title('Blur>thres')\n",
    "        _ = ax5.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Thres>blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_pred, thres, 1, cv2.THRESH_BINARY)\n",
    "        img = cv2.blur(img, blur_kernel) > thres\n",
    "        _ = ax6.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax6.set_title('Thres>blur>thres')\n",
    "        _ = ax6.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Blur>thres>blur>thres prediction mask\n",
    "        thres, img = cv2.threshold(y_gaus, thres, 1, cv2.THRESH_BINARY)\n",
    "        img = cv2.blur(img, blur_kernel) > thres\n",
    "        _ = ax7.imshow(img, cmap=\"seismic\"); \n",
    "        _ = ax7.set_title('Blur>thres>blur>thres')\n",
    "        _ = ax7.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Experiment with special thresholding algo's\n",
    "    for i in random.sample(range(Y_val.shape[0]), 5):\n",
    "        thres = .7\n",
    "        blur_kernel, blurr = (3, 3), 3\n",
    "        Y_pred_gaus = np.array([cv2.blur(y, blur_kernel) for y in preds_valid]).reshape(preds_valid.shape)\n",
    "        \n",
    "        y = Y_val[...,0][i]\n",
    "        y_pred = preds_valid[...,0][i]\n",
    "        y_pred = (255 * y_pred).astype(np.uint8)\n",
    "        y_gaus = Y_pred_gaus[...,0][i]\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1,6, figsize=(25,5), sharey=True)\n",
    "        # GT mask\n",
    "        _ = ax1.imshow(y, cmap=\"seismic\"); \n",
    "        _ = ax1.set_title('Ground truth mask')\n",
    "        \n",
    "        # Prediction mask\n",
    "        Y_ = y_pred > thres\n",
    "        _ = ax2.imshow(Y_, cmap=\"seismic\"); \n",
    "        _ = ax2.set_title('Prediction mask')\n",
    "        _ = ax2.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "\n",
    "        # Otsu prediction mask\n",
    "        _, Y_otsu = cv2.threshold(y_pred, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        _ = ax3.imshow(Y_otsu, cmap=\"seismic\"); \n",
    "        _ = ax3.set_title('Otsu mask')\n",
    "        _ = ax3.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        # Adaptive threshold\n",
    "        _, Y_gaus = cv2.threshold(y_pred, 60, 240, cv2.ADAPTIVE_THRESH_GAUSSIAN_C)\n",
    "        _ = ax4.imshow(cv2.bitwise_not(Y_gaus), cmap=\"seismic\"); \n",
    "        _ = ax4.set_title('Gaussian threshold')  # reverse image: cv2.bitwise_not()\n",
    "        _ = ax4.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        # Compbined Otsu and Adaptive\n",
    "        Y_mean = (Y_otsu+Y_gaus)/2\n",
    "        _ = ax5.imshow(cv2.bitwise_not(Y_mean), cmap=\"seismic\"); \n",
    "        _ = ax5.set_title('IoU Otsu and Gaussian') \n",
    "        _ = ax5.imshow(y, alpha=.3, cmap=\"Blues\")\n",
    "        \n",
    "        #\n",
    "        img_thresh_Gaussian = cv2.adaptiveThreshold(y_pred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 1)\n",
    "        _ = ax6.imshow(img_thresh_Gaussian, cmap=\"seismic\"); \n",
    "        _ = ax6.set_title('Adaptive Gaussian')\n",
    "        _ = ax6.imshow(y, alpha=.3, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Otsu masking\n",
    "def pred_otsu(y, thresh, blurr=3):\n",
    "    \"\"\"Preprocess mask for otsu threshold\n",
    "    Returns:\n",
    "    binary mask\"\"\"\n",
    "    blur_kernel = (blurr, blurr)\n",
    "    try:\n",
    "        im = cv2.blur(y, blur_kernel) > thresh\n",
    "        im = np.array(im * 255, dtype=np.uint8)\n",
    "        _, Y_otsu = cv2.threshold(im, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    except:\n",
    "        Y_otsu = np.zeros_like(y)\n",
    "    return Y_otsu//255\n",
    "\n",
    "def preds_otsu(Y, threshold, blurr):\n",
    "    \"\"\"Preprocess masks for otsu threshold\"\"\"\n",
    "    Y_otsu = [pred_otsu(y, threshold, blurr) for y in Y[...,0]]\n",
    "    return np.array(Y_otsu, dtype=np.int8).reshape(Y.shape)\n",
    "\n",
    "# Sanity check function\n",
    "if DEV:\n",
    "    for i in [6, 13, 17, 27]:\n",
    "        thres = .7\n",
    "        blurr = 3\n",
    "        _ = plt.imshow(preds_otsu(preds_valid[:28], threshold=thres, blurr=blurr)[i,...,0], cmap=\"seismic\")\n",
    "        _ = plt.imshow(Y_val[...,0][i], alpha=.3, cmap=\"Blues\")\n",
    "        _ = plt.title('Otsu'); \n",
    "        _ = plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring<a name='O'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "\n",
    "Score the model and do a threshold optimization by the best IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(Y_true, Y_pred):\n",
    "    \"\"\"IoU of unary/boolean mask using confusion matrix\"\"\"\n",
    "    CM, _, _ = np.histogram2d(Y_true.ravel(), Y_pred.ravel(), bins=(2,2))\n",
    "    TN, FN, FP, TP = np.split(CM.ravel(), 4)\n",
    "    if TP==0:\n",
    "        return 0\n",
    "    else:\n",
    "        iou = TP / (FN + FP + TP)\n",
    "    return np.mean(np.arange(0.5, 1.0, 0.05) < iou)\n",
    "\n",
    "def miou(Y_trues, Y_preds):\n",
    "    \"\"\"Mean intersection over union of all masks\"\"\"\n",
    "    return np.mean([IoU(Y_trues[i], Y_preds[i]) for i in range(Y_trues.shape[0])])\n",
    "\n",
    "def best_miou(mious):\n",
    "    \"\"\"Returns tuple of (best IoU, corresponding threshold)\"\"\"\n",
    "    best_idx = np.argmax(mious)\n",
    "    return mious[best_idx], thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold for best IoU score (submission)\n",
    "\n",
    "Checking which threshold delivers the best IoU score, so this threshold will be used for Test set prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare blur effect on IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    # Explore best params giving best IoU: blur kernel size, threshold\n",
    "    blur_range = [1, 3, 5]\n",
    "    mious_otsu = {}\n",
    "    for i, blurr in enumerate(blur_range):\n",
    "        print(f'computing best IoU @blur: {blurr}')\n",
    "        thresholds = np.linspace(0.1, 0.9, 80)\n",
    "        mious_otsu = {**mious_otsu, **{blurr: {threshold: miou(Y_val, preds_otsu(preds_valid, threshold, blurr=blurr))\n",
    "                     for threshold in tqdm_notebook(thresholds)}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "if DEV:\n",
    "    # Visualize IoU by blur and as function of thresholds\n",
    "    _ = plt.figure(figsize=(20,10))\n",
    "    _ = plt.xlabel(\"Threshold\")\n",
    "    _ = plt.ylabel(\"IoU\")\n",
    "    _ = plt.title(f\"Mean-IoU\")\n",
    "    \n",
    "    for i in blur_range:\n",
    "        best_idx = np.argmax(list(mious_otsu[i].values()))\n",
    "        threshold_best_ = list(mious_otsu[i])[best_idx]\n",
    "        miou_best_ = list(mious_otsu[i].values())[best_idx]\n",
    "        _ = plt.plot(list(mious_otsu[i]), mious_otsu[i].values(), label=f'Blur: {i}')\n",
    "        _ = plt.plot(threshold_best_, miou_best_, \"ok\", label=f\"Best threshold: {threshold_best_.round(3)}\")\n",
    "        _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU scores for best params\n",
    "# Blur 3 gives same reuslts as more blur, this is the savest bet\n",
    "blurr = 3 \n",
    "thresholds = np.linspace(0.1, 0.9, 80)\n",
    "mious_otsu = np.array([miou(Y_val, preds_otsu(preds_valid, threshold, blurr=blurr))\n",
    "                 for threshold in tqdm_notebook(thresholds)])\n",
    "\n",
    "# Get best scores by threshold\n",
    "miou_best_otsu, threshold_best_otsu = best_miou(mious_otsu)\n",
    "\n",
    "if DEV:\n",
    "    mious_no = np.array([miou(Y_val, np.int8(preds_valid > threshold)) \n",
    "                     for threshold in tqdm_notebook(thresholds)])\n",
    "    miou_best, threshold_best = best_miou(mious_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #DEV:\n",
    "    # Visualize best IoU as function of thresholds\n",
    "    plt.figure(figsize=(20,10))\n",
    "    _ = plt.plot(thresholds, mious_otsu, label=f'Otsu threshold with blur {blurr}')\n",
    "    _ = plt.plot(thresholds, mious_no, label='No blur')\n",
    "    _ = plt.plot(threshold_best, miou_best, \"xr\", label=f\"Best threshold: {threshold_best.round(3)}\")\n",
    "    _ = plt.plot(threshold_best_otsu, miou_best_otsu, \"xk\", label=f\"Best threshold otsu: {threshold_best_otsu.round(3)}\")\n",
    "    _ = plt.xlabel(\"Threshold\")\n",
    "    _ = plt.ylabel(\"IoU\")\n",
    "    _ = plt.title(f\"Mean-IoU: {miou_best.round(3)}(default) - {miou_best_otsu.round(3)}(otsu)\")\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize binary masks based on best threshold\n",
    "\n",
    "- Green are false positives (FP)\n",
    "- Pink are false negatives (FN)\n",
    "- Brown are true positives (TP)\n",
    "- Grey are true negatives (TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    plot_imgs_masks(X_val[N], Y_val[N], preds_valid=preds_valid[N], thres=threshold_best, title='GT vs. Validation masks - best threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    blurr = 5\n",
    "    thres = .5\n",
    "    plot_imgs_masks(X_val[N], Y_val[N], preds_valid=preds_otsu(preds_valid, threshold_best_otsu, blurr=blurr)[N], \n",
    "                    thres=threshold_best_otsu, title='GT vs. Validation masks - best Otsu threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of using Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    blurr = 5\n",
    "\n",
    "    Y_pred_non = np.array([y for y in preds_valid[N]]).reshape(preds_valid[N].shape)\n",
    "    Y_pred_otsu = preds_otsu(preds_valid, threshold_best_otsu, blurr=blurr)[N]\n",
    "\n",
    "    plot_imgs_masks((np.zeros_like(Y_pred_non)), Y_pred_non, preds_valid=Y_pred_otsu, thres=threshold_best_otsu, title='Overlay validation and Otso')\n",
    "    diff = abs(Y_pred_non - Y_pred_otsu) - np.ones_like(Y_pred_non)\n",
    "    plot_imgs_masks((np.zeros_like(Y_pred_non)), diff, preds_valid=diff, title='Difference between validation and Otsu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory: need ~7gb for test set\n",
    "gc.collect()\n",
    "print(f'notebook memory used: {mem_used()}mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set<a name='P'></a>\n",
    "<div align=\"right\">[>>TOC](#TOC)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check if all indices and images match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    test_ids = next(os.walk(f\"{path_test}/images\"))[2]\n",
    "    print(set(test_ids) ^ set(test_df.index+'.png'))\n",
    "    assert len(set(test_ids) ^ set(test_df.index+'.png')) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Convert test set images and depths to layers\n",
    "\n",
    " - Upsample images to arrays\n",
    " - Reshape for modeling\n",
    " - Create depth layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set has 17570/18000 = 97,6% non-empty images\n",
    "X_test = np.array([upsample(np.array(load_img(f'{imgs_test}/{name}.png', grayscale=True)), TGT_SIZE) / 255  # image vector\n",
    "              for name in tqdm_notebook(test_indices)])[...,np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict X_test - keep around for multi model input\n",
    "# X_test_dict = {'img': X_test, \n",
    "#                'csum': X_test_csum,\n",
    "#                'feat': X_test_norm_depth}\n",
    "\n",
    "Y_test = model.predict(X_test) # probability\n",
    "Y_test_otsu = preds_otsu(Y_test, threshold_best_otsu, 3) # is already unary/boolean!\n",
    "print('prediction done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually check of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of predictions without/with applying Otsu threshold\n",
    "if DEV:\n",
    "    N = np.arange(60)\n",
    "    plot_imgs_masks(X_test[N], np.zeros_like(X_test[N]), preds_valid=Y_test[N], thres=threshold_best, title='Predicted masks - best threshold')\n",
    "    plot_imgs_masks(X_test[N], np.zeros_like(X_test[N]), preds_valid=Y_test_otsu[N], thres=threshold_best_otsu, title='Predicted masks - best Otsu threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission  <a name='R'></a> \n",
    "<div align=\"right\">[>>TOC](#TOC)</div>\n",
    "Submission is in csv form:\n",
    " - `id`: index (equals filename)\n",
    " - `rle_mask`: run-length format (down-then-right): `masked_pixel_start` `<space>` `length_of_masked_pixels` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F'):\n",
    "    \"\"\"Convert binary mask image to run-length array or string.\n",
    "    \n",
    "    Args:\n",
    "    img: image in shape [n, m]\n",
    "    order: is down-then-right, i.e. Fortran(F)\n",
    "    string: return in string or array\n",
    "\n",
    "    Return:\n",
    "    run-length as a string: <start[1s] length[1s] ... ...>\n",
    "    \"\"\"\n",
    "    bytez = img.reshape(img.shape[0]*img.shape[1], order=order)\n",
    "    bytez = np.concatenate([[0], bytez, [0]])\n",
    "    runs = np.where(bytez[1:] != bytez[:-1])[0] + 1 # pos start at 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Use for sanity check the encode function\n",
    "def RLdec(rl_string, shape=(101, 101), order='F'):\n",
    "    \"\"\"Convert run-length string to binary mask image.\n",
    "    \n",
    "    Args:\n",
    "    rl_string: \n",
    "    shape: target shape of array\n",
    "    order: decode order is down-then-right, i.e. Fortran(F)\n",
    "\n",
    "    Return:\n",
    "    binary mask image as array\n",
    "    \"\"\"\n",
    "    s = rl_string.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img, dim=101):\n",
    "    \"\"\"Downsize an image\"\"\"\n",
    "    return cv2.resize(img, (dim, dim), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def downsample_(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode predictions on Otsu validation threshold - upsample doesnot work (??)\n",
    "pred_dict = {idx: RLenc(downsample(Y_test_otsu[i], dim=101))\n",
    "             for i, idx in enumerate(tqdm_notebook(test_indices))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV:\n",
    "    pd.DataFrame.from_dict(pred_dict, orient='index').sample(10)\n",
    "    \n",
    "pd.DataFrame.from_dict(pred_dict, orient='index').sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')\n",
    "print('submission saved!')\n",
    "\n",
    "if DEV:\n",
    "    sub.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for upvoting and sharing your thoughts!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
