{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Import libraries, modules\n",
    "\n",
    "Sequence; core/generic, specific, modules [a-z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from extensive list of imports\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import inspect\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# plot inline\n",
    "%matplotlib inline\n",
    "\n",
    "# set seeds for reproduction\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# interactive mode on\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define data transforms and augmentation before loading\n",
    "\n",
    "Compose augmentation(random), transforms for training and validation/test sets.\n",
    "Set size equal to model \n",
    "Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All torchvision pre-trained models expect input images normalized in the same way, \n",
    "# i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), \n",
    "# where H and W are expected to be at least 224. \n",
    "# The images have to be loaded into a range of [0, 1] and then normalized using \n",
    "MEAN, SD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Generic function to build transforms\n",
    "def transform_composer(img_size=0, val_size=0, **kwargs):\n",
    "    \"\"\"Build composed data transforms.\n",
    "    \n",
    "    :args: e.g.: img_size=224, val_size=256\n",
    "           kwargs: boolean list of transforms in correct order:\n",
    "           resize=True, c_crop=True, r_crop=True, flip=True, rotate=True, \n",
    "           tensor=True, normalize=True\n",
    "    :return: composed transform\"\"\"\n",
    "    train_dict = {\n",
    "         'r_crop': transforms.RandomResizedCrop(img_size),\n",
    "         'flip': transforms.RandomHorizontalFlip(),\n",
    "         'rotate': transforms.RandomRotation(5),\n",
    "         'tensor': transforms.ToTensor(),      \n",
    "         'normalize': transforms.Normalize(MEAN, SD)\n",
    "    }\n",
    "    val_dict = {\n",
    "         'resize': transforms.Resize(val_size),\n",
    "         'c_crop': transforms.CenterCrop(img_size),\n",
    "         'tensor': transforms.ToTensor(),      \n",
    "         'normalize': transforms.Normalize(MEAN, SD)\n",
    "    }\n",
    "    train_transforms = [train_dict[k] for k, v in kwargs.items() if v and k in train_dict]\n",
    "    val_transforms = [val_dict[k] for k, v in kwargs.items() if v and k in val_dict]\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose(train_transforms),\n",
    "        'val': transforms.Compose(val_transforms)\n",
    "        }\n",
    "    return data_transforms  \n",
    "    \n",
    "    \n",
    "# Compose transforms\n",
    "# img_size, val_size = 224, 256\n",
    "cnn_transforms = transform_composer(img_size=224, val_size=256, resize=True, c_crop=True, \n",
    "                                    r_crop=True, flip=True, rotate=True, tensor=True, normalize=True)\n",
    "# Sanity check: list of transforms\n",
    "print(repr(cnn_transforms['train'].__dict__))\n",
    "print(repr(cnn_transforms['val'].__dict__))\n",
    "\n",
    "# FashionMNIST\n",
    "fmnist_transforms = transform_composer(flip=True, rotate=True, tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check data directory for subfolders and files\n",
    "data_dir = 'hymenoptera_data'\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", data_dir]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders\n",
    "M_BATCH = 4\n",
    "WORKERS = 4\n",
    "PHASES = ['train', 'val']\n",
    "TRANSFORMS = cnn_transforms\n",
    "    \n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          TRANSFORMS[x])\n",
    "                                          for x in PHASES}\n",
    "\n",
    "dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=M_BATCH,\n",
    "                                  shuffle=True, num_workers=WORKERS)\n",
    "                                  for x in PHASES}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Sanity check images, class labels\n",
    "print((repr(image_datasets['train'].__dict__))[:500])\n",
    "print(dataset_sizes)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image viewer function\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Show images of Tensors in Dataloader.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0)) # convert to np\n",
    "    inp = (SD * inp) + MEAN\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Build model \n",
    "or \n",
    "Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network Architecture Class\n",
    "# IF NOT PRELOADED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Pytorch pretrained models\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() \n",
    "                     and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to set/define pretrained model\n",
    "def pre_model(model, pretrained=True, freeze=True):\n",
    "    \"\"\"\"\"\"\n",
    "    model = models.__dict__[model](pretrained=pretrained)\n",
    "    # freeze parameters in backprop\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features    # no. of features in fc layer\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # change out_features to 2 (binary loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define/set model\n",
    "model_all = pre_model('resnet18', pretrained=True, freeze=False)\n",
    "model_freeze = pre_model('resnet18', pretrained=True, freeze=True)\n",
    "\n",
    "# Sanity check: show model and architecture change\n",
    "print(model_all)\n",
    "print(models.resnet18(pretrained=True).fc)\n",
    "print(model_all.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer functions\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DECAY_STEP = 7  # epoch steps between LR decay\n",
    "DECAY_LR = 0.1\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer functions: optimize ALL parameters vs. final layer only\n",
    "optimizer_all = optim.SGD(model_all.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "optimizer_freeze = optim.SGD(model_freeze.fc.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "# Decay LR\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_all, step_size=DECAY_STEP, gamma=DECAY_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing model, load and resume\n",
    "def load_model(model, optimizer, num_epochs, resume=True):\n",
    "    \"\"\"Load and resume from existing model.\n",
    "    :return: model path\"\"\"\n",
    "    model_name = os.path.join(data_dir,\n",
    "                              str(model.__class__.__name__)+'_'+\n",
    "                              str(optimizer.__class__.__name__)+'_'+\n",
    "                              str(num_epochs)+'.pk1')\n",
    "    if os.path.exists(model_name) and resume:\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "    return model_name\n",
    "\n",
    "# Sanity check: path\n",
    "load_model(model_all, optimizer_all, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic train helper functions\n",
    "\n",
    "\n",
    "def b_ward(loss, optimizer, scheduler):\n",
    "    \"\"\"Backpropagate loss.\"\"\"\n",
    "    optimizer.zero_grad()   # reset gradients\n",
    "    loss.backward()         # backprop loss\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "\n",
    "def f_ward(model, phase, criterion, inputs, labels):\n",
    "    \"\"\"Forward pass.\n",
    "    \n",
    "    http://pytorch.org/docs/master/notes/autograd.html#volatile\n",
    "    :return: loss and accuracy\n",
    "    \"\"\"\n",
    "    inputs = Variable(inputs, volatile=(phase == 'val'), requires_grad=(phase == 'train'))\n",
    "    labels = Variable(labels)\n",
    "    \n",
    "    # Compute loss and predict label(max log-probability)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    acc = torch.sum(preds == labels.data)\n",
    "    return loss, acc\n",
    "\n",
    "    \n",
    "def train(model, loader, scheduler, criterion, optimizer, phase):\n",
    "    \"\"\"Training, validation for each epoch. Forward, backward props and caching metrics.\n",
    "    \n",
    "    :return: loss and accuracy\"\"\"\n",
    "    model.train(phase == 'train')\n",
    "    cache = {'cum_count': 0, 'cum_loss': 0.0, 'cum_acc': 0.0, \n",
    "             'avg_loss': 0.0, 'avg_acc': 0.0}\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        \n",
    "        # forward\n",
    "        loss, acc = f_ward(model, phase, criterion, inputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        if phase == 'train':\n",
    "            b_ward(loss, optimizer, scheduler)\n",
    "            \n",
    "        # stats\n",
    "        cache['cum_count'] += inputs.size()[0]\n",
    "        cache['cum_loss'] += loss.data[0]\n",
    "        cache['cum_acc'] += acc\n",
    "        cache['avg_loss'] = cache['cum_loss']/cache['cum_count']\n",
    "        cache['avg_acc'] = cache['cum_acc']/cache['cum_count']\n",
    "    return cache['avg_loss'], cache['avg_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function for training and evaluation of validation set\n",
    "def eval_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \"\"\"Running training and validation.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Load last best model saved\n",
    "    model_name = load_model(model, optimizer, num_epochs, resume=True)\n",
    "    print(model_name)\n",
    "    best_model = {'model': model_name, 'best_acc': 0.0, 'best_model_wts': model.state_dict()}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #lap = time.time()\n",
    "        print_header()\n",
    "    \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            lap = time.time()\n",
    "            loss, acc = train(model, dataloaders[phase], scheduler, criterion, optimizer, phase)\n",
    "            \n",
    "            # update LR decay\n",
    "            if phase == 'val':\n",
    "                scheduler.step(loss)\n",
    "            # update and save best_model\n",
    "            if phase == 'val' and acc > best_model['best_acc']:\n",
    "                best_model['best_acc'], best_model['best_model_wts'] = acc, model.state_dict()\n",
    "                torch.save(model.state_dict(), best_model['model'])\n",
    "                \n",
    "            end = time.time()\n",
    "            print_stat(phase, epoch, loss, acc, end-lap)\n",
    "            \n",
    "    finish = time.time()            \n",
    "    print_model_performance(finish-start, best_model)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model['best_model_wts'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic print helper functions\n",
    "\n",
    "\n",
    "# Helper functions for printing stats\n",
    "def time_format(secs):\n",
    "    \"\"\"Convert seconds to h:mm:ss.\"\"\"\n",
    "    m, s = divmod(secs, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "\n",
    "def print_header():\n",
    "    \"\"\"Print header.\"\"\"\n",
    "    h_template = \"\"\"{:8}\\t\\t {:8}\\t\\t    {:12}\\t {:8}\\t\\t {:8}\"\"\"\n",
    "    print()\n",
    "    print(h_template.format('Phase', 'Epoch', 'Loss', 'Accurracy', 'Duration'))\n",
    "       \n",
    "       \n",
    "def print_stat(phase, epoch, loss, acc, duration):\n",
    "    \"\"\"Print loss, accuracy and duration at each epoch/phase.\"\"\"\n",
    "    p_template = \"\"\"{:8}\\t\\t {:8}\\t\\t {:8.4f}\\t\\t    {:8.1f}\\t\\t {:8}\"\"\"\n",
    "    print(p_template.format(phase, epoch, loss, acc*100, time_format(duration)))\n",
    "    \n",
    "        \n",
    "def print_model_performance(duration, best_model):\n",
    "    \"\"\"Print best model performance and total duration.\"\"\"\n",
    "    print('Training and validation complete in: {:8}\\n'\n",
    "          'Best validation Accuracy: {:2.1f}%\\n'\n",
    "          'Learned model saved: {:16}\\n'.format(\n",
    "           time_format(duration), round(best_model['best_acc']*100), 2), best_model['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "# Train and evaluate validation set\n",
    "model_all = eval_model(model_all, criterion, optimizer_all, \n",
    "                       exp_lr_scheduler, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluate performance\n",
    "\n",
    "Tune hypermparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Transfer learning\n",
    "\n",
    "Only learn final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train weights in final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "model_freeze = eval_model(model_freeze, criterion, optimizer_freeze, \n",
    "                          exp_lr_scheduler, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict per batch\n",
    "def pred_batch(model):\n",
    "    \"\"\"Predict labels for one batch\"\"\"\n",
    "    inputs, labels = next(iter(dataloaders['val']))\n",
    "    v_inputs, v_labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "    outputs = model(v_inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return zip(inputs, preds, labels)\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "def show_pred_batch(model, n_batches, n_columns=M_BATCH):\n",
    "    \"\"\"Show from n batches n predictions\"\"\"\n",
    "    for _ in range(n_batches):\n",
    "        it_batch = list(pred_batch(model))\n",
    "        title = [(class_names[yhat], (yhat == y)) \n",
    "                  for _, yhat, y in it_batch][:n_columns]\n",
    "        inputs = ([input for input, _, _ in it_batch])[:n_columns]\n",
    "    \n",
    "        # Make a grid from batch\n",
    "        out = torchvision.utils.make_grid(inputs, padding=10)\n",
    "        imshow(out, title)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Show predictions\n",
    "show_pred_batch(model_freeze, 5, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
