{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch fundamentals\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(5, 3) # create tensor with memory pointer, can cause overflow\n",
    "torch.Tensor([5, 3])\n",
    "torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "z = torch.zeros([2, 4], dtype=torch.int32)\n",
    "x\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(x, y) == (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = None               # initiate variable\n",
    "torch.add(x, y, out=result) # assign to existing variable\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-place operations\n",
    "Any operation that mutates a tensor in-place is post-fixed with an _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x)\n",
    "x.t_()\n",
    "x.t_()\n",
    "x.copy_(y)\n",
    "x\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing - numpy syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy-tensor convertion and mutability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a torch Tensor to a numpy array and vice versa\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "c = np.copy(a)\n",
    "a.add_(1)\n",
    "a\n",
    "b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) # mutable\n",
    "np.add(a, 1, out=a)\n",
    "a\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.tensor(a) # copy\n",
    "np.add(a, 1, out=a)\n",
    "a\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `torch.tensor()` always copies data. If you have a Tensor data and just want to change its `requires_grad` flag, use `requires_grad_()` or `detach()` to avoid a copy. If you have a numpy array and want to avoid a copy, use `torch.from_numpy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "** `autograd.Variable`  is the central class of the package**. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call `.backward()` and have all the gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the **`.data`** attribute, while the gradient w.r.t. this variable is accumulated into **`.grad`**.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a `Function`.\n",
    "\n",
    "**`Variable` and `Function`** are interconnected and build up an acyclic graph, that encodes a complete history of computation.  \n",
    "Each variable has a **`.grad_fn`** attribute that references a `Function` that has created the `Variable` (except for Variables created by the user - their `grad_fn is None`).\n",
    "\n",
    "If you want to compute the **derivatives**, you can call **`.backward()`** on a `Variable`.   \n",
    " - if `Variable` is a scalar, you don’t need to specify any arguments to `backward()`\n",
    " - if it has more elements, you need to specify a **`grad_output`** argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "y = x + 2\n",
    "print('y:', y)\n",
    "print('y.data:', y.data)\n",
    "print('y.grad: ', y.grad)\n",
    "print('y.grad_fn:', y.grad_fn)\n",
    "\n",
    "print('x.data:', x.data)\n",
    "print('x.grad: ', x.grad)\n",
    "print('x.grad_fn', x.grad_fn)  # we've created x ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 3 * y**2\n",
    "out = z.mean()\n",
    "z\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of x is None without backprop\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagate\n",
    "\n",
    "**`backward()`** propagates back the *loss* and works like generator next().\n",
    "Differentiate the whole graph w.r.t. out and print gradient with respect to z(δout/δx), y(δout/δx) and x(δout/δx)\n",
    "\n",
    "out = mean(z) = 1/n * sum(z)  \n",
    "∂out/∂z = 1  \n",
    "\n",
    "z = 3 * y^2  \n",
    "∂z/∂y = 6 * y  \n",
    "\n",
    "y = x + 2  \n",
    "∂y/∂x = 1  \n",
    "\n",
    "∂out/∂x = ∂out/∂z * ∂z/∂y * ∂y/∂x = 6  \n",
    "27/6 = 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(z.grad)\n",
    "print(y.grad)\n",
    "print(x.grad) # requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ATTENTION: By default, gradient computation flushes all the internal buffers contained in the graph, so when you want to do mulitiple backwardprops and retain the gradients, you need to pass in **`retain_graph=True`** during the first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# the retain_graph flag will prevent the internal buffers from being freed\n",
    "loss = torch.ones(2, 2)\n",
    "y.backward(loss, retain_graph=True)\n",
    "y.backward(loss)\n",
    "y.backward(loss)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple forward props and one backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# Run multiple forward passes\n",
    "y = x * 2\n",
    "i = 1\n",
    "while y.data.norm() < 2000:\n",
    "    i *= 2\n",
    "    y = y * 2\n",
    "print(i, y, y.grad_fn)\n",
    "\n",
    "# backprop given gradients\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "\n",
    "# differentiate the whole graph w.r.t. y given gradients\n",
    "y.backward(gradients, retain_graph=True)\n",
    "\n",
    "print(i, x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use profile to see computational metrics\n",
    "x = Variable(torch.randn(1, 1), requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    y = x**2\n",
    "    y.backward(retain_graph=True)\n",
    "    \n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 kernel\n",
    "        # feature map = channel\n",
    "        # (nSamples x nChannels x Height x Width)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # == (1, 6, 5, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2, 2))\n",
    "        \n",
    "        # if the size is square specify (2, 2) as 2\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        # reshape with 'free' dimension - flatten\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"multiply dimensions\"\"\"\n",
    "        size = x.size()[1:] # all dimension except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "for param in params:\n",
    "    print(param.size()) # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnable params\n",
    "params[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The input to the forward is an `autograd.Variable` and so is the output. \n",
    "\n",
    "Note: Expected input size to this net(LeNet) is 32x32.  \n",
    "To use this net on MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Variable(torch.rand(1, 1, 32, 32))\n",
    "print(inputs)\n",
    "\n",
    "out = net(inputs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprop with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute loss (distance between input and target)\n",
    "\n",
    "If you have a single sample or target, just use `.unsqueeze(0)` to add a batch dimension at index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(inputs)\n",
    "target = Variable(torch.arange(1, 11)) # dummy target\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "output.size(), target.size(), target.unsqueeze(0).size()\n",
    "\n",
    "# Ensure the dimensions are the same\n",
    "loss = criterion(output, target.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you follow loss in the backward direction, using it’s `.grad_fn` attribute, you will see a graph of computations that looks like this:  \n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n",
    "      -> view -> linear -> relu -> linear -> relu -> linear  \n",
    "      -> MSELoss  \n",
    "      -> loss  \n",
    "      \n",
    "When we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their `.grad` Variable accumulated with the gradient.  \n",
    "\n",
    "A few steps backwards:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn)                                            # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])                       # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to loss.backward(). \n",
    "\n",
    "You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward:\\n{}'.format(net.conv1.bias.grad))\n",
    "\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward:\\n{}'.format(net.conv1.bias.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):  \n",
    "weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    _ = f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as:\n",
    " - SGD\n",
    " - Nesterov-SGD\n",
    " - Adam\n",
    " - RMSProp\n",
    " - etc. \n",
    "\n",
    "To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use optimizer in training loop like so;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = net(inputs)\n",
    "loss = criterion(output, target.unsqueeze(0))\n",
    "loss.backward()\n",
    "optimizer.step() # does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format to run (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    import time, datetime\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __call__(self):\n",
    "        return '{}'.format(str(datetime.timedelta(seconds=int(time.time()-self.start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainloader\n",
    "# TODO\n",
    "\n",
    "trainset_size = len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "N_EPOCHS = 5\n",
    "PRINT_FREQ = 200\n",
    "\n",
    "for epoch in np.arange(N_EPOCHS)+1:  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    stopwatch = Timer()\n",
    "    for i, data in enumerate(trainloader, 1):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        if i % PRINT_FREQ == 0:    # print every N mini-batches\n",
    "            print('Epoch: {}/{}, Batch: {}/{}, loss: {:.3f}, duration: {}'.format(epoch, N_EPOCHS, \n",
    "                                                                                  i, trainset_size//BATCH_SIZE, \n",
    "                                                                                  running_loss/PRINT_FREQ, stopwatch()))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "N_EPOCHS = 5\n",
    "PRINT_FREQ = 200\n",
    "\n",
    "for epoch in np.arange(N_EPOCHS)+1:  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    stopwatch = Timer()\n",
    "    for i, data in enumerate(trainloader, 1):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        if i % PRINT_FREQ == 0:    # print every N mini-batches\n",
    "            print('Epoch: {}/{}, Batch: {}/{}, loss: {:.3f}, duration: {}'.format(epoch, N_EPOCHS, \n",
    "                                                                                  i, trainset_size//BATCH_SIZE, \n",
    "                                                                                  running_loss/PRINT_FREQ, stopwatch()))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
