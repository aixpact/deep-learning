{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - image classifier\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "- Load and normalizing the CIFAR10 training and test datasets using torchvision\n",
    "- Define a Convolution Neural Network\n",
    "- Define a loss function\n",
    "- Train the network on the training data\n",
    "- Test the network on the test data\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "from mymods.lauthom import *\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "The output of torchvision datasets are PILImage images of range `[0, 1]`. \n",
    "We transform them to Tensors of normalized range `[-1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image standardisation constants:\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Other constants\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets and define minibatch loaders\n",
    "kwargs = {'batch_size': BATCH_SIZE, 'num_workers': 4}\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data', \n",
    "                                        train=True,\n",
    "                                        download=True, \n",
    "                                        transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          shuffle=True, \n",
    "                                          **kwargs) # stochastic shuffle\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root = './data', \n",
    "                                       train=False,\n",
    "                                       download=True, \n",
    "                                       transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         shuffle=False, \n",
    "                                         **kwargs)\n",
    "\n",
    "print('ready downloading')\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset)\n",
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "start = -BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate and show some random training images with respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('{}'.format(classes[labels[j]]) for j in np.arange(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer\n",
    "\n",
    "We can compute the spatial **size of the output** volume as a function of:\n",
    " - the input volume size (W)\n",
    " - the receptive field size of the Conv Layer neurons (F)\n",
    " - the stride with which they are applied (S)\n",
    " - the amount of zero padding used (P) on the border\n",
    " \n",
    "The formula for calculating how many neurons 'fit' is given by:\n",
    "(Wâˆ’F+2P)/S+1\n",
    "\n",
    "*E.g. for a 7x7 input and a 3x3 filter with stride 1 and pad 0 => (7 - 3 + 2*0)/1 +1 => 4/1 + 1 = 5 => output = 5x5\n",
    "With stride 2 we would get a 3x3 output => (7 - 3 + 2*0)/2 +1 => 4/2 + 1 = 3 => output = 3x3*\n",
    "\n",
    "Pooling layer\n",
    "\n",
    "\n",
    "Conv1 5x5:\n",
    "(224 - 5)/1 + 1 = 220x220\n",
    "\n",
    "Pooling 2x2\n",
    "220/2 = 110x110\n",
    "\n",
    "Conv2 5x5:\n",
    "(110 - 5)/1 + 1 = 106x106\n",
    "\n",
    "Pooling 2x2\n",
    "106/2 = 53x53 (=3364)\n",
    "\n",
    "179776 = (4, 16, 53, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN for 3 channel images\n",
    "# 2 5x5 filters\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)         # in: (4, 3, 224, 224), out: (4, 6, 220, 220)\n",
    "        self.pool = nn.MaxPool2d(2, 2)          # in: (4, 3, 220, 220), out: (4, 3, 110, 110)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)        # in: (4, 6, 110, 110), out: (4, 16, 106, 106)\n",
    "        # pool(2,2)                             # in: (4, 6, 106, 106), out: (4, 16, 53, 53)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120) # in: (4, 400), out: (4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)           # in: (4, 120), out: (4, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)            # in: (4, 84), out: (4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # \n",
    "        x = self.pool(F.relu(self.conv2(x)))    # out: 179776\n",
    "        x = x.view(-1, 16 * 53 * 53)            # resize: (-1, 400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.\n",
    "\n",
    "Conv1 3x3:\n",
    "(224 - 3)/1 + 1 = 222x222\n",
    "\n",
    "Conv2 3x3:\n",
    "(222 - 3)/1 + 1 = 220x220\n",
    "\n",
    "Conv2 1x1:\n",
    "(220 - 1)/1 + 1 = 220x220\n",
    "\n",
    "Pooling 2x2\n",
    "220/2 = 110x110 (=3364)\n",
    "\n",
    "Conv3 3x3:\n",
    "(110 - 3)/1 + 1 = 108x108\n",
    "\n",
    "Pooling 2x2\n",
    "108/2 = 54x54 (=2916)\n",
    "\n",
    "186624 = (4, 16, 54, 54) =  (4, 46656)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN for 3 channel images\n",
    "# 3 3x3 filters\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)         # in: (4, 3, 224, 224), out: (4, 6, 222, 222)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)        # in: (4, 6, 222, 222), out: (4, 16, 220, 220)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 1)        # in: (4, 16, 220, 220), out: (4, 8, 220, 220)\n",
    "        self.pool = nn.MaxPool2d(2, 2)          # in: (4, 8, 110, 110), out: (4, 8, 110, 110)\n",
    "        self.conv4 = nn.Conv2d(8, 4, 3)         # in: (4, 8, 110, 110), out: (4, 4, 108, 108)\n",
    "        # pool(2,2)                             # in: (4, 4, 108, 108), out: (4, 4, 54, 54)\n",
    "        self.fc1 = nn.Linear(4 * 54 * 54, 120)  # in: (4, 400), out: (4, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)           # in: (4, 120), out: (4, 84)\n",
    "        self.fc3 = nn.Linear(60, 10)            # in: (4, 84), out: (4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 4 * 54 * 54)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net2()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_size = len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    import time, datetime\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __call__(self):\n",
    "        return '{}'.format(str(datetime.timedelta(seconds=int(time.time()-self.start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "N_EPOCHS = 5\n",
    "PRINT_FREQ = 200\n",
    "\n",
    "for epoch in np.arange(N_EPOCHS)+1:  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    stopwatch = Timer()\n",
    "    for i, data in enumerate(trainloader, 1):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        if i % PRINT_FREQ == 0:    # print every N mini-batches\n",
    "            print('Epoch: {}/{}, Batch: {}/{}, loss: {:.3f}, duration: {}'.format(epoch, N_EPOCHS, \n",
    "                                                                                  i, trainset_size//BATCH_SIZE, \n",
    "                                                                                  running_loss/PRINT_FREQ, stopwatch()))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs are the energies for the 10 classes, highest energy wins\n",
    "outputs = net(Variable(images))\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance full testset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 2500 batches of 4 images, labels\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0) # tensor size (batch_size of dataloader)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {:.1f}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance per class\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of {} : {:.1f}%'.format(\n",
    "        classes[i], 100*class_correct[i]/class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
