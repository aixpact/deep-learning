{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Bag of Words\n",
    "\n",
    "Predict word relationships within a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "# Instantiate embedding; 2 words in vocab, 5 dimensional embeddings\n",
    "embeds = nn.Embedding(2, 5)  \n",
    "\n",
    "# Create look_up tensor\n",
    "# Input; vocab[index]\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "\n",
    "# Out; embedding vector/Variable\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "\n",
    "print(word_to_ix[\"hello\"])\n",
    "print(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram: 2 words to the left, 2 to the right\n",
    "CONTEXT_SIZE = 2  \n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(wordlist, n):\n",
    "    \"\"\"create nested list with context words and word\"\"\"\n",
    "    r = range(2*n+1)\n",
    "    neighbors = list(zip(*[(wordlist[i:]) for i in r if i != n]))  # *[...] == unlist args\n",
    "    return list(zip(neighbors, wordlist[n:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = get_ngrams(raw_text, CONTEXT_SIZE)\n",
    "print(ngrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word indexed vocabulary\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word indices to pytorch.Variable - LongTensor\n",
    "def idx_Variable(context, word_to_idx):\n",
    "    \"\"\"Convert a context to index Variable.\n",
    "    Includes a single word context conversion.\n",
    "    \n",
    "    return Variable same size of word vector\"\"\"\n",
    "    # Single or multiple words\n",
    "    if isinstance(context, str):\n",
    "        idx = [word_to_idx[context]]\n",
    "    else: \n",
    "        idx = [word_to_idx[w] for w in context]\n",
    "    return Variable(torch.LongTensor(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Compute the log probabilities with the `log_softmax` function of the most related word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"Continuous Bag of Words class implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)       # sum of w_embeddings\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"compute vocab log scores\"\"\"\n",
    "        embeds = self.embeddings(inputs)                   # in: context_size\n",
    "                                                           # out: context_size x embedding_dim\n",
    "        out = embeds.sum(dim=0)                            # out: embedding_dim\n",
    "        out = F.relu(self.linear1(out))                    # out: 128\n",
    "        out = self.linear2(out)                            # out: vocab_size\n",
    "        log_probs = F.log_softmax(out, dim=0).view(1, -1)  # out: vocab_size\n",
    "        return log_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Ideas to try:\n",
    " - dynamic loss functions\n",
    " - dynamic layers, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(VOCAB_SIZE, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    \n",
    "    for context, target in get_ngrams(raw_text, CONTEXT_SIZE):\n",
    "        \n",
    "        # Step 1. Convert context words into integer indices Variable\n",
    "        context_var = idx_Variable(context, word_to_idx)\n",
    "        \n",
    "        # Step 2. Zero out the gradients (reset) to avoid accumulation\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Forward pass\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute loss\n",
    "        loss = loss_function(log_probs, idx_Variable(target, word_to_idx))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "    losses.append(total_loss.item())\n",
    "    \n",
    "print(losses[-5:])  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log probabilities\n",
    "n = 10\n",
    "for target in list(vocab)[:n]:\n",
    "    test_var = idx_Variable(target, word_to_idx)\n",
    "    log_probs = model(test_var)\n",
    "    perc = torch.exp(log_probs)*100\n",
    "    perc = perc.squeeze().detach().numpy() # add .detach() when Variable has gradients\n",
    "    log_probs = log_probs.data.numpy()[0]\n",
    "    \n",
    "    # Sort words to relationship\n",
    "    idx_to_word = {v:k for k, v in word_to_idx.items()}\n",
    "    related = [idx_to_word[i] for i in np.argsort(log_probs)[::-1]]\n",
    "    \n",
    "    # min and max related words\n",
    "    print(\"Target: {}\\n-most related: {}\\n-least related: {}\\n\".format(target, related[0], related[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % probabilities - relationships with other words\n",
    "print(['{} is {:.0f}% related'.format(word, perc) for perc, word in zip(perc, word_to_idx.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
