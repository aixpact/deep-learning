{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV - capture Facetime and IP webcams (pictures and streams)\n",
    "by: frank@aixpact.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib ipympl\n",
    "# %matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# from imageio import imread\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from mymods.lauthom import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pycode')\n",
    "\n",
    "import cv2\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on FaceTime camera (green LED on), grab a single image, and then turned it off (LED off), \n",
    "saved the image to disk. \n",
    "When I have a USB camera plugged in as well, using device 0 or 1 seems to work, and I've captured test images from both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://192.168.192.42:8080/video'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facetime picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "retbool, frame = cap.read()\n",
    "\n",
    "# Convert to b/w\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Release capture and close camera\n",
    "cap.release() \n",
    "cv2.imwrite('cam_shot_bw.png', gray)\n",
    "cv2.imwrite('cam_shot_col.png', frame)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "plt.imshow(imread('cam_shot_col.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facetime or webcam stream in seperate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kill video output - close window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_stream(cap):\n",
    "    \"\"\"Kill the video stream in external video and restart kernel(hard kill of )\"\"\"\n",
    "    import os\n",
    "    cv2.waitKey(0) # kill camera when a key press is detected\n",
    "    cap.release() # kill camera\n",
    "    cv2.waitKey(0) # close window when a key press is detected\n",
    "    for _ in range(10):\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.waitKey(0) # exit child process(python kernel) - cv2 hangs python on closing windows\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open video stream in seperate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "timer = Timer()\n",
    "while timer.seconds() < 30:\n",
    "    # Capture frame-by-frame and mirror image\n",
    "    is_capturing, frame = cap.read()\n",
    "    mirror = cv2.flip(frame, 1)\n",
    "\n",
    "    # Covert to grayscale image\n",
    "    gray = cv2.cvtColor(mirror, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the camera in a seperate window\n",
    "    # Stop the camera when typing 'q' in the window\n",
    "    cv2.imshow('webcam', gray)\n",
    "    if cv2.waitKey(1) == ord('q') & 0xFF:\n",
    "        break\n",
    "\n",
    "# restart kernel when camera hangs\n",
    "kill_stream(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video capture with motion detection (output in jupyter lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Frame class\n",
    "\n",
    "Keeps track of motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame():\n",
    "    def __init__(self, source=0):\n",
    "        self.cap = cv2.VideoCapture(source)\n",
    "        \n",
    "        # set frame rate FPS\n",
    "        self.cap.set(cv2.CAP_PROP_FPS, 40) # 60\n",
    "        # alternatively use wait for N milisecs to change the frame rate\n",
    "        cv2.waitKey(1)\n",
    "        _, frame = self.cap.read()\n",
    "        mirror = cv2.flip(frame, 1)\n",
    "        self.bg_gray = cv2.cvtColor(mirror, cv2.COLOR_BGR2GRAY)\n",
    "        self.bg_col = cv2.cvtColor(mirror, cv2.COLOR_BGR2RGB)\n",
    "        self.rgb = None\n",
    "        self.mirror = None\n",
    "        self.gray = None\n",
    "        self.blur = None\n",
    "        self.motions = [(None, None, None, None)]\n",
    "        self.times = [datetime.now()]\n",
    "        \n",
    "    def capture(self):\n",
    "        is_capturing, frame = self.cap.read()\n",
    "        self.mirror = cv2.flip(frame, 1)\n",
    "        self.rgb = cv2.cvtColor(self.mirror, cv2.COLOR_BGR2RGB)\n",
    "        self.gray = cv2.cvtColor(self.mirror, cv2.COLOR_BGR2GRAY)\n",
    "        self.blur = cv2.GaussianBlur(self.gray, (21, 21), 0)\n",
    "        \n",
    "        diff_frame = cv2.absdiff(self.bg_gray, self.gray)\n",
    "        thresh_frame = cv2.threshold(diff_frame, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "        self.thres = cv2.dilate(thresh_frame, None, iterations=2)\n",
    "        self.get_rectangles()\n",
    "        \n",
    "    def get_contours(self):\n",
    "        \"\"\"Finding contour of moving object\"\"\"\n",
    "        (_, contours, _) = cv2.findContours(self.thres.copy(), \n",
    "                   cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        return contours\n",
    "        \n",
    "    def get_rectangles(self):\n",
    "        for contour in self.get_contours():\n",
    "            if cv2.contourArea(contour) < 10000:\n",
    "                continue\n",
    "            self.times.append(datetime.now())\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            self.motions.append((x, y, w, h))\n",
    "            # making green rectangle arround the moving object - updating rgb\n",
    "            self.rgb = cv2.rectangle(self.rgb, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Stop capturing\"\"\"\n",
    "        self.cap.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facetime = Frame(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facetime.capture()\n",
    "plt.imshow(facetime.rgb)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture new frame\n",
    "facetime.capture()\n",
    "plt.imshow(facetime.rgb)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object class paradigm - 2 camera's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuncAnimation needs %matplotlib ipympl\n",
    "\n",
    "# Initiate the two cameras\n",
    "# VideoCapture(0) is first connected camera \n",
    "# VideoCapture(url) is for ip camera \n",
    "cam1 = Frame(0)\n",
    "cam2 = Frame(0) #url)\n",
    "\n",
    "# Create two subplots\n",
    "f = plt.figure(figsize=(12,6))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Take with 2 camera's 1 shot and outpot as image\n",
    "cam1.capture()\n",
    "cam2.capture()\n",
    "im1 = ax1.imshow(cam1.rgb)\n",
    "im2 = ax2.imshow(cam2.blur)\n",
    "\n",
    "# Interactive animation - update images (new shots)\n",
    "def update(i):\n",
    "    cam1.capture()\n",
    "    cam2.capture()\n",
    "    im1.set_data(cam1.rgb)\n",
    "    im2.set_data(cam2.blur)\n",
    "    timest = cam1.times[-1].strftime(\"%H:%M:%S %Z\")\n",
    "    plt.suptitle(f'Last move: {timest}')\n",
    "\n",
    "ani = FuncAnimation(plt.gcf(), update, interval=60)\n",
    "\n",
    "def close(event):\n",
    "    if event.key == 'q':\n",
    "        plt.close(event.canvas.figure)\n",
    "        cam1.close()\n",
    "        cam2.close()\n",
    "\n",
    "cid = plt.gcf().canvas.mpl_connect(\"key_press_event\", close)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuncAnimation needs %matplotlib ipympl\n",
    "\n",
    "def mirror_frame(cap):\n",
    "    \"\"\"Grap frame and mirror it\"\"\"\n",
    "    is_capturing, frame = cap.read()\n",
    "    return cv2.flip(frame, 1)\n",
    "\n",
    "def rgb_frame(cap):\n",
    "    \"\"\"Grap frame and mirror it\"\"\"\n",
    "    mirror = mirror_frame(cap)\n",
    "    return cv2.cvtColor(mirror, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def bw_frame(cap):\n",
    "    \"\"\"Grap frame and mirror it\"\"\"\n",
    "    mirror = mirror_frame(cap)\n",
    "    return cv2.cvtColor(mirror, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def blur_frame(cap):\n",
    "    \"\"\"Grap frame and mirror it\"\"\"\n",
    "    gray = bw_frame(cap)\n",
    "    return cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "\n",
    "# Initiate the two cameras\n",
    "cap1 = cv2.VideoCapture(0)\n",
    "cap2 = cv2.VideoCapture(url)\n",
    "\n",
    "# Create two subplots\n",
    "f = plt.figure(figsize=(12,6))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Take with 2 camera's 1 shot and outpot as image\n",
    "im1 = ax1.imshow(rgb_frame(cap1))\n",
    "im2 = ax2.imshow(blur_frame(cap2))\n",
    "\n",
    "# Interactive animation\n",
    "def update(i):\n",
    "    im1.set_data(rgb_frame(cap1))\n",
    "    im2.set_data(blur_frame(cap2))\n",
    "\n",
    "ani = FuncAnimation(plt.gcf(), update, interval=100)\n",
    "\n",
    "def close(event):\n",
    "    if event.key == 'q':\n",
    "        plt.close(event.canvas.figure)\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "\n",
    "cid = plt.gcf().canvas.mpl_connect(\"key_press_event\", close)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Detection with 4 camera's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames\n",
    "facetime = Frame(url)\n",
    "facetime.capture()\n",
    "captures = [facetime.rgb, facetime.gray, facetime.blur, facetime.thres]\n",
    "images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 images in row\n",
    "fig12, axes = plt.subplots(2, 2, figsize=(10.5, 5.95))\n",
    "_ = plt.subplots_adjust(wspace=0, hspace=0) #left=None, bottom=None, right=None, top=None,\n",
    "_ = plt.suptitle('Motion detection')\n",
    "\n",
    "# Create subplots\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    images[i] = ax.imshow(captures[i])\n",
    "    _ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive animation\n",
    "def update(i):\n",
    "    \"\"\"Callback\"\"\"\n",
    "    facetime.capture()\n",
    "    captures = [facetime.rgb, facetime.gray, facetime.blur, facetime.thres]\n",
    "    for n, ax in enumerate(axes.flatten()):\n",
    "        images[n].set_data(captures[n])\n",
    "        \n",
    "def close(event):\n",
    "    if event.key == 'q':\n",
    "        facetime.close()\n",
    "        plt.close(event.canvas.figure)\n",
    "        \n",
    "ani = FuncAnimation(fig12, update, interval=100) # plt.gcf()\n",
    "\n",
    "cid = fig12.canvas.mpl_connect(\"key_press_event\", close) # plt.gcf()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
